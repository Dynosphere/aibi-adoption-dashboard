{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 10485760,
      "rowLimit": 1000
     },
     "inputWidgets": {},
     "nuid": "a5725d13-951c-4859-8a29-88b6f5b2561c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE MATERIALIZED VIEW your_catalog_name.your_schema_name.dbsql_cost_per_query_table\n",
    "(statement_id string,\n",
    "query_source_id string,\n",
    "query_source_type string,\n",
    "client_application string,\n",
    "executed_by string,\n",
    "warehouse_id string,\n",
    "statement_text string,\n",
    "workspace_id string,\n",
    "statement_hour_bucket_costs array<struct<hour_bucket:timestamp,hour_attributed_cost:double,hour_attributed_dbus:double>>,\n",
    "start_time timestamp,\n",
    "end_time timestamp,\n",
    "query_work_start_time timestamp,\n",
    "query_work_end_time timestamp,\n",
    "duration_seconds double,\n",
    "query_work_duration_seconds double,\n",
    "query_work_task_time_seconds double,\n",
    "query_attributed_dollars_estimation double,\n",
    "query_attributed_dbus_estimation double,\n",
    "url_helper string,\n",
    "query_profile_url string,\n",
    "most_recent_billing_hour timestamp,\n",
    "billing_record_check string,\n",
    "query_start_hour timestamp\n",
    ")\n",
    "PARTITIONED BY (query_start_hour, workspace_id)\n",
    "TBLPROPERTIES ('pipelines.autoOptimize.zOrderCols' = 'warehouse_id')\n",
    "AS\n",
    "(\n",
    "WITH \n",
    "-- Must make sure the time window the MV is built on has enough data from ALL 3 tables to generate accurate results (both starts and end time ranges)\n",
    "table_boundaries AS (\n",
    "SELECT \n",
    "(SELECT MAX(event_time) FROM system.compute.warehouse_events) AS max_events_ts,\n",
    "(SELECT MAX(end_time) FROM system.query.history) AS max_query_end_ts,\n",
    "(SELECT MAX(usage_end_time) FROM system.billing.usage) AS max_billing_ts,\n",
    "(SELECT MIN(event_time) FROM system.compute.warehouse_events) AS min_event_ts,\n",
    "(SELECT MIN(start_time) FROM system.query.history) AS min_query_start_ts,\n",
    "(SELECT MIN(usage_end_time) FROM system.billing.usage) AS min_billing_ts,\n",
    "date_trunc('HOUR', LEAST(max_events_ts, max_query_end_ts, max_billing_ts)) AS selected_end_time,\n",
    "(date_trunc('HOUR', GREATEST(min_event_ts, min_query_start_ts, min_billing_ts)) + INTERVAL 1 HOUR)::timestamp AS selected_start_time\n",
    "),\n",
    "\n",
    "----===== Warehouse Level Calculations =====-----\n",
    "cpq_warehouse_usage AS (\n",
    "  SELECT\n",
    "    usage_metadata.warehouse_id AS warehouse_id,\n",
    "    *\n",
    "  FROM\n",
    "    system.billing.usage AS u\n",
    "  WHERE\n",
    "    usage_metadata.warehouse_id IS NOT NULL\n",
    "    AND usage_start_time >= (SELECT MIN(selected_start_time) FROM table_boundaries)\n",
    "    AND usage_end_time <= (SELECT MAX(selected_end_time) FROM table_boundaries)\n",
    "),\n",
    "\n",
    "prices AS (\n",
    "  select coalesce(price_end_time, date_add(current_date, 1)) as coalesced_price_end_time, *\n",
    "  from system.billing.list_prices\n",
    "  where currency_code = 'USD'\n",
    "),\n",
    "\n",
    "filtered_warehouse_usage AS (\n",
    "    -- Warehouse usage is aggregated hourly, that will be the base assumption and grain of allocation moving forward. \n",
    "    -- Assume no duplicate records\n",
    "    SELECT \n",
    "      u.warehouse_id warehouse_id,\n",
    "      date_trunc('HOUR',u.usage_start_time) AS usage_start_hour,\n",
    "      date_trunc('HOUR',u.usage_end_time) AS usage_end_hour,\n",
    "      u.usage_quantity AS dbus,\n",
    "      (\n",
    "        CAST(p.pricing.effective_list.default AS FLOAT) * dbus\n",
    "      ) AS usage_dollars\n",
    "    FROM\n",
    "      cpq_warehouse_usage AS u\n",
    "        left join prices as p\n",
    "        on u.sku_name=p.sku_name\n",
    "        and u.usage_unit=p.usage_unit\n",
    "        and (u.usage_end_time between p.price_start_time and p.coalesced_price_end_time)\n",
    "),\n",
    "\n",
    "table_bound_expld AS \n",
    "(\n",
    "select timestampadd(hour, h, selected_start_time) as selected_hours\n",
    "  from table_boundaries\n",
    "  join lateral explode(sequence(0, timestampdiff(hour, selected_start_time, selected_end_time), 1)) as t (h)\n",
    "),\n",
    "\n",
    "----===== Query Level Calculations =====-----\n",
    "cpq_warehouse_query_history AS (\n",
    "  SELECT\n",
    "    account_id,\n",
    "    workspace_id,\n",
    "    statement_id,\n",
    "    executed_by,\n",
    "    statement_text,\n",
    "    compute.warehouse_id AS warehouse_id,\n",
    "    execution_status,\n",
    "    COALESCE(client_application, 'Unknown') AS client_application,\n",
    "    (COALESCE(CAST(total_task_duration_ms AS FLOAT) / 1000, 0) +\n",
    "      COALESCE(CAST(result_fetch_duration_ms AS FLOAT) / 1000, 0) +\n",
    "      COALESCE(CAST(compilation_duration_ms AS FLOAT) / 1000, 0)\n",
    "    )  AS query_work_task_time,\n",
    "    start_time,\n",
    "    end_time,\n",
    "    timestampadd(MILLISECOND , coalesce(waiting_at_capacity_duration_ms, 0) + coalesce(waiting_for_compute_duration_ms, 0) + coalesce(compilation_duration_ms, 0), start_time) AS query_work_start_time,\n",
    "    timestampadd(MILLISECOND, coalesce(result_fetch_duration_ms, 0), end_time) AS query_work_end_time,\n",
    "    -- NEW - Query source\n",
    "    CASE\n",
    "      WHEN query_source.job_info.job_id IS NOT NULL THEN 'JOB'\n",
    "      WHEN query_source.legacy_dashboard_id IS NOT NULL THEN 'LEGACY DASHBOARD'\n",
    "      WHEN query_source.dashboard_id IS NOT NULL THEN 'AI/BI DASHBOARD'\n",
    "      WHEN query_source.alert_id IS NOT NULL THEN 'ALERT'\n",
    "      WHEN query_source.notebook_id IS NOT NULL THEN 'NOTEBOOK'\n",
    "      WHEN query_source.sql_query_id IS NOT NULL THEN 'SQL QUERY'\n",
    "      WHEN query_source.genie_space_id IS NOT NULL THEN 'GENIE SPACE'\n",
    "      WHEN client_application IS NOT NULL THEN client_application\n",
    "      ELSE 'UNKNOWN'\n",
    "    END AS query_source_type,\n",
    "    COALESCE(\n",
    "      query_source.job_info.job_id,\n",
    "      query_source.legacy_dashboard_id,\n",
    "      query_source.dashboard_id,\n",
    "      query_source.alert_id,\n",
    "      query_source.notebook_id,\n",
    "      query_source.sql_query_id,\n",
    "      query_source.genie_space_id,\n",
    "      'UNKNOWN'\n",
    "    ) AS query_source_id\n",
    "  FROM\n",
    "    system.query.history AS h\n",
    "  WHERE\n",
    "    statement_type IS NOT NULL\n",
    "    -- If query touches the boundaries at all, we will divy it up\n",
    "    AND start_time < (SELECT selected_end_time FROM table_boundaries)\n",
    "    AND end_time > (SELECT selected_start_time FROM table_boundaries)\n",
    "    AND total_task_duration_ms > 0 --exclude metadata operations\n",
    "     and compute.warehouse_id is not null -- = 'd13162f928a069c7'\n",
    ")\n",
    "  ,  cte_warehouse as\n",
    "(\n",
    "  select warehouse_id, min(query_work_start_time) as min_start_time\n",
    "    from cpq_warehouse_query_history\n",
    "group by warehouse_id\n",
    ")\n",
    ",\n",
    "--- Warehouse + Query Level level allocation\n",
    "window_events AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        event_type,\n",
    "        event_time,\n",
    "        cluster_count AS cluster_count,\n",
    "        CASE\n",
    "            WHEN cluster_count = 0 THEN 'OFF'\n",
    "            WHEN cluster_count > 0 THEN 'ON'\n",
    "        END AS warehouse_state\n",
    "    FROM system.compute.warehouse_events AS we\n",
    "    -- Only get window events for when we have query history, otherwise, not usable\n",
    "    WHERE warehouse_id in (SELECT warehouse_id FROM cte_warehouse)\n",
    "    AND event_time >= (SELECT timestampadd(day, -1, selected_start_time) FROM table_boundaries)\n",
    "    AND event_time <= (SELECT selected_end_time FROM table_boundaries)\n",
    ")\n",
    "  ,  cte_agg_events_prep as\n",
    "(\n",
    "select warehouse_id\n",
    "     , warehouse_state\n",
    "     , event_time\n",
    "     , row_number() over W1\n",
    "     - row_number() over W2 as grp\n",
    "  from window_events\n",
    "window W1 as (partition by warehouse_id                  order by event_time asc)\n",
    "     , W2 as (partition by warehouse_id, warehouse_state order by event_time asc)\n",
    ")\n",
    "  ,  cte_agg_events as\n",
    "(\n",
    "  select warehouse_id\n",
    "       , warehouse_state                                    as window_state\n",
    "       , min(event_time)                                    as event_window_start\n",
    "       , lead(min(event_time), 1, selected_end_time) over W as event_window_end\n",
    "    from cte_agg_events_prep\n",
    "    join table_boundaries\n",
    "group by warehouse_id\n",
    "       , warehouse_state\n",
    "       , grp\n",
    "       , selected_end_time\n",
    "  window W as (partition by warehouse_id order by min(event_time) asc)\n",
    ")\n",
    "  ,  cte_all_events as\n",
    "(\n",
    "select warehouse_id\n",
    "     , window_state\n",
    "     , date_trunc('second', event_window_start) as event_window_start\n",
    "     , date_trunc('second', event_window_end  ) as event_window_end\n",
    "  from cte_agg_events\n",
    " where date_trunc('second', event_window_start) < date_trunc('second', event_window_end)\n",
    " --and date_trunc('second', event_window_start) >= timestamp '2024-11-14 09:00:00'\n",
    ")\n",
    "  ,  cte_queries_event_cnt as\n",
    "(\n",
    "  select warehouse_id\n",
    "       , case num\n",
    "           when 1\n",
    "           then date_trunc('second', query_work_start_time)\n",
    "           else timestampadd(second, case when date_trunc('second', query_work_start_time) = date_trunc('second', query_work_end_time) then 1 else 0 end, date_trunc('second', query_work_end_time))\n",
    "         end as query_event_time\n",
    "       , sum(num) as num_queries\n",
    "    from cpq_warehouse_query_history\n",
    "    join lateral explode(array(1, -1)) as t (num)\n",
    "group by 1, 2\n",
    ")\n",
    "  ,  cte_raw_history as\n",
    "(\n",
    "select warehouse_id\n",
    "     , query_event_time                                    as query_start\n",
    "     , lead(query_event_time, 1, selected_end_time) over W as query_end\n",
    "     , sum(num_queries) over W as queries_active\n",
    "  from cte_queries_event_cnt\n",
    "  join table_boundaries\n",
    "window W as (partition by warehouse_id order by query_event_time asc)\n",
    ")\n",
    "  ,  cte_raw_history_byday as\n",
    "(\n",
    "  select /*+ repartition(64, warehouse_id, query_start_dt) */\n",
    "         warehouse_id\n",
    "       , case num\n",
    "           when 0\n",
    "           then query_start\n",
    "           else timestampadd(day, num, query_start::date)\n",
    "         end::date as query_start_dt\n",
    "       , case num\n",
    "           when 0\n",
    "           then query_start\n",
    "           else timestampadd(day, num, query_start::date)\n",
    "         end as query_start\n",
    "       , case num\n",
    "           when timestampdiff(day, query_start::date, query_end::date)\n",
    "           then query_end\n",
    "           else timestampadd(day, num + 1, query_start::date)\n",
    "         end as query_end\n",
    "       , queries_active\n",
    "    from cte_raw_history\n",
    "    join lateral explode(sequence(0, timestampdiff(day, query_start::date, query_end::date), 1)) as t (num)\n",
    ")\n",
    "  ,  cte_all_time_union as\n",
    "(\n",
    "select warehouse_id\n",
    "     , case num when 1 then event_window_start else event_window_end end ts_start\n",
    "  from cte_all_events\n",
    "  join lateral explode(array(1, -1)) as t (num)\n",
    " union \n",
    "select warehouse_id\n",
    "     , case num when 1 then query_start else query_end end\n",
    "  from cte_raw_history_byday\n",
    "  join lateral explode(array(1, -1)) as t (num)\n",
    " union\n",
    "select warehouse_id, selected_hours\n",
    "  from cte_warehouse\n",
    "  join table_bound_expld on true\n",
    "-- where selected_hours >= timestampadd(day, -1, min_start_time)\n",
    ")\n",
    "  ,  cte_periods as\n",
    "(\n",
    "select /*+ repartition(64, warehouse_id, dt_start) */\n",
    "       warehouse_id\n",
    "     , ts_start::date as dt_start\n",
    "     , ts_start\n",
    "     , lead(ts_start, 1, selected_end_time) over W as ts_end\n",
    "  from cte_all_time_union\n",
    "  join table_boundaries\n",
    "window W as (partition by warehouse_id order by ts_start asc)\n",
    ")\n",
    "  ,  cte_merge_periods as\n",
    "(\n",
    "    select /*+ broadcast(r) */\n",
    "           p.warehouse_id\n",
    "         , date_trunc('hour', p.ts_start) as ts_hour\n",
    "         , sum(timestampdiff(second, p.ts_start, p.ts_end)) as duration\n",
    "         , case\n",
    "             when e.window_state = 'OFF'\n",
    "               or e.window_state is null\n",
    "             then 'OFF'\n",
    "             when r.queries_active > 0\n",
    "             then 'UTILIZED'\n",
    "             else 'ON_IDLE'\n",
    "           end as utilization_flag\n",
    "      from cte_periods           as p\n",
    " left join cte_all_events        as e  on e.warehouse_id       = p.warehouse_id\n",
    "                                      and e.event_window_start < p.ts_end\n",
    "                                      and e.event_window_end   > p.ts_start\n",
    " left join cte_raw_history_byday as r  on r.warehouse_id       = p.warehouse_id\n",
    "                                      and r.query_start_dt     = p.dt_start\n",
    "                                      and r.query_start        < p.ts_end\n",
    "                                      and r.query_end          > p.ts_start\n",
    "                                      and r.queries_active     > 0\n",
    "                                      and e.window_state      <> 'OFF'\n",
    "     where p.ts_start < p.ts_end\n",
    "  group by all\n",
    "),\n",
    "\n",
    "utilization_by_warehouse AS (\n",
    "  select warehouse_id\n",
    "       , ts_hour as warehouse_hour\n",
    "       , coalesce(sum(duration) filter(where utilization_flag = 'UTILIZED'), 0) as utilized_seconds\n",
    "       , coalesce(sum(duration) filter(where utilization_flag = 'ON_IDLE' ), 0) as idle_seconds\n",
    "       , coalesce(sum(duration) filter(where utilization_flag = 'OFF'     ), 0) as off_seconds\n",
    "       , coalesce(sum(duration), 0) as total_seconds\n",
    "       , try_divide(utilized_seconds, utilized_seconds + idle_seconds)::decimal(3,2) as utilization_proportion\n",
    "    from cte_merge_periods\n",
    "group by warehouse_id\n",
    "       , ts_hour\n",
    "),\n",
    "\n",
    "cleaned_warehouse_info AS (\n",
    "  SELECT\n",
    "  wu.warehouse_id,\n",
    "  wu.usage_start_hour AS hour_bucket,\n",
    "  wu.dbus,\n",
    "  wu.usage_dollars,\n",
    "  ut.utilized_seconds,\n",
    "  ut.idle_seconds,\n",
    "  ut.total_seconds,\n",
    "  ut.utilization_proportion\n",
    "  FROM filtered_warehouse_usage wu\n",
    "  LEFT JOIN utilization_by_warehouse AS ut ON wu.warehouse_id = ut.warehouse_id -- Join on calculation grain - warehouse/hour\n",
    "    AND wu.usage_start_hour = ut.warehouse_hour\n",
    "),\n",
    "\n",
    "hour_intervals AS (\n",
    "  -- Generate valid hourly buckets for each query\n",
    "  SELECT\n",
    "    statement_id,\n",
    "    warehouse_id,\n",
    "    query_work_start_time,\n",
    "    query_work_end_time,\n",
    "    query_work_task_time,\n",
    "    explode(\n",
    "      sequence(\n",
    "        0,\n",
    "        floor((UNIX_TIMESTAMP(query_work_end_time) - UNIX_TIMESTAMP(date_trunc('hour', query_work_start_time))) / 3600)\n",
    "      )\n",
    "    ) AS hours_interval,\n",
    "    timestampadd(hour, hours_interval, date_trunc('hour', query_work_start_time)) AS hour_bucket\n",
    "  FROM\n",
    "    cpq_warehouse_query_history\n",
    "),\n",
    "\n",
    "statement_proportioned_work AS (\n",
    "    SELECT * , \n",
    "        GREATEST(0,\n",
    "          UNIX_TIMESTAMP(LEAST(query_work_end_time, timestampadd(hour, 1, hour_bucket))) -\n",
    "          UNIX_TIMESTAMP(GREATEST(query_work_start_time, hour_bucket))\n",
    "        ) AS overlap_duration,\n",
    "        CASE WHEN CAST(query_work_end_time AS DOUBLE) - CAST(query_work_start_time AS DOUBLE) = 0\n",
    "        THEN 0\n",
    "        ELSE query_work_task_time * (overlap_duration / (CAST(query_work_end_time AS DOUBLE) - CAST(query_work_start_time AS DOUBLE)))\n",
    "        END AS proportional_query_work\n",
    "    FROM hour_intervals\n",
    "),\n",
    "\n",
    "\n",
    "attributed_query_work_all AS (\n",
    "    SELECT\n",
    "      statement_id,\n",
    "      hour_bucket,\n",
    "      warehouse_id,\n",
    "      SUM(proportional_query_work) AS attributed_query_work\n",
    "    FROM\n",
    "      statement_proportioned_work\n",
    "    GROUP BY\n",
    "      statement_id,\n",
    "      warehouse_id,\n",
    "      hour_bucket\n",
    "),\n",
    "\n",
    "--- Cost Attribution\n",
    "warehouse_time as (\n",
    "  select\n",
    "    warehouse_id,\n",
    "    hour_bucket,\n",
    "    SUM(attributed_query_work) as total_work_done_on_warehouse\n",
    "  from\n",
    "    attributed_query_work_all\n",
    "  group by\n",
    "    warehouse_id, hour_bucket\n",
    "),\n",
    "\n",
    "-- Create statement_id / hour bucket allocated combinations\n",
    "history AS (\n",
    "  SELECT\n",
    "    a.*,\n",
    "    b.total_work_done_on_warehouse,\n",
    "    CASE\n",
    "      WHEN attributed_query_work = 0 THEN NULL\n",
    "      ELSE attributed_query_work / total_work_done_on_warehouse\n",
    "    END AS proportion_of_warehouse_time_used_by_query\n",
    "  FROM attributed_query_work_all a\n",
    "    inner join warehouse_time b on a.warehouse_id = b.warehouse_id\n",
    "              AND a.hour_bucket = b.hour_bucket -- Will only run for completed hours from warehouse usage - nice clean boundary\n",
    "),\n",
    "\n",
    "history_with_pricing AS (\n",
    "  SELECT\n",
    "    h1.*,\n",
    "    wh.dbus AS total_warehouse_period_dbus,\n",
    "    wh.usage_dollars AS total_warehouse_period_dollars,\n",
    "    wh.utilization_proportion AS warehouse_utilization_proportion,\n",
    "    wh.hour_bucket AS warehouse_hour_bucket,\n",
    "    MAX(wh.hour_bucket) OVER() AS warehouse_max_hour_bucket\n",
    "  FROM\n",
    "    history AS h1\n",
    "    LEFT JOIN cleaned_warehouse_info AS wh ON h1.warehouse_id = wh.warehouse_id AND h1.hour_bucket = wh.hour_bucket\n",
    "),\n",
    "\n",
    "-- This is at the statement_id / hour grain (there will be duplicates for each statement for each hour bucket the query spans)\n",
    "\n",
    "query_attribution AS (\n",
    "  SELECT\n",
    "    a.*,\n",
    "    warehouse_max_hour_bucket AS most_recent_billing_hour,\n",
    "    CASE WHEN warehouse_hour_bucket IS NOT NULL THEN 'Has Billing Record' ELSE 'No Billing Record for this hour and warehouse yet available' END AS billing_record_check,\n",
    "    CASE\n",
    "      WHEN total_work_done_on_warehouse = 0 THEN NULL\n",
    "      ELSE attributed_query_work / total_work_done_on_warehouse\n",
    "    END AS query_task_time_proportion,\n",
    "\n",
    "    (warehouse_utilization_proportion * total_warehouse_period_dollars) * query_task_time_proportion  AS query_attributed_dollars_estimation,\n",
    "    (warehouse_utilization_proportion * total_warehouse_period_dbus) * query_task_time_proportion  AS query_attributed_dbus_estimation\n",
    "  FROM\n",
    "    history_with_pricing a\n",
    ")\n",
    "\n",
    "-- Final Output\n",
    "select\n",
    "      qq.statement_id,\n",
    "      FIRST(qq.query_source_id) AS query_source_id,\n",
    "      FIRST(qq.query_source_type) AS query_source_type,\n",
    "      FIRST(qq.client_application) AS client_application,\n",
    "      FIRST(qq.executed_by) AS executed_by,\n",
    "      FIRST(qq.warehouse_id) AS warehouse_id,\n",
    "      FIRST(qq.statement_text) AS statement_text,\n",
    "      FIRST(qq.workspace_id) AS workspace_id,\n",
    "      COLLECT_LIST(NAMED_STRUCT('hour_bucket', qa.hour_bucket, 'hour_attributed_cost', query_attributed_dollars_estimation, 'hour_attributed_dbus', query_attributed_dbus_estimation)) AS statement_hour_bucket_costs,\n",
    "      FIRST(qq.start_time) AS start_time,\n",
    "      FIRST(qq.end_time) AS end_time,\n",
    "      FIRST(qq.query_work_start_time) AS query_work_start_time,\n",
    "      FIRST(qq.query_work_end_time) AS query_work_end_time,\n",
    "      COALESCE(timestampdiff(MILLISECOND, FIRST(qq.start_time), FIRST(qq.end_time))/1000, 0) AS duration_seconds,\n",
    "      COALESCE(timestampdiff(MILLISECOND, FIRST(qq.query_work_start_time), FIRST(qq.query_work_end_time))/1000, 0) AS query_work_duration_seconds,\n",
    "      FIRST(query_work_task_time) AS query_work_task_time_seconds,\n",
    "      SUM(query_attributed_dollars_estimation) AS query_attributed_dollars_estimation,\n",
    "      SUM(query_attributed_dbus_estimation) AS query_attributed_dbus_estimation,\n",
    "      FIRST(CASE\n",
    "        WHEN query_source_type = 'JOB' THEN CONCAT('/jobs/', query_source_id)\n",
    "        WHEN query_source_type = 'SQL QUERY' THEN CONCAT('/sql/queries/', query_source_id)\n",
    "        WHEN query_source_type = 'AI/BI DASHBOARD' THEN CONCAT('/sql/dashboardsv3/', query_source_id)\n",
    "        WHEN query_source_type = 'LEGACY DASHBOARD' THEN CONCAT('/sql/dashboards/', query_source_id)\n",
    "        WHEN query_source_type = 'ALERTS' THEN CONCAT('/sql/alerts/', query_source_id)\n",
    "        WHEN query_source_type = 'GENIE SPACE' THEN CONCAT('/genie/rooms/', query_source_id)\n",
    "        WHEN query_source_type = 'NOTEBOOK' THEN CONCAT('/editor/notebooks/', query_source_id)\n",
    "        ELSE ''\n",
    "      END) as url_helper,\n",
    "      FIRST(CONCAT('/sql/history?uiQueryProfileVisible=true&queryId=', qq.statement_id)) AS query_profile_url,\n",
    "       FIRST(most_recent_billing_hour) AS most_recent_billing_hour,\n",
    "       FIRST(billing_record_check) AS billing_record_check,\n",
    "       date_trunc('HOUR', FIRST(qq.start_time)) AS query_start_hour\n",
    "      from query_attribution qa\n",
    "      LEFT JOIN cpq_warehouse_query_history AS qq ON qa.statement_id = qq.statement_id -- creating dups of the objects but just re-aggregating\n",
    "            AND qa.warehouse_id = qq.warehouse_id\n",
    "      GROUP BY qq.statement_id\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "sqlQueryOptions": {
     "applyAutoLimit": true,
     "catalog": "kg_test_workspace",
     "schema": "default"
    }
   },
   "notebookName": "dbsql_cost_per_query_mv.dbquery.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
