{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aee1651c-4ec6-4cc7-b6e6-df71e494ddef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Genie Space Observability E2E Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a0f9eb-9bdc-4655-8a2d-77fc32419849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Introduction\n",
    "This notebook covers how to leverage Genie REST APIs and Databricks System Tables to create an e2e Genie Observability workflow, complete with a Genie Observability Dashboard and a Meta-Genie space to ask questions about your Genie spaces!\n",
    "\n",
    "Running this notebook will create **Two Tables**; _genie_observability_main_table_ and _dbsql_cost_per_query_table_.\n",
    "\n",
    "- **genie_observability_main_table:** This table consists of ALL the messages sent to your genie spaces in a given workspace along with Genie natural langauge response and the SQL code generated as part of it. There are additional columns that provide the conversation_id, user_feedback, user_email, the statement ID linked to the SQL code and much more.\n",
    "- **dbsql_cost_per_query_table:** This table consists of the full set of SQL commands that has been executed each linked to a unique statement ID. This table will be useful to filter out those SQL commands executed as part of a Genie space.\n",
    "\n",
    "### Disclaimer:\n",
    "- This is a Standalone Notebook, so please ensure the catalog, schema and table names **DO NOT** overwrite any exisitng tables\n",
    "- The entire solution is vibe coded (with human in the loop!), so please verify and sense check code before running.\n",
    "- The cost per query attribution code adpated from this materialised view definition - https://github.com/databrickslabs/sandbox/blob/main/dbsql/cost_per_query/PrPr/DBSQL%20Cost%20Per%20Query%20MV%20(PrPr).sql\n",
    "\n",
    "**Usage Guidance:**\n",
    "- You can use this notebook as part of the wider Databricks Adoption Dashboard and pipeline that can be deployed as part of Databricks Asset Bundles. Equally, you can download this notebook incorporate it into your own workflow.\n",
    "- We recommend running this notebook in a test environment first and tailor the code as per your preferance.\n",
    "\n",
    "**Pre-requisites:**\n",
    "- This notebook/workflow is targetted towards a Genie space manager\n",
    "- Entitlements: You must have the Databricks SQL workspace entitlement. See Manage entitlements.\n",
    "- Compute: CAN USE access on at least one pro or serverless SQL warehouse.\n",
    "- Data access: SELECT privileges on the data used in the space.\n",
    "- Genie space ACLs: At least CAN EDIT permissions on the Genie space. Genie space creators automatically have CAN MANAGE permissions on spaces they create. See Genie space ACLs.\n",
    "- You should have enough permissions to create tables in a schema within a catalog of your choice\n",
    "- Access to system tables is governed by Unity Catalog. Account admins have access to system tables by default. To allow a user to query system tables, an admin must grant that user USE and SELECT permissions on the system schemas. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4cd870-8b3a-474d-9225-2774b9725c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upgrade to the latest version of databricks_sdk package\n",
    "%pip install databricks_sdk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff69540-d03b-4f4e-9bb8-aab8014e1e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart Python to ensure all libraries are reloaded\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261aa6d3-a8ae-4eb3-8162-19618b71abe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a text widget for catalog_name with default value 'users'\n",
    "dbutils.widgets.text(\"catalog_name\", \"users\")\n",
    "# Create a text widget for schema_name with no default value\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "\n",
    "# Retrieve the value of catalog_name from the widget\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "# Retrieve the value of schema_name from the widget\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "# Ensure both catalog_name and schema_name are provided\n",
    "assert catalog_name and schema_name, \"catalog_name and schema_name must be provided\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb27c2d-e967-40f3-a3d4-d988889958ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get current user information using the workspace client\n",
    "current_user = w.current_user.me()\n",
    "user_name = current_user.user_name\n",
    "\n",
    "# Print the current user's username\n",
    "print(f\"Current user: {user_name}\")\n",
    "\n",
    "# Print the Databricks workspace host URL\n",
    "print(f\"Workspace: {w.config.host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ec0d27-ff1f-4464-a648-7ba92d1a60f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below we list all Genie spaces in the current workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0f699f-c346-4d5b-9754-193c3d26b41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store Genie space metadata\n",
    "spaces = []\n",
    "page_token = None\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Databricks Workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Paginate through all Genie spaces using the SDK\n",
    "while True:\n",
    "    response = w.genie.list_spaces(page_token=page_token)\n",
    "    for s in response.spaces:\n",
    "        # Append relevant space details to the list\n",
    "        spaces.append({\n",
    "            \"space_id\": getattr(s, \"space_id\", None),\n",
    "            \"name\": getattr(s, \"title\", None),\n",
    "            \"description\": getattr(s, \"description\", None),\n",
    "            \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "        })\n",
    "    # Break if there are no more pages\n",
    "    if not response.next_page_token or response.next_page_token == \"\":\n",
    "        break\n",
    "    page_token = response.next_page_token\n",
    "\n",
    "# Convert the list of spaces to a Pandas DataFrame\n",
    "genie_spaces_pdf = pd.DataFrame(spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d99a3ab-9b40-4a6e-b229-8711026c1845",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 9"
    }
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame containing all Genie spaces metadata\n",
    "if len(genie_spaces_pdf) > 0:\n",
    "  display(genie_spaces_pdf)\n",
    "  print(\"---------------\")\n",
    "  print(f\"Total Genie spaces: {len(genie_spaces_pdf)}\")\n",
    "else:\n",
    "  print(\"---------------\")\n",
    "  print(\"No Genie spaces found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b087ff-9909-4518-9a4d-99bf099ef509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code cell below is where we define the single most important function that allows us to generate the genie_observability_main_table table. You can edit this function as per your requirements or even reverse engineer this to generate newer insights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc06be5-470f-4524-b65a-125f9cf832cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Databricks Python SDK implementation\n",
    "For implementation with python requests library see appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80d2c3a-4661-4996-89bb-8d35a02b3fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Optional\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.iam import User\n",
    "\n",
    "# Cache for user email lookups to prevent redundant API calls\n",
    "USER_CACHE: Dict[str, str] = {}\n",
    "\n",
    "\n",
    "def get_genie_observability_table(space_id: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches and constructs a Spark DataFrame containing observability data \n",
    "    for all messages in a Genie space using the Databricks SDK.\n",
    "    \n",
    "    Args:\n",
    "        space_id: The ID of the Genie space to fetch data from.\n",
    "        \n",
    "    Returns:\n",
    "        A Spark DataFrame with message-level observability data.\n",
    "    \"\"\"\n",
    "    w = WorkspaceClient()\n",
    "    \n",
    "    # Get space details\n",
    "    space = w.genie.get_space(space_id=space_id)\n",
    "    space_name = space.title or f\"Space_{space_id}\"\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    # List all conversations with pagination\n",
    "    conversations = _list_all_conversations(w, space_id)\n",
    "    \n",
    "    for conv in conversations:\n",
    "        # List all messages in the conversation with pagination\n",
    "        messages = _list_all_messages(w, space_id, conv.conversation_id)\n",
    "        \n",
    "        for msg in messages:\n",
    "            record = _extract_message_data(msg, space_id, space_name, w)\n",
    "            records.append(record)\n",
    "    \n",
    "    # Create Spark DataFrame\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    schema = _get_schema()\n",
    "    \n",
    "    if not records:\n",
    "        return spark.createDataFrame([], schema)\n",
    "    \n",
    "    # Convert dicts to tuples in schema field order\n",
    "    ordered_data = [\n",
    "        tuple(r.get(field.name) for field in schema.fields)\n",
    "        for r in records\n",
    "    ]\n",
    "        \n",
    "    df = spark.createDataFrame(ordered_data, schema=schema)\n",
    "    \n",
    "    # Add human-readable datetime columns\n",
    "    df = (df\n",
    "        .withColumn(\"created_datetime\", from_unixtime(col(\"created_timestamp\") / 1000).cast(TimestampType()))\n",
    "        .withColumn(\"last_updated_datetime\", from_unixtime(col(\"last_updated_timestamp\") / 1000).cast(TimestampType()))\n",
    "        .orderBy(\"created_timestamp\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _list_all_conversations(w: WorkspaceClient, space_id: str) -> List:\n",
    "    \"\"\"Fetches all conversations in a space, handling pagination.\"\"\"\n",
    "    conversations = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        response = w.genie.list_conversations(\n",
    "            space_id=space_id,\n",
    "            include_all=True,\n",
    "            page_token=page_token\n",
    "        )\n",
    "        conversations.extend(response.conversations or [])\n",
    "        \n",
    "        page_token = response.next_page_token\n",
    "        if not page_token:\n",
    "            break\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "\n",
    "def _list_all_messages(w: WorkspaceClient, space_id: str, conversation_id: str) -> List:\n",
    "    \"\"\"Fetches all messages in a conversation, handling pagination.\"\"\"\n",
    "    messages = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        response = w.genie.list_conversation_messages(\n",
    "            space_id=space_id,\n",
    "            conversation_id=conversation_id,\n",
    "            page_token=page_token\n",
    "        )\n",
    "        messages.extend(response.messages or [])\n",
    "        \n",
    "        page_token = response.next_page_token\n",
    "        if not page_token:\n",
    "            break\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def _extract_message_data(message, space_id: str, space_name: str, w: WorkspaceClient) -> Dict[str, Any]:\n",
    "    \"\"\"Extracts relevant fields from a GenieMessage into a flat dictionary.\"\"\"\n",
    "    msg_dict = message.as_dict()\n",
    "    \n",
    "    # Extract IDs (API has both 'id' and 'message_id' for legacy compatibility)\n",
    "    msg_id = msg_dict.get('message_id') or msg_dict.get('id')\n",
    "    user_id = msg_dict.get('user_id')\n",
    "    user_email = _resolve_user_email(str(user_id) if user_id else None, w)\n",
    "\n",
    "    record = {\n",
    "        'space_id': space_id,\n",
    "        'space_name': space_name,\n",
    "        'message_id': msg_id,\n",
    "        'conversation_id': msg_dict.get('conversation_id'),\n",
    "        'user_id': str(user_id) if user_id else None,\n",
    "        'user_email': user_email,\n",
    "        'status': str(msg_dict.get('status')) if msg_dict.get('status') else None,\n",
    "        'created_timestamp': msg_dict.get('created_timestamp'),\n",
    "        'last_updated_timestamp': msg_dict.get('last_updated_timestamp'),\n",
    "        'user_question': msg_dict.get('content'),\n",
    "    }\n",
    "    \n",
    "    # Process attachments\n",
    "    ai_responses, sql_queries, statement_ids, suggested_qs = [], [], [], []\n",
    "    attachments = msg_dict.get('attachments') or []\n",
    "    \n",
    "    for att in attachments:\n",
    "        # Text attachment\n",
    "        if text_obj := att.get('text'):\n",
    "            ai_responses.append(text_obj.get('content', ''))\n",
    "        \n",
    "        # Query attachment\n",
    "        if query_obj := att.get('query'):\n",
    "            sql_queries.append(query_obj.get('query', ''))\n",
    "            if stmt_id := query_obj.get('statement_id'):\n",
    "                statement_ids.append(str(stmt_id))\n",
    "        \n",
    "        # Suggested questions attachment\n",
    "        if sq_obj := att.get('suggested_questions'):\n",
    "            suggested_qs.extend(sq_obj.get('questions', []))\n",
    "\n",
    "    def join_non_empty(items: List, sep: str = ' | ') -> Optional[str]:\n",
    "        filtered = list(filter(None, items))\n",
    "        return sep.join(filtered) if filtered else None\n",
    "\n",
    "    record.update({\n",
    "        'ai_response': join_non_empty(ai_responses),\n",
    "        'sql_query': join_non_empty(sql_queries),\n",
    "        'statement_id': join_non_empty(statement_ids),\n",
    "        'suggested_questions': join_non_empty(suggested_qs, ', '),\n",
    "        'num_attachments': len(attachments),\n",
    "    })\n",
    "    \n",
    "    # Feedback rating\n",
    "    feedback = msg_dict.get('feedback') or {}\n",
    "    record['feedback_rating'] = str(feedback.get('rating')) if feedback.get('rating') else 'NONE'\n",
    "    \n",
    "    # Error info\n",
    "    error = msg_dict.get('error')\n",
    "    if error and isinstance(error, dict):\n",
    "        record['error_type'] = error.get('type')\n",
    "        record['error_message'] = error.get('message') or error.get('error')\n",
    "    elif error:\n",
    "        record['error_type'] = 'Unknown'\n",
    "        record['error_message'] = str(error)\n",
    "    else:\n",
    "        record['error_type'] = None\n",
    "        record['error_message'] = None\n",
    "    \n",
    "    return record\n",
    "\n",
    "def _resolve_user_email(user_id: Optional[str], w: WorkspaceClient) -> Optional[str]:\n",
    "    \"\"\"Resolves a user ID to an email address using the SCIM Users API.\"\"\"\n",
    "    if not user_id or user_id in (\"None\", \"0\"):\n",
    "        return None\n",
    "    \n",
    "    if user_id in USER_CACHE:\n",
    "        return USER_CACHE[user_id]\n",
    "    \n",
    "    try:\n",
    "        user: User = w.users.get(user_id)\n",
    "        email = user.user_name\n",
    "        USER_CACHE[user_id] = email\n",
    "        return email\n",
    "    except Exception:\n",
    "        # Return a placeholder if user lookup fails\n",
    "        return f\"ID_{user_id}\"\n",
    "\n",
    "def _get_schema() -> StructType:\n",
    "    return StructType([\n",
    "        StructField(\"space_id\", StringType(), True),\n",
    "        StructField(\"space_name\", StringType(), True),\n",
    "        StructField(\"message_id\", StringType(), True),\n",
    "        StructField(\"conversation_id\", StringType(), True),\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"user_email\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"created_timestamp\", LongType(), True),\n",
    "        StructField(\"last_updated_timestamp\", LongType(), True),\n",
    "        StructField(\"user_question\", StringType(), True),\n",
    "        StructField(\"ai_response\", StringType(), True),\n",
    "        StructField(\"sql_query\", StringType(), True),\n",
    "        StructField(\"statement_id\", StringType(), True),\n",
    "        StructField(\"suggested_questions\", StringType(), True),\n",
    "        StructField(\"num_attachments\", IntegerType(), True),\n",
    "        StructField(\"feedback_rating\", StringType(), True),\n",
    "        StructField(\"error_type\", StringType(), True),\n",
    "        StructField(\"error_message\", StringType(), True),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7169a669-09fd-4808-9a11-b7c51d306b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can test the function by running the function on a Genie space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bfa2c4-f981-487a-a9bc-324f86ce3770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Usage Example ---\n",
    "\n",
    "# Define your Space ID\n",
    "space_id = \"XXX\" # Replace with your actual Space ID\n",
    "\n",
    "# Run the function\n",
    "df = get_genie_observability_table(space_id)\n",
    "\n",
    "# Display results\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5092fae2-0439-4e4a-9e4c-7b18b4c1dabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below is a simple scripts that runs this function on ALL your genie spaces (currently limited to 10). \n",
    "\n",
    "Note: You may not have permssion to view messages of certain Genie spaces as you may not have access to the underlying tables. In this case you will see a lot of NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746bd481-9213-4280-aef4-2ce086cfbc00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Fetch all Genie spaces\n",
    "print(\"ðŸ” Fetching all Genie spaces...\")\n",
    "spaces = []\n",
    "page_token = None\n",
    "\n",
    "while True:\n",
    "    response = w.genie.list_spaces(page_token=page_token)\n",
    "    for s in response.spaces:\n",
    "        spaces.append({\n",
    "            \"space_id\": getattr(s, \"space_id\", None),\n",
    "            \"name\": getattr(s, \"title\", None),\n",
    "            \"description\": getattr(s, \"description\", None),\n",
    "            \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "        })\n",
    "    if not response.next_page_token or response.next_page_token == \"\":\n",
    "        break\n",
    "    page_token = response.next_page_token\n",
    "\n",
    "print(f\"âœ“ Found {len(spaces)} Genie spaces\")\n",
    "\n",
    "# Limit to 10 spaces\n",
    "MAX_SPACES = 10\n",
    "if len(spaces) > MAX_SPACES:\n",
    "    print(f\"âš  Limiting processing to first {MAX_SPACES} spaces\\n\")\n",
    "    spaces = spaces[:MAX_SPACES]\n",
    "else:\n",
    "    print()\n",
    "\n",
    "# Collect observability data from all spaces\n",
    "all_dfs = []\n",
    "\n",
    "for i, space in enumerate(spaces, 1):\n",
    "    space_id = space['space_id']\n",
    "    space_name = space['name']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{i}/{len(spaces)}] Processing Space: {space_name}\")\n",
    "    print(f\"Space ID: {space_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Get observability data for this space\n",
    "        df_space = get_genie_observability_table(space_id)\n",
    "        \n",
    "        if df_space.count() > 0:\n",
    "            all_dfs.append(df_space)\n",
    "            print(f\"\\nâœ“ Successfully extracted {df_space.count()} messages from {space_name}\")\n",
    "        else:\n",
    "            print(f\"\\nâš  No messages found in {space_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error processing space {space_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_dfs:\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"ðŸ“Š COMBINING ALL RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Union all DataFrames\n",
    "    df = all_dfs[0]\n",
    "    for df_next in all_dfs[1:]:\n",
    "        df = df.union(df_next)\n",
    "    \n",
    "    total_messages = df.count()\n",
    "    total_spaces = df.select(\"space_id\").distinct().count()\n",
    "    \n",
    "    print(f\"\\nâœ… SUCCESS!\")\n",
    "    print(f\"   Total Spaces Processed: {total_spaces}\")\n",
    "    print(f\"   Total Messages Extracted: {total_messages}\")\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    display(df)\n",
    "else:\n",
    "    print(\"\\nâš  No data found across any Genie spaces\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0aa111e-7973-42cd-ad46-5c3b80131761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save the table as a delta table and add a nice detailed table description!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2b6e6e-a659-4c8a-82bc-378301c18104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the observability data to a Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.genie_observability_main_table\")\n",
    "\n",
    "# Add a detailed table description\n",
    "spark.sql(f\"\"\"\n",
    "  COMMENT ON TABLE {catalog_name}.{schema_name}.genie_observability_main_table IS \n",
    "  'Comprehensive observability table for Genie AI/BI spaces. Contains detailed message-level data including user questions, AI responses, generated SQL queries, execution metadata, user feedback ratings, and error information. Used for monitoring Genie usage, analyzing query patterns, and tracking user engagement across all accessible Genie spaces.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3cdb112-7eb8-4f1d-9938-26a223da8e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We successfully created the **genie_observability_main_table** table! Now lets proceed to create the **dbsql_cost_per_query_table** table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b591ee-8c1c-4110-a722-a9ae595e5af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**An Aside: How we approached attributing costs from a Genie Space.**\n",
    "\n",
    "The Strategy: \n",
    "- We first run a SQL comman that returns a table containing all SQL queries executed across all workspaces along with the cost attributed to each query. This allows us to understand how much each query actually costed.\n",
    "- Each query in this table will have a statement ID unique to that query.\n",
    "- During cost analysis, we can augment the genie_observability_main_table table by performing a left join on statement_id to get a final table ready for cost analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "319b2f34-a68f-4358-8877-9a5cc0454a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The SQL code below is an adaptation of the Materialised View definition. For the MV please see dbsql_cost_per_query_mv.sql file in this folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98f58d9-1b88-4635-84ba-a992fcec6eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.dbsql_cost_per_query_table \n",
    "PARTITIONED BY (query_start_hour, workspace_id)\n",
    "AS\n",
    "WITH \n",
    "-- Must make sure the time window the MV is built on has enough data from ALL 3 tables to generate accurate results (both starts and end time ranges)\n",
    "table_boundaries AS (\n",
    "SELECT \n",
    "(SELECT MAX(event_time) FROM system.compute.warehouse_events) AS max_events_ts,\n",
    "(SELECT MAX(end_time) FROM system.query.history) AS max_query_end_ts,\n",
    "(SELECT MAX(usage_end_time) FROM system.billing.usage) AS max_billing_ts,\n",
    "(SELECT MIN(event_time) FROM system.compute.warehouse_events) AS min_event_ts,\n",
    "(SELECT MIN(start_time) FROM system.query.history) AS min_query_start_ts,\n",
    "(SELECT MIN(usage_end_time) FROM system.billing.usage) AS min_billing_ts,\n",
    "date_trunc('HOUR', LEAST(max_events_ts, max_query_end_ts, max_billing_ts)) AS selected_end_time,\n",
    "(date_trunc('HOUR', GREATEST(min_event_ts, min_query_start_ts, min_billing_ts)) + INTERVAL 1 HOUR)::timestamp AS selected_start_time\n",
    "),\n",
    "\n",
    "----===== Warehouse Level Calculations =====-----\n",
    "cpq_warehouse_usage AS (\n",
    "  SELECT\n",
    "    usage_metadata.warehouse_id AS warehouse_id,\n",
    "    *\n",
    "  FROM\n",
    "    system.billing.usage AS u\n",
    "  WHERE\n",
    "    usage_metadata.warehouse_id IS NOT NULL\n",
    "    AND usage_start_time >= (SELECT MIN(selected_start_time) FROM table_boundaries)\n",
    "    AND usage_end_time <= (SELECT MAX(selected_end_time) FROM table_boundaries)\n",
    "),\n",
    "\n",
    "prices AS (\n",
    "  select coalesce(price_end_time, date_add(current_date, 1)) as coalesced_price_end_time, *\n",
    "  from system.billing.list_prices\n",
    "  where currency_code = 'USD'\n",
    "),\n",
    "\n",
    "filtered_warehouse_usage AS (\n",
    "    -- Warehouse usage is aggregated hourly, that will be the base assumption and grain of allocation moving forward. \n",
    "    -- Assume no duplicate records\n",
    "    SELECT \n",
    "      u.warehouse_id warehouse_id,\n",
    "      date_trunc('HOUR',u.usage_start_time) AS usage_start_hour,\n",
    "      date_trunc('HOUR',u.usage_end_time) AS usage_end_hour,\n",
    "      u.usage_quantity AS dbus,\n",
    "      (\n",
    "        CAST(p.pricing.effective_list.default AS FLOAT) * dbus\n",
    "      ) AS usage_dollars\n",
    "    FROM\n",
    "      cpq_warehouse_usage AS u\n",
    "        left join prices as p\n",
    "        on u.sku_name=p.sku_name\n",
    "        and u.usage_unit=p.usage_unit\n",
    "        and (u.usage_end_time between p.price_start_time and p.coalesced_price_end_time)\n",
    "),\n",
    "\n",
    "table_bound_expld AS \n",
    "(\n",
    "select timestampadd(hour, h, selected_start_time) as selected_hours\n",
    "  from table_boundaries\n",
    "  join lateral explode(sequence(0, timestampdiff(hour, selected_start_time, selected_end_time), 1)) as t (h)\n",
    "),\n",
    "\n",
    "----===== Query Level Calculations =====-----\n",
    "cpq_warehouse_query_history AS (\n",
    "  SELECT\n",
    "    account_id,\n",
    "    workspace_id,\n",
    "    statement_id,\n",
    "    executed_by,\n",
    "    statement_text,\n",
    "    compute.warehouse_id AS warehouse_id,\n",
    "    execution_status,\n",
    "    COALESCE(client_application, 'Unknown') AS client_application,\n",
    "    (COALESCE(CAST(total_task_duration_ms AS FLOAT) / 1000, 0) +\n",
    "      COALESCE(CAST(result_fetch_duration_ms AS FLOAT) / 1000, 0) +\n",
    "      COALESCE(CAST(compilation_duration_ms AS FLOAT) / 1000, 0)\n",
    "    )  AS query_work_task_time,\n",
    "    start_time,\n",
    "    end_time,\n",
    "    timestampadd(MILLISECOND , coalesce(waiting_at_capacity_duration_ms, 0) + coalesce(waiting_for_compute_duration_ms, 0) + coalesce(compilation_duration_ms, 0), start_time) AS query_work_start_time,\n",
    "    timestampadd(MILLISECOND, coalesce(result_fetch_duration_ms, 0), end_time) AS query_work_end_time,\n",
    "    -- NEW - Query source\n",
    "    CASE\n",
    "      WHEN query_source.job_info.job_id IS NOT NULL THEN 'JOB'\n",
    "      WHEN query_source.legacy_dashboard_id IS NOT NULL THEN 'LEGACY DASHBOARD'\n",
    "      WHEN query_source.dashboard_id IS NOT NULL THEN 'AI/BI DASHBOARD'\n",
    "      WHEN query_source.alert_id IS NOT NULL THEN 'ALERT'\n",
    "      WHEN query_source.notebook_id IS NOT NULL THEN 'NOTEBOOK'\n",
    "      WHEN query_source.sql_query_id IS NOT NULL THEN 'SQL QUERY'\n",
    "      WHEN query_source.genie_space_id IS NOT NULL THEN 'GENIE SPACE'\n",
    "      WHEN client_application IS NOT NULL THEN client_application\n",
    "      ELSE 'UNKNOWN'\n",
    "    END AS query_source_type,\n",
    "    COALESCE(\n",
    "      query_source.job_info.job_id,\n",
    "      query_source.legacy_dashboard_id,\n",
    "      query_source.dashboard_id,\n",
    "      query_source.alert_id,\n",
    "      query_source.notebook_id,\n",
    "      query_source.sql_query_id,\n",
    "      query_source.genie_space_id,\n",
    "      'UNKNOWN'\n",
    "    ) AS query_source_id\n",
    "  FROM\n",
    "    system.query.history AS h\n",
    "  WHERE\n",
    "    statement_type IS NOT NULL\n",
    "    -- If query touches the boundaries at all, we will divy it up\n",
    "    AND start_time < (SELECT selected_end_time FROM table_boundaries)\n",
    "    AND end_time > (SELECT selected_start_time FROM table_boundaries)\n",
    "    AND total_task_duration_ms > 0 --exclude metadata operations\n",
    "     and compute.warehouse_id is not null -- = 'd13162f928a069c7'\n",
    ")\n",
    "  ,  cte_warehouse as\n",
    "(\n",
    "  select warehouse_id, min(query_work_start_time) as min_start_time\n",
    "    from cpq_warehouse_query_history\n",
    "group by warehouse_id\n",
    ")\n",
    ",\n",
    "--- Warehouse + Query Level level allocation\n",
    "window_events AS (\n",
    "    SELECT\n",
    "        warehouse_id,\n",
    "        event_type,\n",
    "        event_time,\n",
    "        cluster_count AS cluster_count,\n",
    "        CASE\n",
    "            WHEN cluster_count = 0 THEN 'OFF'\n",
    "            WHEN cluster_count > 0 THEN 'ON'\n",
    "        END AS warehouse_state\n",
    "    FROM system.compute.warehouse_events AS we\n",
    "    -- Only get window events for when we have query history, otherwise, not usable\n",
    "    WHERE warehouse_id in (SELECT warehouse_id FROM cte_warehouse)\n",
    "    AND event_time >= (SELECT timestampadd(day, -1, selected_start_time) FROM table_boundaries)\n",
    "    AND event_time <= (SELECT selected_end_time FROM table_boundaries)\n",
    ")\n",
    "  ,  cte_agg_events_prep as\n",
    "(\n",
    "select warehouse_id\n",
    "      , warehouse_state\n",
    "      , event_time\n",
    "      , row_number() over W1\n",
    "      - row_number() over W2 as grp\n",
    "  from window_events\n",
    "window W1 as (partition by warehouse_id                  order by event_time asc)\n",
    "      , W2 as (partition by warehouse_id, warehouse_state order by event_time asc)\n",
    ")\n",
    "  ,  cte_agg_events as\n",
    "(\n",
    "  select warehouse_id\n",
    "       , warehouse_state                                           as window_state\n",
    "       , min(event_time)                                           as event_window_start\n",
    "       , lead(min(event_time), 1, selected_end_time) over W as event_window_end\n",
    "    from cte_agg_events_prep\n",
    "    join table_boundaries\n",
    "group by warehouse_id\n",
    "       , warehouse_state\n",
    "       , grp\n",
    "       , selected_end_time\n",
    "  window W as (partition by warehouse_id order by min(event_time) asc)\n",
    ")\n",
    "  ,  cte_all_events as\n",
    "(\n",
    "select warehouse_id\n",
    "     , window_state\n",
    "     , date_trunc('second', event_window_start) as event_window_start\n",
    "     , date_trunc('second', event_window_end  ) as event_window_end\n",
    "  from cte_agg_events\n",
    " where date_trunc('second', event_window_start) < date_trunc('second', event_window_end)\n",
    " --and date_trunc('second', event_window_start) >= timestamp '2024-11-14 09:00:00'\n",
    ")\n",
    "  ,  cte_queries_event_cnt as\n",
    "(\n",
    "  select warehouse_id\n",
    "       , case num\n",
    "           when 1\n",
    "           then date_trunc('second', query_work_start_time)\n",
    "           else timestampadd(second, case when date_trunc('second', query_work_start_time) = date_trunc('second', query_work_end_time) then 1 else 0 end, date_trunc('second', query_work_end_time))\n",
    "         end as query_event_time\n",
    "       , sum(num) as num_queries\n",
    "    from cpq_warehouse_query_history\n",
    "    join lateral explode(array(1, -1)) as t (num)\n",
    "group by 1, 2\n",
    ")\n",
    "  ,  cte_raw_history as\n",
    "(\n",
    "select warehouse_id\n",
    "     , query_event_time                                           as query_start\n",
    "     , lead(query_event_time, 1, selected_end_time) over W as query_end\n",
    "     , sum(num_queries) over W as queries_active\n",
    "  from cte_queries_event_cnt\n",
    "  join table_boundaries\n",
    "window W as (partition by warehouse_id order by query_event_time asc)\n",
    ")\n",
    "  ,  cte_raw_history_byday as\n",
    "(\n",
    "  select /*+ repartition(64, warehouse_id, query_start_dt) */\n",
    "         warehouse_id\n",
    "       , case num\n",
    "           when 0\n",
    "           then query_start\n",
    "           else timestampadd(day, num, query_start::date)\n",
    "         end::date as query_start_dt\n",
    "       , case num\n",
    "           when 0\n",
    "           then query_start\n",
    "           else timestampadd(day, num, query_start::date)\n",
    "         end as query_start\n",
    "       , case num\n",
    "           when timestampdiff(day, query_start::date, query_end::date)\n",
    "           then query_end\n",
    "           else timestampadd(day, num + 1, query_start::date)\n",
    "         end as query_end\n",
    "       , queries_active\n",
    "    from cte_raw_history\n",
    "    join lateral explode(sequence(0, timestampdiff(day, query_start::date, query_end::date), 1)) as t (num)\n",
    ")\n",
    "  ,  cte_all_time_union as\n",
    "(\n",
    "select warehouse_id\n",
    "     , case num when 1 then event_window_start else event_window_end end ts_start\n",
    "  from cte_all_events\n",
    "  join lateral explode(array(1, -1)) as t (num)\n",
    " union \n",
    "select warehouse_id\n",
    "     , case num when 1 then query_start else query_end end\n",
    "  from cte_raw_history_byday\n",
    "  join lateral explode(array(1, -1)) as t (num)\n",
    " union\n",
    "select warehouse_id, selected_hours\n",
    "  from cte_warehouse\n",
    "  join table_bound_expld on true\n",
    "-- where selected_hours >= timestampadd(day, -1, min_start_time)\n",
    ")\n",
    "  ,  cte_periods as\n",
    "(\n",
    "select /*+ repartition(64, warehouse_id, dt_start) */\n",
    "       warehouse_id\n",
    "     , ts_start::date as dt_start\n",
    "     , ts_start\n",
    "     , lead(ts_start, 1, selected_end_time) over W as ts_end\n",
    "  from cte_all_time_union\n",
    "  join table_boundaries\n",
    "window W as (partition by warehouse_id order by ts_start asc)\n",
    ")\n",
    "  ,  cte_merge_periods as\n",
    "(\n",
    "    select /*+ broadcast(r) */\n",
    "           p.warehouse_id\n",
    "         , date_trunc('hour', p.ts_start) as ts_hour\n",
    "         , sum(timestampdiff(second, p.ts_start, p.ts_end)) as duration\n",
    "         , case\n",
    "             when e.window_state = 'OFF'\n",
    "               or e.window_state is null\n",
    "             then 'OFF'\n",
    "             when r.queries_active > 0\n",
    "             then 'UTILIZED'\n",
    "             else 'ON_IDLE'\n",
    "           end as utilization_flag\n",
    "      from cte_periods           as p\n",
    " left join cte_all_events        as e  on e.warehouse_id       = p.warehouse_id\n",
    "                                      and e.event_window_start < p.ts_end\n",
    "                                      and e.event_window_end   > p.ts_start\n",
    " left join cte_raw_history_byday as r  on r.warehouse_id       = p.warehouse_id\n",
    "                                      and r.query_start_dt     = p.dt_start\n",
    "                                      and r.query_start        < p.ts_end\n",
    "                                      and r.query_end          > p.ts_start\n",
    "                                      and r.queries_active     > 0\n",
    "                                      and e.window_state      <> 'OFF'\n",
    "     where p.ts_start < p.ts_end\n",
    "  group by all\n",
    "),\n",
    "\n",
    "utilization_by_warehouse AS (\n",
    "  select warehouse_id\n",
    "       , ts_hour as warehouse_hour\n",
    "       , coalesce(sum(duration) filter(where utilization_flag = 'UTILIZED'), 0) as utilized_seconds\n",
    "       , coalesce(sum(duration) filter(where utilization_flag = 'ON_IDLE' ), 0) as idle_seconds\n",
    "       , coalesce(sum(duration) filter(where utilization_flag = 'OFF'      ), 0) as off_seconds\n",
    "       , coalesce(sum(duration), 0) as total_seconds\n",
    "       , try_divide(utilized_seconds, utilized_seconds + idle_seconds)::decimal(3,2) as utilization_proportion\n",
    "    from cte_merge_periods\n",
    "group by warehouse_id\n",
    "       , ts_hour\n",
    "),\n",
    "\n",
    "cleaned_warehouse_info AS (\n",
    "  SELECT\n",
    "  wu.warehouse_id,\n",
    "  wu.usage_start_hour AS hour_bucket,\n",
    "  wu.dbus,\n",
    "  wu.usage_dollars,\n",
    "  ut.utilized_seconds,\n",
    "  ut.idle_seconds,\n",
    "  ut.total_seconds,\n",
    "  ut.utilization_proportion\n",
    "  FROM filtered_warehouse_usage wu\n",
    "  LEFT JOIN utilization_by_warehouse AS ut ON wu.warehouse_id = ut.warehouse_id -- Join on calculation grain - warehouse/hour\n",
    "    AND wu.usage_start_hour = ut.warehouse_hour\n",
    "),\n",
    "\n",
    "hour_intervals AS (\n",
    "  -- Generate valid hourly buckets for each query\n",
    "  SELECT\n",
    "    statement_id,\n",
    "    warehouse_id,\n",
    "    query_work_start_time,\n",
    "    query_work_end_time,\n",
    "    query_work_task_time,\n",
    "    explode(\n",
    "      sequence(\n",
    "        0,\n",
    "        floor((UNIX_TIMESTAMP(query_work_end_time) - UNIX_TIMESTAMP(date_trunc('hour', query_work_start_time))) / 3600)\n",
    "      )\n",
    "    ) AS hours_interval,\n",
    "    timestampadd(hour, hours_interval, date_trunc('hour', query_work_start_time)) AS hour_bucket\n",
    "  FROM\n",
    "    cpq_warehouse_query_history\n",
    "),\n",
    "\n",
    "statement_proportioned_work AS (\n",
    "    SELECT * , \n",
    "        GREATEST(0,\n",
    "          UNIX_TIMESTAMP(LEAST(query_work_end_time, timestampadd(hour, 1, hour_bucket))) -\n",
    "          UNIX_TIMESTAMP(GREATEST(query_work_start_time, hour_bucket))\n",
    "        ) AS overlap_duration,\n",
    "        CASE WHEN CAST(query_work_end_time AS DOUBLE) - CAST(query_work_start_time AS DOUBLE) = 0\n",
    "        THEN 0\n",
    "        ELSE query_work_task_time * (overlap_duration / (CAST(query_work_end_time AS DOUBLE) - CAST(query_work_start_time AS DOUBLE)))\n",
    "        END AS proportional_query_work\n",
    "    FROM hour_intervals\n",
    "),\n",
    "\n",
    "\n",
    "attributed_query_work_all AS (\n",
    "    SELECT\n",
    "      statement_id,\n",
    "      hour_bucket,\n",
    "      warehouse_id,\n",
    "      SUM(proportional_query_work) AS attributed_query_work\n",
    "    FROM\n",
    "      statement_proportioned_work\n",
    "    GROUP BY\n",
    "      statement_id,\n",
    "      warehouse_id,\n",
    "      hour_bucket\n",
    "),\n",
    "\n",
    "--- Cost Attribution\n",
    "warehouse_time as (\n",
    "  select\n",
    "    warehouse_id,\n",
    "    hour_bucket,\n",
    "    SUM(attributed_query_work) as total_work_done_on_warehouse\n",
    "  from\n",
    "    attributed_query_work_all\n",
    "  group by\n",
    "    warehouse_id, hour_bucket\n",
    "),\n",
    "\n",
    "-- Create statement_id / hour bucket allocated combinations\n",
    "history AS (\n",
    "  SELECT\n",
    "    a.*,\n",
    "    b.total_work_done_on_warehouse,\n",
    "    CASE\n",
    "      WHEN attributed_query_work = 0 THEN NULL\n",
    "      ELSE attributed_query_work / total_work_done_on_warehouse\n",
    "    END AS proportion_of_warehouse_time_used_by_query\n",
    "  FROM attributed_query_work_all a\n",
    "    inner join warehouse_time b on a.warehouse_id = b.warehouse_id\n",
    "              AND a.hour_bucket = b.hour_bucket -- Will only run for completed hours from warehouse usage - nice clean boundary\n",
    "),\n",
    "\n",
    "history_with_pricing AS (\n",
    "  SELECT\n",
    "    h1.*,\n",
    "    wh.dbus AS total_warehouse_period_dbus,\n",
    "    wh.usage_dollars AS total_warehouse_period_dollars,\n",
    "    wh.utilization_proportion AS warehouse_utilization_proportion,\n",
    "    wh.hour_bucket AS warehouse_hour_bucket,\n",
    "    MAX(wh.hour_bucket) OVER() AS warehouse_max_hour_bucket\n",
    "  FROM\n",
    "    history AS h1\n",
    "    LEFT JOIN cleaned_warehouse_info AS wh ON h1.warehouse_id = wh.warehouse_id AND h1.hour_bucket = wh.hour_bucket\n",
    "),\n",
    "\n",
    "-- This is at the statement_id / hour grain (there will be duplicates for each statement for each hour bucket the query spans)\n",
    "\n",
    "query_attribution AS (\n",
    "  SELECT\n",
    "    a.*,\n",
    "    warehouse_max_hour_bucket AS most_recent_billing_hour,\n",
    "    CASE WHEN warehouse_hour_bucket IS NOT NULL THEN 'Has Billing Record' ELSE 'No Billing Record for this hour and warehouse yet available' END AS billing_record_check,\n",
    "    CASE\n",
    "      WHEN total_work_done_on_warehouse = 0 THEN NULL\n",
    "      ELSE attributed_query_work / total_work_done_on_warehouse\n",
    "    END AS query_task_time_proportion,\n",
    "\n",
    "    (warehouse_utilization_proportion * total_warehouse_period_dollars) * query_task_time_proportion  AS query_attributed_dollars_estimation,\n",
    "    (warehouse_utilization_proportion * total_warehouse_period_dbus) * query_task_time_proportion  AS query_attributed_dbus_estimation\n",
    "  FROM\n",
    "    history_with_pricing a\n",
    ")\n",
    "\n",
    "-- Final Output\n",
    "select\n",
    "      qq.statement_id,\n",
    "      FIRST(qq.query_source_id) AS query_source_id,\n",
    "      FIRST(qq.query_source_type) AS query_source_type,\n",
    "      FIRST(qq.client_application) AS client_application,\n",
    "      FIRST(qq.executed_by) AS executed_by,\n",
    "      FIRST(qq.warehouse_id) AS warehouse_id,\n",
    "      FIRST(qq.statement_text) AS statement_text,\n",
    "      FIRST(qq.workspace_id) AS workspace_id,\n",
    "      COLLECT_LIST(NAMED_STRUCT('hour_bucket', qa.hour_bucket, 'hour_attributed_cost', query_attributed_dollars_estimation, 'hour_attributed_dbus', query_attributed_dbus_estimation)) AS statement_hour_bucket_costs,\n",
    "      FIRST(qq.start_time) AS start_time,\n",
    "      FIRST(qq.end_time) AS end_time,\n",
    "      FIRST(qq.query_work_start_time) AS query_work_start_time,\n",
    "      FIRST(qq.query_work_end_time) AS query_work_end_time,\n",
    "      COALESCE(timestampdiff(MILLISECOND, FIRST(qq.start_time), FIRST(qq.end_time))/1000, 0) AS duration_seconds,\n",
    "      COALESCE(timestampdiff(MILLISECOND, FIRST(qq.query_work_start_time), FIRST(qq.query_work_end_time))/1000, 0) AS query_work_duration_seconds,\n",
    "      FIRST(query_work_task_time) AS query_work_task_time_seconds,\n",
    "      SUM(query_attributed_dollars_estimation) AS query_attributed_dollars_estimation,\n",
    "      SUM(query_attributed_dbus_estimation) AS query_attributed_dbus_estimation,\n",
    "      FIRST(CASE\n",
    "        WHEN query_source_type = 'JOB' THEN CONCAT('/jobs/', query_source_id)\n",
    "        WHEN query_source_type = 'SQL QUERY' THEN CONCAT('/sql/queries/', query_source_id)\n",
    "        WHEN query_source_type = 'AI/BI DASHBOARD' THEN CONCAT('/sql/dashboardsv3/', query_source_id)\n",
    "        WHEN query_source_type = 'LEGACY DASHBOARD' THEN CONCAT('/sql/dashboards/', query_source_id)\n",
    "        WHEN query_source_type = 'ALERTS' THEN CONCAT('/sql/alerts/', query_source_id)\n",
    "        WHEN query_source_type = 'GENIE SPACE' THEN CONCAT('/genie/rooms/', query_source_id)\n",
    "        WHEN query_source_type = 'NOTEBOOK' THEN CONCAT('/editor/notebooks/', query_source_id)\n",
    "        ELSE ''\n",
    "      END) as url_helper,\n",
    "      FIRST(CONCAT('/sql/history?uiQueryProfileVisible=true&queryId=', qq.statement_id)) AS query_profile_url,\n",
    "       FIRST(most_recent_billing_hour) AS most_recent_billing_hour,\n",
    "       FIRST(billing_record_check) AS billing_record_check,\n",
    "       date_trunc('HOUR', FIRST(qq.start_time)) AS query_start_hour\n",
    "      from query_attribution qa\n",
    "      LEFT JOIN cpq_warehouse_query_history AS qq ON qa.statement_id = qq.statement_id -- creating dups of the objects but just re-aggregating\n",
    "            AND qa.warehouse_id = qq.warehouse_id\n",
    "      GROUP BY qq.statement_id;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074c61ec-b0cc-459d-b0f8-28cac4652279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{catalog_name}.{schema_name}.dbsql_cost_per_query_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec6ffa4-9c2b-4325-b9dd-a0fb8ff5fbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a detailed table description\n",
    "spark.sql(f\"\"\"\n",
    "  COMMENT ON TABLE {catalog_name}.{schema_name}.dbsql_cost_per_query_table IS \n",
    "  'This table contains information about the cost of each query executed in Databricks SQL.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a643c8a5-b915-4c0e-89a5-a052e022f6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analytics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dcec5d8-896c-4062-aca4-5fb4d86ee82f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With both critical tables created, you can now join them to generate actionable insights! Use these SQL queries as a foundation for custom dashboards and Meta-Genie spaces. For inspiration, explore the example datasets and dashboards available in this folder and navigate to the Genie Drill Down tab - src/dashboards/lh_adoption_dashboard.lvdash.json\n",
    "\n",
    "Some example SQL queries (recommended by Databricks Assistant!) are below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f6e4e89-cecf-4cf6-b85d-a15063192807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cost Analysis of Genie-Generated Queries: Join both tables to see how much Genie AI queries cost compared to other query sources, and identify the most expensive AI-generated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e32a9a8-76bc-4765-bef5-acc8eee9bca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    g.space_name,\n",
    "    g.user_email,\n",
    "    g.user_question,\n",
    "    c.query_attributed_dollars_estimation,\n",
    "    c.query_attributed_dbus_estimation,\n",
    "    c.start_time,\n",
    "    c.end_time\n",
    "FROM kg_test_workspace.default.genie_observability_main_table g\n",
    "LEFT JOIN kg_test_workspace.default.dbsql_cost_per_query_table c\n",
    "    ON g.statement_id = c.statement_id\n",
    "WHERE c.query_attributed_dollars_estimation IS NOT NULL\n",
    "ORDER BY c.query_attributed_dollars_estimation DESC\n",
    "LIMIT 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7774190-c773-4608-a814-9ad6cff06c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Genie Success Rate & Error Analysis: Analyze which Genie spaces have the highest error rates and what types of errors occur most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c768e7e-1753-44f1-b6e4-06469e248e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    space_name,\n",
    "    status,\n",
    "    error_type,\n",
    "    COUNT(*) as message_count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY space_name), 2) as pct_of_space\n",
    "FROM kg_test_workspace.default.genie_observability_main_table\n",
    "GROUP BY space_name, status, error_type\n",
    "ORDER BY space_name, message_count DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fe49d5c-6385-4f42-ad72-e5d95ad97a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### User Engagement & Feedback Patterns: Identify which users are most active with Genie and how they rate the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5196f36-fac7-4c53-a281-801e6636a2d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    user_email,\n",
    "    COUNT(DISTINCT conversation_id) as total_conversations,\n",
    "    COUNT(message_id) as total_messages,\n",
    "    COUNT(CASE WHEN feedback_rating = 'positive' THEN 1 END) as positive_feedback,\n",
    "    COUNT(CASE WHEN feedback_rating = 'negative' THEN 1 END) as negative_feedback,\n",
    "    ROUND(AVG(duration_seconds), 2) as avg_response_time_sec\n",
    "FROM kg_test_workspace.default.genie_observability_main_table\n",
    "WHERE user_email IS NOT NULL\n",
    "GROUP BY user_email\n",
    "ORDER BY total_messages DESC\n",
    "LIMIT 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a42aad10-3c9d-4a0f-a62b-4a34c239cd4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Query Performance by Source Type: Compare query performance and costs across different query sources (Genie vs dashboards vs notebooks vs direct queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b4d45e-05ec-43b9-9cf2-40c89dcfd12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    query_source_type,\n",
    "    client_application,\n",
    "    COUNT(*) as query_count,\n",
    "    ROUND(AVG(query_attributed_dollars_estimation), 4) as avg_cost_dollars,\n",
    "    ROUND(SUM(query_attributed_dollars_estimation), 2) as total_cost_dollars,\n",
    "    ROUND(AVG(TIMESTAMPDIFF(SECOND, start_time, end_time)), 2) as avg_duration_sec\n",
    "FROM kg_test_workspace.default.dbsql_cost_per_query_table\n",
    "WHERE query_attributed_dollars_estimation IS NOT NULL\n",
    "GROUP BY query_source_type, client_application\n",
    "ORDER BY total_cost_dollars DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04023e4e-27e9-44fa-8b70-8b2c2e677119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time-Based Usage & Cost Trends: Analyze hourly patterns to identify peak usage times and associated costs for both Genie and overall SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdcf0b6d-7ff3-4553-b029-f6a82654498a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    DATE_TRUNC('hour', c.start_time) as hour_bucket,\n",
    "    COUNT(DISTINCT c.statement_id) as total_queries,\n",
    "    COUNT(DISTINCT g.message_id) as genie_queries,\n",
    "    ROUND(SUM(c.query_attributed_dollars_estimation), 2) as total_cost,\n",
    "    ROUND(AVG(c.query_attributed_dollars_estimation), 4) as avg_cost_per_query\n",
    "FROM kg_test_workspace.default.dbsql_cost_per_query_table c\n",
    "LEFT JOIN kg_test_workspace.default.genie_observability_main_table g\n",
    "    ON c.statement_id = g.statement_id\n",
    "WHERE c.start_time >= CURRENT_DATE - INTERVAL 30 DAYS\n",
    "GROUP BY DATE_TRUNC('hour', c.start_time)\n",
    "ORDER BY hour_bucket DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bea3229f-7bbe-4806-9073-49bca9c600b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This concludes this notebook. We now have created two critical tables that will help us monitor and analyse our genie spaces across multiple dimensions:\n",
    "- Cost attribution and chargeback\n",
    "- Usability and accuracy\n",
    "- Insights to further improve the space by adding more tables, adding SQL examples, trusted assets etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c128ab42-23d0-455e-99fc-a3027bee4156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Take this to the next level\n",
    "You can enhance this notebook by incorporating more advanced techniques such as:\n",
    "- Run topic modelling on user user questions to identify common patterns that can help you improve youe Genie space\n",
    "- Incorporate more advanced cost attribution strategies for charge backs.\n",
    "- Incorporate this notebook as part of your custome Databricks Jobs process or the full observability stack by deploying this repo through Databricks Asset Bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c743a683-8da9-432a-bdc9-c1b3136f14e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a3d31a-cd3d-46a9-8554-36a33c96c2a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "REST API Implementation for get_genie_observability_table function using Python Requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5abb6b6d-6359-4b33-a788-9447d4c970cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# from typing import Dict, Any, List\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, IntegerType\n",
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# # Cache for User Email lookups to prevent redundant API calls\n",
    "# USER_CACHE = {}\n",
    "\n",
    "# def get_genie_observability_table(\n",
    "#     space_id: str, \n",
    "#     databricks_token: str, \n",
    "#     host_url: str,\n",
    "#     include_all_users: bool = True\n",
    "# ) -> 'pyspark.sql.DataFrame':\n",
    "#     \"\"\"\n",
    "#     Fetches and constructs a Spark DataFrame containing observability data for all messages in a Genie space.\n",
    "\n",
    "#     Args:\n",
    "#         space_id (str): The Genie space ID.\n",
    "#         databricks_token (str): Databricks personal access token for authentication.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         include_all_users (bool, optional): Whether to include all users' conversations. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         pyspark.sql.DataFrame: DataFrame with observability records for the specified Genie space.\n",
    "#     \"\"\"\n",
    "#     host_url = host_url.rstrip('/')\n",
    "#     headers = {'Authorization': f'Bearer {databricks_token}', 'Content-Type': 'application/json'}\n",
    "    \n",
    "#     # Step 1: Get space name\n",
    "#     space_url = f\"{host_url}/api/2.0/genie/spaces/{space_id}\"\n",
    "#     space_resp = requests.get(space_url, headers=headers)\n",
    "#     space_resp.raise_for_status()\n",
    "#     space_name = space_resp.json().get('title', f\"Space_{space_id}\")\n",
    "    \n",
    "#     # Step 2: Get conversations\n",
    "#     conversations = _get_all_conversations(space_id, host_url, headers, include_all_users)\n",
    "    \n",
    "#     # Step 3: Extract and Flatten\n",
    "#     records = []\n",
    "#     for conv in conversations:\n",
    "#         messages = _get_all_conversation_messages(space_id, conv['conversation_id'], host_url, headers)\n",
    "#         for msg in messages:\n",
    "#             record = _extract_message_data(msg, space_id, space_name, host_url, headers)\n",
    "#             records.append(record)\n",
    "    \n",
    "#     # Step 4: Create Spark DataFrame\n",
    "#     spark = SparkSession.builder.getOrCreate()\n",
    "#     schema = _get_schema()\n",
    "    \n",
    "#     if not records:\n",
    "#         return spark.createDataFrame([], schema)\n",
    "        \n",
    "#     df = spark.createDataFrame(records, schema=schema)\n",
    "    \n",
    "#     # Convert timestamps to Datetime\n",
    "#     df = df.withColumn(\"created_datetime\", from_unixtime(col(\"created_timestamp\") / 1000).cast(TimestampType())) \\\n",
    "#            .withColumn(\"last_updated_datetime\", from_unixtime(col(\"last_updated_timestamp\") / 1000).cast(TimestampType())) \\\n",
    "#            .orderBy(\"created_timestamp\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def _get_all_conversations(space_id: str, host_url: str, headers: Dict[str, str], \n",
    "#                            include_all: bool) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Retrieves all conversations for a given Genie space, handling pagination.\n",
    "\n",
    "#     Args:\n",
    "#         space_id (str): Genie space ID.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         headers (Dict[str, str]): HTTP headers for authentication.\n",
    "#         include_all (bool): Whether to include all users' conversations.\n",
    "\n",
    "#     Returns:\n",
    "#         List[Dict[str, Any]]: List of conversation metadata dictionaries.\n",
    "#     \"\"\"\n",
    "#     all_conversations = []\n",
    "#     page_token = None\n",
    "    \n",
    "#     while True:\n",
    "#         url = f\"{host_url}/api/2.0/genie/spaces/{space_id}/conversations\"\n",
    "#         params = {'page_size': 100}\n",
    "        \n",
    "#         if page_token:\n",
    "#             params['page_token'] = page_token\n",
    "#         if include_all:\n",
    "#             params['include_all'] = 'true'\n",
    "        \n",
    "#         response = requests.get(url, headers=headers, params=params)\n",
    "#         response.raise_for_status()\n",
    "#         result = response.json()\n",
    "        \n",
    "#         conversations = result.get('conversations', [])\n",
    "#         all_conversations.extend(conversations)\n",
    "        \n",
    "#         page_token = result.get('next_page_token')\n",
    "#         if not page_token:\n",
    "#             break\n",
    "    \n",
    "#     return all_conversations\n",
    "\n",
    "\n",
    "# def _get_all_conversation_messages(space_id: str, conversation_id: str, \n",
    "#                                    host_url: str, headers: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Retrieves all messages from a specific conversation, handling pagination.\n",
    "\n",
    "#     Args:\n",
    "#         space_id (str): Genie space ID.\n",
    "#         conversation_id (str): Conversation ID.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         headers (Dict[str, str]): HTTP headers for authentication.\n",
    "\n",
    "#     Returns:\n",
    "#         List[Dict[str, Any]]: List of message dictionaries.\n",
    "#     \"\"\"\n",
    "#     all_messages = []\n",
    "#     page_token = None\n",
    "    \n",
    "#     while True:\n",
    "#         url = f\"{host_url}/api/2.0/genie/spaces/{space_id}/conversations/{conversation_id}/messages\"\n",
    "#         params = {'page_size': 100}\n",
    "        \n",
    "#         if page_token:\n",
    "#             params['page_token'] = page_token\n",
    "        \n",
    "#         response = requests.get(url, headers=headers, params=params)\n",
    "#         response.raise_for_status()\n",
    "#         result = response.json()\n",
    "        \n",
    "#         messages = result.get('messages', [])\n",
    "#         all_messages.extend(messages)\n",
    "        \n",
    "#         page_token = result.get('next_page_token')\n",
    "#         if not page_token:\n",
    "#             break\n",
    "    \n",
    "#     return all_messages\n",
    "    \n",
    "# def _extract_message_data(message: Dict[str, Any], space_id: str, space_name: str, host_url: str, headers: dict) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Extracts and flattens relevant fields from a Genie message object for observability.\n",
    "\n",
    "#     Args:\n",
    "#         message (Dict[str, Any]): Message dictionary from Genie API.\n",
    "#         space_id (str): Genie space ID.\n",
    "#         space_name (str): Genie space name.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         headers (dict): HTTP headers for authentication.\n",
    "\n",
    "#     Returns:\n",
    "#         Dict[str, Any]: Flattened record with observability fields.\n",
    "#     \"\"\"\n",
    "#     # Resolve User Email\n",
    "#     user_id = str(message.get('user_id')) if message.get('user_id') else None\n",
    "#     user_email = _resolve_email(user_id, host_url, headers)\n",
    "\n",
    "#     record = {\n",
    "#         'space_id': space_id,\n",
    "#         'space_name': space_name,\n",
    "#         'message_id': message.get('message_id'),\n",
    "#         'conversation_id': message.get('conversation_id'),\n",
    "#         'user_id': user_id,\n",
    "#         'user_email': user_email,\n",
    "#         'status': message.get('status'),\n",
    "#         'created_timestamp': message.get('created_timestamp'),\n",
    "#         'last_updated_timestamp': message.get('last_updated_timestamp'),\n",
    "#         'user_question': message.get('content'),\n",
    "#     }\n",
    "    \n",
    "#     ai_responses, sql_queries, statement_ids, suggested_qs = [], [], [], []\n",
    "    \n",
    "#     for att in message.get('attachments', []):\n",
    "#         # Text Attachment\n",
    "#         if att.get('text'):\n",
    "#             ai_responses.append(att['text'].get('content', ''))\n",
    "        \n",
    "#         # Query Attachment (Corrected sibling path)\n",
    "#         query_obj = att.get('query')\n",
    "#         if query_obj:\n",
    "#             sql_queries.append(query_obj.get('query', ''))\n",
    "#             s_id = query_obj.get('statement_id')\n",
    "#             if s_id: statement_ids.append(str(s_id))\n",
    "            \n",
    "#         # Suggested Questions\n",
    "#         if att.get('suggested_questions'):\n",
    "#             suggested_qs.extend(att['suggested_questions'].get('questions', []))\n",
    "\n",
    "#     # Helper to return None instead of empty string\n",
    "#     def join_clean(lst, sep=' | '): return sep.join(filter(None, lst)) if lst else None\n",
    "\n",
    "#     record.update({\n",
    "#         'ai_response': join_clean(ai_responses),\n",
    "#         'sql_query': join_clean(sql_queries),\n",
    "#         'statement_id': join_clean(statement_ids),\n",
    "#         'suggested_questions': join_clean(suggested_qs, sep=', '),\n",
    "#         'num_attachments': len(message.get('attachments', []))\n",
    "#     })\n",
    "    \n",
    "#     # Feedback - Where the \"Review Comments\" live\n",
    "#     feedback = message.get('feedback', {})\n",
    "#     record['feedback_rating'] = feedback.get('rating', 'NONE')\n",
    "        \n",
    "#     # Errors\n",
    "#     error = message.get('error', {})\n",
    "#     record['error_type'] = error.get('type')\n",
    "#     record['error_message'] = error.get('error')\n",
    "    \n",
    "#     return record\n",
    "\n",
    "# def _resolve_email(user_id: str, host_url: str, headers: dict) -> str:\n",
    "#     \"\"\"\n",
    "#     Resolves a Databricks user ID to an email address using the SCIM API, with caching.\n",
    "\n",
    "#     Args:\n",
    "#         user_id (str): Databricks user ID.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         headers (dict): HTTP headers for authentication.\n",
    "\n",
    "#     Returns:\n",
    "#         str: User email if found, or a fallback string.\n",
    "#     \"\"\"\n",
    "#     if not user_id or user_id in [\"None\", \"0\"]: return None\n",
    "#     if user_id in USER_CACHE: return USER_CACHE[user_id]\n",
    "    \n",
    "#     try:\n",
    "#         url = f\"{host_url}/api/2.0/preview/scim/v2/Users/{user_id}\"\n",
    "#         resp = requests.get(url, headers=headers)\n",
    "#         if resp.status_code == 200:\n",
    "#             email = resp.json().get('userName')\n",
    "#             USER_CACHE[user_id] = email\n",
    "#             return email\n",
    "#     except: pass\n",
    "#     return f\"ID_{user_id}\"\n",
    "\n",
    "# def _get_schema() -> StructType:\n",
    "#     \"\"\"\n",
    "#     Returns the schema for the Genie observability Spark DataFrame.\n",
    "\n",
    "#     Returns:\n",
    "#         StructType: Spark schema for observability records.\n",
    "#     \"\"\"\n",
    "#     return StructType([\n",
    "#         StructField(\"space_id\", StringType(), True),\n",
    "#         StructField(\"space_name\", StringType(), True),\n",
    "#         StructField(\"message_id\", StringType(), True),\n",
    "#         StructField(\"conversation_id\", StringType(), True),\n",
    "#         StructField(\"user_id\", StringType(), True),\n",
    "#         StructField(\"user_email\", StringType(), True),\n",
    "#         StructField(\"status\", StringType(), True),\n",
    "#         StructField(\"created_timestamp\", LongType(), True),\n",
    "#         StructField(\"last_updated_timestamp\", LongType(), True),\n",
    "#         StructField(\"user_question\", StringType(), True),\n",
    "#         StructField(\"ai_response\", StringType(), True),\n",
    "#         StructField(\"sql_query\", StringType(), True),\n",
    "#         StructField(\"statement_id\", StringType(), True),\n",
    "#         StructField(\"suggested_questions\", StringType(), True),\n",
    "#         StructField(\"num_attachments\", IntegerType(), True),\n",
    "#         StructField(\"feedback_rating\", StringType(), True),\n",
    "#         StructField(\"error_type\", StringType(), True),\n",
    "#         StructField(\"error_message\", StringType(), True),\n",
    "#     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88cadb84-31a5-4b36-8b1f-0ad5a2406f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Set your Genie space ID\n",
    "# space_id = \"XXX\"  # Replace with your space ID\n",
    "\n",
    "# # Use WorkspaceClient to get host and token\n",
    "# from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# # Initialize the Databricks workspace client\n",
    "# w = WorkspaceClient()\n",
    "\n",
    "# # Retrieve the workspace host URL\n",
    "# host = w.config.host\n",
    "\n",
    "# # Retrieve the API token from the workspace client config\n",
    "# token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# # Fetch Genie observability data for the specified space\n",
    "# df = get_genie_observability_table(space_id, token, host)\n",
    "\n",
    "# # Display the resulting DataFrame\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5697fe63-c82e-442f-a9b7-715e4f1fc2d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from databricks.sdk import WorkspaceClient\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "\n",
    "# # Initialize the Databricks workspace client\n",
    "# w = WorkspaceClient()\n",
    "\n",
    "# # Get authentication parameters\n",
    "# token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "# host = w.config.host\n",
    "\n",
    "# # Fetch all Genie spaces\n",
    "# print(\"ðŸ” Fetching all Genie spaces...\")\n",
    "# spaces = []\n",
    "# page_token = None\n",
    "\n",
    "# while True:\n",
    "#     response = w.genie.list_spaces(page_token=page_token)\n",
    "#     for s in response.spaces:\n",
    "#         spaces.append({\n",
    "#             \"space_id\": getattr(s, \"space_id\", None),\n",
    "#             \"name\": getattr(s, \"title\", None),\n",
    "#             \"description\": getattr(s, \"description\", None),\n",
    "#             \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "#         })\n",
    "#     if not response.next_page_token or response.next_page_token == \"\":\n",
    "#         break\n",
    "#     page_token = response.next_page_token\n",
    "\n",
    "# print(f\"âœ“ Found {len(spaces)} Genie spaces\")\n",
    "\n",
    "# # Limit to 10 spaces\n",
    "# MAX_SPACES = 10\n",
    "# if len(spaces) > MAX_SPACES:\n",
    "#     print(f\"âš  Limiting processing to first {MAX_SPACES} spaces\\n\")\n",
    "#     spaces = spaces[:MAX_SPACES]\n",
    "# else:\n",
    "#     print()\n",
    "\n",
    "# # Collect observability data from all spaces\n",
    "# all_dfs = []\n",
    "\n",
    "# for i, space in enumerate(spaces, 1):\n",
    "#     space_id = space['space_id']\n",
    "#     space_name = space['name']\n",
    "    \n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"[{i}/{len(spaces)}] Processing Space: {space_name}\")\n",
    "#     print(f\"Space ID: {space_id}\")\n",
    "#     print(f\"{'='*80}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Get observability data for this space\n",
    "#         df_space = get_genie_observability_table(space_id, token, host)\n",
    "        \n",
    "#         if df_space.count() > 0:\n",
    "#             all_dfs.append(df_space)\n",
    "#             print(f\"\\nâœ“ Successfully extracted {df_space.count()} messages from {space_name}\")\n",
    "#         else:\n",
    "#             print(f\"\\nâš  No messages found in {space_name}\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ Error processing space {space_name}: {str(e)}\")\n",
    "#         continue\n",
    "\n",
    "# # Combine all DataFrames\n",
    "# if all_dfs:\n",
    "#     print(f\"\\n\\n{'='*80}\")\n",
    "#     print(\"ðŸ“Š COMBINING ALL RESULTS\")\n",
    "#     print(f\"{'='*80}\")\n",
    "    \n",
    "#     # Union all DataFrames\n",
    "#     df = all_dfs[0]\n",
    "#     for df_next in all_dfs[1:]:\n",
    "#         df = df.union(df_next)\n",
    "    \n",
    "#     total_messages = df.count()\n",
    "#     total_spaces = df.select(\"space_id\").distinct().count()\n",
    "    \n",
    "#     print(f\"\\nâœ… SUCCESS!\")\n",
    "#     print(f\"   Total Spaces Processed: {total_spaces}\")\n",
    "#     print(f\"   Total Messages Extracted: {total_messages}\")\n",
    "#     print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "#     display(df)\n",
    "# else:\n",
    "#     print(\"\\nâš  No data found across any Genie spaces\")\n",
    "#     df = None"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "genie_observability_deep_dive",
   "widgets": {
    "catalog_name": {
     "currentValue": "kg_test_workspace",
     "nuid": "4e44dffa-bf52-4452-9c0e-5cee0134286e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "users",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "default",
     "nuid": "80c4b03f-ad5e-4661-a5cb-c56cf9bbd3c0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
