{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aee1651c-4ec6-4cc7-b6e6-df71e494ddef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Genie Space Observability E2E Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a0f9eb-9bdc-4655-8a2d-77fc32419849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Introduction\n",
    "This notebook covers how to leverage Genie REST APIs and Databricks System Tables to create an e2e Genie Observability workflow, complete with a Genie Observability Dashboard and a Meta-Genie space to ask questions about your Genie spaces!\n",
    "\n",
    "Running this notebook will create **Two Delta Tables**; _genie_observability_main_table_ and _genie_cost_analysis_main_table_.\n",
    "\n",
    "- **genie_observability_main_table:** This table consists of ALL the messages sen to your genie spaces along with Genie natural langauge response and the SQL code generated as part of it. There are additional columns that provide the conversation_id, user_feedback,user_email, the statement execution ID linked to the SQL code and much more.\n",
    "- **genie_cost_analysis_main_table:** This table consist of ALL information required to attribute costs to a genie space through Genie space ID. Additional columns include statement_id, user_email, start_time, cost per statement ID and much more. \n",
    "\n",
    "**Notes:**\n",
    "- The entire solution is vibe coded (with human in the loop!), so please verify code before running in prod.\n",
    "- The cost per query attribution logic is borrowed from DBSQL SME blog and dashboard. Link here: https://github.com/CodyAustinDavis/dbsql_sme/tree/main/Observability%20Dashboards%20and%20DBA%20Resources/Observability%20Lakeview%20Dashboard%20Templates/DBSQL%20Warehouse%20Advisor%20With%20Data%20Model \n",
    "\n",
    "**Usage Guidance:**\n",
    "- You can use this notebook as part of the wider Lakehouse Adoption Dashboard and pipeline that can be deployed as part of Databricks Asset Bundles. Equally, you can download this notebook and Dashboard JSON and incorporate it into your own workflow.\n",
    "- We recommend running this notebook in a test environment first and tailor the code as per your preferance.\n",
    "\n",
    "**Pre-requisites:**\n",
    "- You should be a Genie Space Author of atleast one Genie space (CAN MANAGE PERMISSION)\n",
    "- You should have enough permissions to create tables in a schema within a catalog of your choice\n",
    "- SELECT permission on system catalog and usage and billing schema\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4cd870-8b3a-474d-9225-2774b9725c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upgrade to the latest version of databricks_sdk package\n",
    "%pip install databricks_sdk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff69540-d03b-4f4e-9bb8-aab8014e1e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart Python to ensure all libraries are reloaded\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261aa6d3-a8ae-4eb3-8162-19618b71abe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a text widget for catalog_name with default value 'users'\n",
    "dbutils.widgets.text(\"catalog_name\", \"users\")\n",
    "# Create a text widget for schema_name with no default value\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "\n",
    "# Retrieve the value of catalog_name from the widget\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "# Retrieve the value of schema_name from the widget\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "# Ensure both catalog_name and schema_name are provided\n",
    "assert catalog_name and schema_name, \"catalog_name and schema_name must be provided\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb27c2d-e967-40f3-a3d4-d988889958ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get current user information using the workspace client\n",
    "current_user = w.current_user.me()\n",
    "user_name = current_user.user_name\n",
    "\n",
    "# Print the current user's username\n",
    "print(f\"Current user: {user_name}\")\n",
    "\n",
    "# Print the Databricks workspace host URL\n",
    "print(f\"Workspace: {w.config.host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ec0d27-ff1f-4464-a648-7ba92d1a60f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below we list all Genie spaces in the current workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0f699f-c346-4d5b-9754-193c3d26b41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store Genie space metadata\n",
    "spaces = []\n",
    "page_token = None\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Databricks Workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Paginate through all Genie spaces using the SDK\n",
    "while True:\n",
    "    response = w.genie.list_spaces(page_token=page_token)\n",
    "    for s in response.spaces:\n",
    "        # Append relevant space details to the list\n",
    "        spaces.append({\n",
    "            \"space_id\": getattr(s, \"space_id\", None),\n",
    "            \"name\": getattr(s, \"title\", None),\n",
    "            \"description\": getattr(s, \"description\", None),\n",
    "            \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "        })\n",
    "    # Break if there are no more pages\n",
    "    if not response.next_page_token or response.next_page_token == \"\":\n",
    "        break\n",
    "    page_token = response.next_page_token\n",
    "\n",
    "# Convert the list of spaces to a Pandas DataFrame\n",
    "genie_spaces_pdf = pd.DataFrame(spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d99a3ab-9b40-4a6e-b229-8711026c1845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame containing all Genie spaces metadata\n",
    "display(genie_spaces_pdf)\n",
    "print(\"---------------\")\n",
    "print(f\"Total Genie spaces: {len(genie_spaces_pdf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b087ff-9909-4518-9a4d-99bf099ef509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code cell below is where we define the single most important function that allows us to generate the genie_observability_main_table table. You can edit this function as per your requirements or even reverse engineer this to generate newer insights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc06be5-470f-4524-b65a-125f9cf832cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Databricks Python SDK implementation\n",
    "For implementation with python requests library see appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80d2c3a-4661-4996-89bb-8d35a02b3fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Optional\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.iam import User\n",
    "\n",
    "# Cache for user email lookups to prevent redundant API calls\n",
    "USER_CACHE: Dict[str, str] = {}\n",
    "\n",
    "\n",
    "def get_genie_observability_table(space_id: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches and constructs a Spark DataFrame containing observability data \n",
    "    for all messages in a Genie space using the Databricks SDK.\n",
    "    \n",
    "    Args:\n",
    "        space_id: The ID of the Genie space to fetch data from.\n",
    "        \n",
    "    Returns:\n",
    "        A Spark DataFrame with message-level observability data.\n",
    "    \"\"\"\n",
    "    w = WorkspaceClient()\n",
    "    \n",
    "    # Get space details\n",
    "    space = w.genie.get_space(space_id=space_id)\n",
    "    space_name = space.title or f\"Space_{space_id}\"\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    # List all conversations with pagination\n",
    "    conversations = _list_all_conversations(w, space_id)\n",
    "    \n",
    "    for conv in conversations:\n",
    "        # List all messages in the conversation with pagination\n",
    "        messages = _list_all_messages(w, space_id, conv.conversation_id)\n",
    "        \n",
    "        for msg in messages:\n",
    "            record = _extract_message_data(msg, space_id, space_name, w)\n",
    "            records.append(record)\n",
    "    \n",
    "    # Create Spark DataFrame\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    schema = _get_schema()\n",
    "    \n",
    "    if not records:\n",
    "        return spark.createDataFrame([], schema)\n",
    "    \n",
    "    # Convert dicts to tuples in schema field order\n",
    "    ordered_data = [\n",
    "        tuple(r.get(field.name) for field in schema.fields)\n",
    "        for r in records\n",
    "    ]\n",
    "        \n",
    "    df = spark.createDataFrame(ordered_data, schema=schema)\n",
    "    \n",
    "    # Add human-readable datetime columns\n",
    "    df = (df\n",
    "        .withColumn(\"created_datetime\", from_unixtime(col(\"created_timestamp\") / 1000).cast(TimestampType()))\n",
    "        .withColumn(\"last_updated_datetime\", from_unixtime(col(\"last_updated_timestamp\") / 1000).cast(TimestampType()))\n",
    "        .orderBy(\"created_timestamp\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _list_all_conversations(w: WorkspaceClient, space_id: str) -> List:\n",
    "    \"\"\"Fetches all conversations in a space, handling pagination.\"\"\"\n",
    "    conversations = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        response = w.genie.list_conversations(\n",
    "            space_id=space_id,\n",
    "            include_all=True,\n",
    "            page_token=page_token\n",
    "        )\n",
    "        conversations.extend(response.conversations or [])\n",
    "        \n",
    "        page_token = response.next_page_token\n",
    "        if not page_token:\n",
    "            break\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "\n",
    "def _list_all_messages(w: WorkspaceClient, space_id: str, conversation_id: str) -> List:\n",
    "    \"\"\"Fetches all messages in a conversation, handling pagination.\"\"\"\n",
    "    messages = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        response = w.genie.list_conversation_messages(\n",
    "            space_id=space_id,\n",
    "            conversation_id=conversation_id,\n",
    "            page_token=page_token\n",
    "        )\n",
    "        messages.extend(response.messages or [])\n",
    "        \n",
    "        page_token = response.next_page_token\n",
    "        if not page_token:\n",
    "            break\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def _extract_message_data(message, space_id: str, space_name: str, w: WorkspaceClient) -> Dict[str, Any]:\n",
    "    \"\"\"Extracts relevant fields from a GenieMessage into a flat dictionary.\"\"\"\n",
    "    msg_dict = message.as_dict()\n",
    "    \n",
    "    # Extract IDs (API has both 'id' and 'message_id' for legacy compatibility)\n",
    "    msg_id = msg_dict.get('message_id') or msg_dict.get('id')\n",
    "    user_id = msg_dict.get('user_id')\n",
    "    user_email = _resolve_user_email(str(user_id) if user_id else None, w)\n",
    "\n",
    "    record = {\n",
    "        'space_id': space_id,\n",
    "        'space_name': space_name,\n",
    "        'message_id': msg_id,\n",
    "        'conversation_id': msg_dict.get('conversation_id'),\n",
    "        'user_id': str(user_id) if user_id else None,\n",
    "        'user_email': user_email,\n",
    "        'status': str(msg_dict.get('status')) if msg_dict.get('status') else None,\n",
    "        'created_timestamp': msg_dict.get('created_timestamp'),\n",
    "        'last_updated_timestamp': msg_dict.get('last_updated_timestamp'),\n",
    "        'user_question': msg_dict.get('content'),\n",
    "    }\n",
    "    \n",
    "    # Process attachments\n",
    "    ai_responses, sql_queries, statement_ids, suggested_qs = [], [], [], []\n",
    "    attachments = msg_dict.get('attachments') or []\n",
    "    \n",
    "    for att in attachments:\n",
    "        # Text attachment\n",
    "        if text_obj := att.get('text'):\n",
    "            ai_responses.append(text_obj.get('content', ''))\n",
    "        \n",
    "        # Query attachment\n",
    "        if query_obj := att.get('query'):\n",
    "            sql_queries.append(query_obj.get('query', ''))\n",
    "            if stmt_id := query_obj.get('statement_id'):\n",
    "                statement_ids.append(str(stmt_id))\n",
    "        \n",
    "        # Suggested questions attachment\n",
    "        if sq_obj := att.get('suggested_questions'):\n",
    "            suggested_qs.extend(sq_obj.get('questions', []))\n",
    "\n",
    "    def join_non_empty(items: List, sep: str = ' | ') -> Optional[str]:\n",
    "        filtered = list(filter(None, items))\n",
    "        return sep.join(filtered) if filtered else None\n",
    "\n",
    "    record.update({\n",
    "        'ai_response': join_non_empty(ai_responses),\n",
    "        'sql_query': join_non_empty(sql_queries),\n",
    "        'statement_id': join_non_empty(statement_ids),\n",
    "        'suggested_questions': join_non_empty(suggested_qs, ', '),\n",
    "        'num_attachments': len(attachments),\n",
    "    })\n",
    "    \n",
    "    # Feedback rating\n",
    "    feedback = msg_dict.get('feedback') or {}\n",
    "    record['feedback_rating'] = str(feedback.get('rating')) if feedback.get('rating') else 'NONE'\n",
    "    \n",
    "    # Error info\n",
    "    error = msg_dict.get('error')\n",
    "    if error and isinstance(error, dict):\n",
    "        record['error_type'] = error.get('type')\n",
    "        record['error_message'] = error.get('message') or error.get('error')\n",
    "    elif error:\n",
    "        record['error_type'] = 'Unknown'\n",
    "        record['error_message'] = str(error)\n",
    "    else:\n",
    "        record['error_type'] = None\n",
    "        record['error_message'] = None\n",
    "    \n",
    "    return record\n",
    "\n",
    "def _resolve_user_email(user_id: Optional[str], w: WorkspaceClient) -> Optional[str]:\n",
    "    \"\"\"Resolves a user ID to an email address using the SCIM Users API.\"\"\"\n",
    "    if not user_id or user_id in (\"None\", \"0\"):\n",
    "        return None\n",
    "    \n",
    "    if user_id in USER_CACHE:\n",
    "        return USER_CACHE[user_id]\n",
    "    \n",
    "    try:\n",
    "        user: User = w.users.get(user_id)\n",
    "        email = user.user_name\n",
    "        USER_CACHE[user_id] = email\n",
    "        return email\n",
    "    except Exception:\n",
    "        # Return a placeholder if user lookup fails\n",
    "        return f\"ID_{user_id}\"\n",
    "\n",
    "def _get_schema() -> StructType:\n",
    "    return StructType([\n",
    "        StructField(\"space_id\", StringType(), True),\n",
    "        StructField(\"space_name\", StringType(), True),\n",
    "        StructField(\"message_id\", StringType(), True),\n",
    "        StructField(\"conversation_id\", StringType(), True),\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"user_email\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"created_timestamp\", LongType(), True),\n",
    "        StructField(\"last_updated_timestamp\", LongType(), True),\n",
    "        StructField(\"user_question\", StringType(), True),\n",
    "        StructField(\"ai_response\", StringType(), True),\n",
    "        StructField(\"sql_query\", StringType(), True),\n",
    "        StructField(\"statement_id\", StringType(), True),\n",
    "        StructField(\"suggested_questions\", StringType(), True),\n",
    "        StructField(\"num_attachments\", IntegerType(), True),\n",
    "        StructField(\"feedback_rating\", StringType(), True),\n",
    "        StructField(\"error_type\", StringType(), True),\n",
    "        StructField(\"error_message\", StringType(), True),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7169a669-09fd-4808-9a11-b7c51d306b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can test the function by running the function on a Genie space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bfa2c4-f981-487a-a9bc-324f86ce3770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Usage Example ---\n",
    "\n",
    "# Define your Space ID\n",
    "space_id = \"XXX\" # Replace with your actual Space ID\n",
    "\n",
    "# Run the function\n",
    "df = get_genie_observability_table(space_id)\n",
    "\n",
    "# Display results\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5092fae2-0439-4e4a-9e4c-7b18b4c1dabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below is a simple scripts that runs this function on ALL your genie spaces (currently limited to 10). \n",
    "\n",
    "Note: You may not have permssion to view messages of certain Genie spaces as you may not have access to the underlying tables. In this case you will see a lot of NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746bd481-9213-4280-aef4-2ce086cfbc00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Fetch all Genie spaces\n",
    "print(\"ðŸ” Fetching all Genie spaces...\")\n",
    "spaces = []\n",
    "page_token = None\n",
    "\n",
    "while True:\n",
    "    response = w.genie.list_spaces(page_token=page_token)\n",
    "    for s in response.spaces:\n",
    "        spaces.append({\n",
    "            \"space_id\": getattr(s, \"space_id\", None),\n",
    "            \"name\": getattr(s, \"title\", None),\n",
    "            \"description\": getattr(s, \"description\", None),\n",
    "            \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "        })\n",
    "    if not response.next_page_token or response.next_page_token == \"\":\n",
    "        break\n",
    "    page_token = response.next_page_token\n",
    "\n",
    "print(f\"âœ“ Found {len(spaces)} Genie spaces\")\n",
    "\n",
    "# Limit to 10 spaces\n",
    "MAX_SPACES = 10\n",
    "if len(spaces) > MAX_SPACES:\n",
    "    print(f\"âš  Limiting processing to first {MAX_SPACES} spaces\\n\")\n",
    "    spaces = spaces[:MAX_SPACES]\n",
    "else:\n",
    "    print()\n",
    "\n",
    "# Collect observability data from all spaces\n",
    "all_dfs = []\n",
    "\n",
    "for i, space in enumerate(spaces, 1):\n",
    "    space_id = space['space_id']\n",
    "    space_name = space['name']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{i}/{len(spaces)}] Processing Space: {space_name}\")\n",
    "    print(f\"Space ID: {space_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Get observability data for this space\n",
    "        df_space = get_genie_observability_table(space_id)\n",
    "        \n",
    "        if df_space.count() > 0:\n",
    "            all_dfs.append(df_space)\n",
    "            print(f\"\\nâœ“ Successfully extracted {df_space.count()} messages from {space_name}\")\n",
    "        else:\n",
    "            print(f\"\\nâš  No messages found in {space_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error processing space {space_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_dfs:\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"ðŸ“Š COMBINING ALL RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Union all DataFrames\n",
    "    df = all_dfs[0]\n",
    "    for df_next in all_dfs[1:]:\n",
    "        df = df.union(df_next)\n",
    "    \n",
    "    total_messages = df.count()\n",
    "    total_spaces = df.select(\"space_id\").distinct().count()\n",
    "    \n",
    "    print(f\"\\nâœ… SUCCESS!\")\n",
    "    print(f\"   Total Spaces Processed: {total_spaces}\")\n",
    "    print(f\"   Total Messages Extracted: {total_messages}\")\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    display(df)\n",
    "else:\n",
    "    print(\"\\nâš  No data found across any Genie spaces\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0aa111e-7973-42cd-ad46-5c3b80131761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save the table as a delta table and add a nice detailed table description!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2b6e6e-a659-4c8a-82bc-378301c18104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the observability data to a Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.genie_observability_main_table\")\n",
    "\n",
    "# Add a detailed table description\n",
    "spark.sql(f\"\"\"\n",
    "  COMMENT ON TABLE {catalog_name}.{schema_name}.genie_observability_main_table IS \n",
    "  'Comprehensive observability table for Genie AI/BI spaces. Contains detailed message-level data including user questions, AI responses, generated SQL queries, execution metadata, user feedback ratings, and error information. Used for monitoring Genie usage, analyzing query patterns, and tracking user engagement across all accessible Genie spaces.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3cdb112-7eb8-4f1d-9938-26a223da8e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We successfully created the **genie_observability_main_table** table! Now lets proceed to create the **genie_cost_analysis_main_table** table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b591ee-8c1c-4110-a722-a9ae595e5af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**An Aside: The Strategy: \"Hourly Time-Weighted\" Attribution**\n",
    "The below SQL code is inspired from - https://github.com/databrickslabs/sandbox/blob/main/dbsql/cost_per_query/PrPr/DBSQL%20Cost%20Per%20Query%20MV%20(PrPr).sql \n",
    "We treat the cost of a warehouse hour as a \"pie\" and slice it up based on how much work each query contributed during that specific hour.\n",
    "\n",
    "Step 1: Calculate the Hourly Bill (Per Warehouse) First, we look at the system billing logs (system.billing.usage) to calculate the exact price tag (in DBUs and Dollars) for every hour a warehouse was active. Unlike the basic approach which averages costs over days, this locks in the specific cost for that specific hour.\n",
    "\n",
    "Step 2: Measure \"Work\" (Per Query, Per Hour) We look at the query history (system.query.history) to measure duration.\n",
    "\n",
    "Crucially, if a long-running query spans multiple hours (e.g., 1:50 PM to 2:10 PM), we split it into two chunks: 10 minutes in the 1 PM bucket and 10 minutes in the 2 PM bucket.\n",
    "\n",
    "Step 3: Calculate the Ratio (The Hourly Slice) We compare the query's duration in that specific hour to the total duration of all queries running on the warehouse in that same hour.\n",
    "\n",
    "Logic: \"In the 1 PM hour, this query ran for 5 minutes. The warehouse processed queries for a total of 50 minutes. Therefore, this query pays for 10% of the 1 PM bill.\"\n",
    "\n",
    "Step 4: Assign the Cost & Absorb Idle Time We multiply that percentage (10%) by the total hourly bill found in Step 1.\n",
    "\n",
    "Note: This method automatically accounts for \"idle time.\" If a warehouse costs $10/hr and only runs your single 5-minute query, your query is 100% of the work, so it absorbs the full $10 cost (including the 55 minutes the warehouse sat idle waiting for you).\n",
    "\n",
    "Step 5: Filter for Genie Finally, we aggregate the hourly slices back together and filter for genie_space_id, giving you the precise total cost for every interaction your users had with your Genie Space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd41dd60-43b2-4911-9bc1-1bb7fd57e67f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Use this query to attribute compute costs to a single Genie space with HOURLY precision.\n",
    "# -- This handles idle time and spot-price fluctuations correctly.\n",
    "\n",
    "# WITH \n",
    "# --------------------------------------------------------------------------------\n",
    "# -- STEP 1: HOURLY WAREHOUSE BILLING\n",
    "# -- Instead of summing the whole month, we calculate the cost for every specific Hour.\n",
    "# --------------------------------------------------------------------------------\n",
    "# hourly_warehouse_bill AS (\n",
    "#   SELECT\n",
    "#     u.usage_metadata.warehouse_id,\n",
    "#     date_trunc('HOUR', u.usage_start_time) AS hour_bucket,\n",
    "#     SUM(u.usage_quantity) AS total_hourly_dbus,\n",
    "#     SUM(u.usage_quantity * p.pricing.default) AS total_hourly_dollars\n",
    "#   FROM system.billing.usage u\n",
    "#   JOIN system.billing.list_prices p \n",
    "#     ON u.sku_name = p.sku_name\n",
    "#     AND u.usage_start_time >= p.price_start_time\n",
    "#     AND (u.usage_start_time < p.price_end_time OR p.price_end_time IS NULL)\n",
    "#   WHERE u.usage_start_time >= :start_date::timestamp\n",
    "#     AND u.usage_end_time <= :end_date::timestamp\n",
    "#     AND u.usage_unit = 'DBU'\n",
    "#     AND u.sku_name ILIKE '%SQL%'\n",
    "#   GROUP BY 1, 2\n",
    "# ),\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# -- STEP 2: SPLIT QUERIES BY HOUR\n",
    "# -- If a query runs from 1:55 to 2:05, we split it into two rows so we can match\n",
    "# -- the 1:00 PM cost and the 2:00 PM cost separately.\n",
    "# --------------------------------------------------------------------------------\n",
    "# query_work_split AS (\n",
    "#   SELECT\n",
    "#     statement_id,\n",
    "#     compute.warehouse_id,\n",
    "#     query_source.genie_space_id,\n",
    "#     executed_by,\n",
    "#     statement_text,\n",
    "#     start_time,\n",
    "    \n",
    "#     -- \"Explode\" logic creates a row for every hour the query touched\n",
    "#     timestampadd(HOUR, h, date_trunc('HOUR', start_time)) AS hour_bucket,\n",
    "    \n",
    "#     -- Calculate precise seconds worked IN THIS SPECIFIC HOUR\n",
    "#     (LEAST(UNIX_TIMESTAMP(end_time), UNIX_TIMESTAMP(timestampadd(HOUR, h + 1, date_trunc('HOUR', start_time)))) - \n",
    "#      GREATEST(UNIX_TIMESTAMP(start_time), UNIX_TIMESTAMP(timestampadd(HOUR, h, date_trunc('HOUR', start_time))))) \n",
    "#      AS seconds_worked_in_hour\n",
    "     \n",
    "#   FROM system.query.history\n",
    "#   -- This sequence generator handles the splitting\n",
    "#   JOIN lateral explode(sequence(0, floor((UNIX_TIMESTAMP(end_time) - UNIX_TIMESTAMP(date_trunc('HOUR', start_time))) / 3600))) t(h)\n",
    "#   WHERE start_time >= :start_date::timestamp\n",
    "#     AND start_time <= :end_date::timestamp\n",
    "#     AND compute.warehouse_id IS NOT NULL\n",
    "#     AND total_task_duration_ms > 0\n",
    "# ),\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# -- STEP 3: HOURLY DENOMINATOR\n",
    "# -- Calculate the total work done by ALL queries on the warehouse for each hour.\n",
    "# --------------------------------------------------------------------------------\n",
    "# warehouse_hourly_activity AS (\n",
    "#   SELECT \n",
    "#     warehouse_id,\n",
    "#     hour_bucket,\n",
    "#     SUM(seconds_worked_in_hour) AS total_seconds_worked_by_all\n",
    "#   FROM query_work_split\n",
    "#   GROUP BY 1, 2\n",
    "# ),\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# -- STEP 4: COST ATTRIBUTION (The Slice)\n",
    "# -- Join the Query Slice + The Hourly Pie Size + The Hourly Bill\n",
    "# --------------------------------------------------------------------------------\n",
    "# allocated_metrics AS (\n",
    "#   SELECT\n",
    "#     q.statement_id,\n",
    "#     q.executed_by,\n",
    "#     q.statement_text,\n",
    "#     q.genie_space_id,\n",
    "#     q.start_time,\n",
    "#     q.warehouse_id,\n",
    "#     q.seconds_worked_in_hour,\n",
    "    \n",
    "#     -- Logic: (My Time / Total Time) * Total Bill\n",
    "#     (q.seconds_worked_in_hour / w.total_seconds_worked_by_all) AS work_proportion,\n",
    "#     b.total_hourly_dbus,\n",
    "#     b.total_hourly_dollars,\n",
    "    \n",
    "#     -- Calculate allocated cost for this specific hour slice\n",
    "#     ((q.seconds_worked_in_hour / w.total_seconds_worked_by_all) * b.total_hourly_dbus) AS allocated_dbus_slice,\n",
    "#     ((q.seconds_worked_in_hour / w.total_seconds_worked_by_all) * b.total_hourly_dollars) AS allocated_dollars_slice\n",
    "\n",
    "#   FROM query_work_split q\n",
    "#   -- Join to get the total activity in this hour\n",
    "#   JOIN warehouse_hourly_activity w \n",
    "#     ON q.warehouse_id = w.warehouse_id AND q.hour_bucket = w.hour_bucket\n",
    "#   -- Join to get the bill for this hour\n",
    "#   LEFT JOIN hourly_warehouse_bill b     \n",
    "#     ON q.warehouse_id = b.warehouse_id AND q.hour_bucket = b.hour_bucket\n",
    "# )\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# -- STEP 5: FINAL AGGREGATION\n",
    "# -- Sum the hourly slices back up to get the total cost per query.\n",
    "# --------------------------------------------------------------------------------\n",
    "# SELECT \n",
    "#   statement_id,\n",
    "#   executed_by AS user_email,\n",
    "#   start_time,\n",
    "#   genie_space_id,\n",
    "#   warehouse_id,\n",
    "#   -- Re-sum the duration from the slices for display\n",
    "#   ROUND(SUM(seconds_worked_in_hour), 2) AS accurate_duration_seconds,\n",
    "#   ROUND(SUM(allocated_dbus_slice), 4) AS dbus_consumed,\n",
    "#   ROUND(SUM(allocated_dollars_slice), 4) AS cost_usd,\n",
    "#   statement_text AS sql_code\n",
    "# FROM allocated_metrics\n",
    "# WHERE genie_space_id = :genie_space_id -- Filter for specific space\n",
    "# GROUP BY 1, 2, 3, 4, 5, 9\n",
    "# ORDER BY start_time DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "319b2f34-a68f-4358-8877-9a5cc0454a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the code below we use system tables and the ability to link query execution source to Genie Space ID to calculate the costs that arise from a genie space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2460bd11-768d-4ff7-b8b2-4a3b7b0842b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.genie_cost_analysis_main_table\n",
    "COMMENT 'Granular cost attribution for Databricks Genie spaces, accounting for idle time and hourly fluctuations.'\n",
    "AS\n",
    "WITH \n",
    "-- 1. DEFINE BOUNDARIES: Auto-detect the time range based on available billing/query history\n",
    "table_boundaries AS (\n",
    "  SELECT \n",
    "    date_trunc('HOUR', LEAST(\n",
    "      (SELECT MAX(event_time) FROM system.compute.warehouse_events),\n",
    "      (SELECT MAX(end_time) FROM system.query.history),\n",
    "      (SELECT MAX(usage_end_time) FROM system.billing.usage)\n",
    "    )) AS selected_end_time,\n",
    "    (date_trunc('HOUR', GREATEST(\n",
    "      (SELECT MIN(event_time) FROM system.compute.warehouse_events),\n",
    "      (SELECT MIN(start_time) FROM system.query.history),\n",
    "      (SELECT MIN(usage_end_time) FROM system.billing.usage)\n",
    "    )) + INTERVAL 1 HOUR)::timestamp AS selected_start_time\n",
    "),\n",
    "\n",
    "-- 2. GET HOURLY WAREHOUSE BILLING (The \"Cost to distribute\")\n",
    "cpq_warehouse_usage AS (\n",
    "  SELECT\n",
    "    usage_metadata.warehouse_id AS warehouse_id,\n",
    "    u.*\n",
    "  FROM system.billing.usage AS u\n",
    "  WHERE usage_metadata.warehouse_id IS NOT NULL\n",
    "    AND usage_start_time >= (SELECT MIN(selected_start_time) FROM table_boundaries)\n",
    "    AND usage_end_time <= (SELECT MAX(selected_end_time) FROM table_boundaries)\n",
    "),\n",
    "\n",
    "prices AS (\n",
    "  SELECT coalesce(price_end_time, date_add(current_date, 1)) as coalesced_price_end_time, *\n",
    "  FROM system.billing.list_prices\n",
    "  WHERE currency_code = 'USD'\n",
    "),\n",
    "\n",
    "filtered_warehouse_usage AS (\n",
    "    SELECT \n",
    "      u.warehouse_id,\n",
    "      date_trunc('HOUR', u.usage_start_time) AS usage_start_hour,\n",
    "      u.usage_quantity AS dbus,\n",
    "      (CAST(p.pricing.effective_list.default AS FLOAT) * u.usage_quantity) AS usage_dollars\n",
    "    FROM cpq_warehouse_usage AS u\n",
    "    LEFT JOIN prices as p\n",
    "      ON u.sku_name = p.sku_name\n",
    "      AND u.usage_unit = p.usage_unit\n",
    "      AND (u.usage_end_time BETWEEN p.price_start_time AND p.coalesced_price_end_time)\n",
    "),\n",
    "\n",
    "-- 3. GET QUERY HISTORY (We need ALL queries, not just Genie, to calculate the denominator correctly)\n",
    "cpq_warehouse_query_history AS (\n",
    "  SELECT\n",
    "    statement_id,\n",
    "    executed_by,\n",
    "    statement_text,\n",
    "    compute.warehouse_id AS warehouse_id,\n",
    "    -- Calculate precise execution time excluding metadata overhead\n",
    "    (COALESCE(CAST(total_task_duration_ms AS FLOAT) / 1000, 0) +\n",
    "      COALESCE(CAST(result_fetch_duration_ms AS FLOAT) / 1000, 0) +\n",
    "      COALESCE(CAST(compilation_duration_ms AS FLOAT) / 1000, 0)\n",
    "    ) AS query_work_task_time,\n",
    "    start_time,\n",
    "    end_time,\n",
    "    -- Normalize start/end times for calculation\n",
    "    timestampadd(MILLISECOND , coalesce(waiting_at_capacity_duration_ms, 0) + coalesce(waiting_for_compute_duration_ms, 0) + coalesce(compilation_duration_ms, 0), start_time) AS query_work_start_time,\n",
    "    timestampadd(MILLISECOND, coalesce(result_fetch_duration_ms, 0), end_time) AS query_work_end_time,\n",
    "    -- Identify Genie Source\n",
    "    CASE\n",
    "      WHEN query_source.genie_space_id IS NOT NULL THEN 'GENIE SPACE'\n",
    "      ELSE 'OTHER'\n",
    "    END AS query_source_type,\n",
    "    query_source.genie_space_id\n",
    "  FROM system.query.history AS h\n",
    "  WHERE statement_type IS NOT NULL\n",
    "    AND start_time < (SELECT selected_end_time FROM table_boundaries)\n",
    "    AND end_time > (SELECT selected_start_time FROM table_boundaries)\n",
    "    AND total_task_duration_ms > 0\n",
    "    AND compute.warehouse_id IS NOT NULL\n",
    "),\n",
    "\n",
    "-- 4. SPLIT QUERIES ACROSS HOURLY BUCKETS (Handling long-running queries)\n",
    "hour_intervals AS (\n",
    "  SELECT\n",
    "    statement_id,\n",
    "    warehouse_id,\n",
    "    query_work_start_time,\n",
    "    query_work_end_time,\n",
    "    query_work_task_time,\n",
    "    explode(\n",
    "      sequence(\n",
    "        0,\n",
    "        floor((UNIX_TIMESTAMP(query_work_end_time) - UNIX_TIMESTAMP(date_trunc('hour', query_work_start_time))) / 3600)\n",
    "      )\n",
    "    ) AS hours_interval,\n",
    "    timestampadd(hour, hours_interval, date_trunc('hour', query_work_start_time)) AS hour_bucket\n",
    "  FROM cpq_warehouse_query_history\n",
    "),\n",
    "\n",
    "statement_proportioned_work AS (\n",
    "    SELECT * , \n",
    "        GREATEST(0,\n",
    "          UNIX_TIMESTAMP(LEAST(query_work_end_time, timestampadd(hour, 1, hour_bucket))) -\n",
    "          UNIX_TIMESTAMP(GREATEST(query_work_start_time, hour_bucket))\n",
    "        ) AS overlap_duration,\n",
    "        CASE WHEN CAST(query_work_end_time AS DOUBLE) - CAST(query_work_start_time AS DOUBLE) = 0\n",
    "        THEN 0\n",
    "        ELSE query_work_task_time * (overlap_duration / (CAST(query_work_end_time AS DOUBLE) - CAST(query_work_start_time AS DOUBLE)))\n",
    "        END AS proportional_query_work\n",
    "    FROM hour_intervals\n",
    "),\n",
    "\n",
    "attributed_query_work_all AS (\n",
    "    SELECT\n",
    "      statement_id,\n",
    "      hour_bucket,\n",
    "      warehouse_id,\n",
    "      SUM(proportional_query_work) AS attributed_query_work\n",
    "    FROM statement_proportioned_work\n",
    "    GROUP BY statement_id, warehouse_id, hour_bucket\n",
    "),\n",
    "\n",
    "-- 5. CALCULATE TOTAL WORK PER WAREHOUSE/HOUR\n",
    "warehouse_time as (\n",
    "  select\n",
    "    warehouse_id,\n",
    "    hour_bucket,\n",
    "    SUM(attributed_query_work) as total_work_done_on_warehouse\n",
    "  from attributed_query_work_all\n",
    "  group by warehouse_id, hour_bucket\n",
    "),\n",
    "\n",
    "-- 6. ATTRIBUTE COSTS (Proportion of Work * Cost of Warehouse Hour)\n",
    "history_with_pricing AS (\n",
    "  SELECT\n",
    "    a.statement_id,\n",
    "    a.warehouse_id,\n",
    "    a.hour_bucket,\n",
    "    a.attributed_query_work,\n",
    "    b.total_work_done_on_warehouse,\n",
    "    wh.dbus AS total_warehouse_period_dbus,\n",
    "    wh.usage_dollars AS total_warehouse_period_dollars,\n",
    "    -- Logic: If I did 10% of the work, I pay 10% of the total bill (including the idle time inherent in the bill)\n",
    "    CASE \n",
    "      WHEN b.total_work_done_on_warehouse = 0 THEN 0 \n",
    "      ELSE a.attributed_query_work / b.total_work_done_on_warehouse \n",
    "    END AS query_task_time_proportion\n",
    "  FROM attributed_query_work_all a\n",
    "  INNER JOIN warehouse_time b ON a.warehouse_id = b.warehouse_id AND a.hour_bucket = b.hour_bucket\n",
    "  LEFT JOIN filtered_warehouse_usage AS wh ON a.warehouse_id = wh.warehouse_id AND a.hour_bucket = wh.usage_start_hour\n",
    "),\n",
    "\n",
    "final_attribution AS (\n",
    "  SELECT\n",
    "    statement_id,\n",
    "    (query_task_time_proportion * total_warehouse_period_dollars) AS query_attributed_dollars,\n",
    "    (query_task_time_proportion * total_warehouse_period_dbus) AS query_attributed_dbus\n",
    "  FROM history_with_pricing\n",
    ")\n",
    "\n",
    "-- 7. FINAL FILTER FOR GENIE OUTPUT\n",
    "SELECT \n",
    "  q.genie_space_id,\n",
    "  q.executed_by AS user_email,\n",
    "  q.statement_id,\n",
    "  q.start_time,\n",
    "  q.warehouse_id,\n",
    "  q.statement_text AS sql_code,\n",
    "  SUM(fa.query_attributed_dbus) AS total_dbus_consumed,\n",
    "  SUM(fa.query_attributed_dollars) AS total_cost_usd,\n",
    "  MAX(q.query_work_task_time) as execution_duration_seconds\n",
    "FROM cpq_warehouse_query_history q\n",
    "JOIN final_attribution fa ON q.statement_id = fa.statement_id\n",
    "WHERE q.query_source_type = 'GENIE SPACE' -- CRITICAL: Filter only for Genie here at the end\n",
    "GROUP BY 1, 2, 3, 4, 5, 6\n",
    "ORDER BY start_time DESC;\n",
    "\"\"\")\n",
    "print(\"Table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074c61ec-b0cc-459d-b0f8-28cac4652279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{catalog_name}.{schema_name}.genie_cost_analysis_main_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec6ffa4-9c2b-4325-b9dd-a0fb8ff5fbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a detailed table description\n",
    "spark.sql(f\"\"\"\n",
    "  COMMENT ON TABLE {catalog_name}.{schema_name}.genie_cost_analysis_main_table IS \n",
    "  'This table provides a granular cost attribution analysis for Databricks Genie spaces. It links individual SQL queries executed within Genie to the underlying SQL Warehouse compute costs. By calculating the \"work proportion\" of every query relative to the warehouse''s total activity, it estimates the specific DBU consumption and USD cost for each query statement.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bea3229f-7bbe-4806-9073-49bca9c600b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This concludes this notebook. We now have created two critical tables that will help us monitor and analyse our genie spaces across multiple dimensions:\n",
    "- Cost attribution and chargeback\n",
    "- Usability and accuracy\n",
    "- Insights to further improve the space by adding more tables, adding SQL examples, trusted assets etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c128ab42-23d0-455e-99fc-a3027bee4156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Take this to the next level\n",
    "You can enhance this notebook by incorporating more advanced techniques such as:\n",
    "- Run topic modelling on user user questions to identify common patterns that can help you improve youe Genie space\n",
    "- Incorporate more advanced cost attribution strategies for charge backs. See [LINK](https://github.com/databrickslabs/sandbox/tree/main/dbsql/cost_per_query/PrPr) and [LINK](https://www.databricks.com/resources/demos/tutorials/governance/system-tables?itm_data=demo_center) for inspiration.\n",
    "- Incorporate this notebook as part of your custome Databricks Jobs process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c743a683-8da9-432a-bdc9-c1b3136f14e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a3d31a-cd3d-46a9-8554-36a33c96c2a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "REST API Implementation for get_genie_observability_table function using Python Requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5abb6b6d-6359-4b33-a788-9447d4c970cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "# from typing import Dict, Any, List\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, IntegerType\n",
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# # Cache for User Email lookups to prevent redundant API calls\n",
    "# USER_CACHE = {}\n",
    "\n",
    "# def get_genie_observability_table(\n",
    "#     space_id: str, \n",
    "#     databricks_token: str, \n",
    "#     host_url: str,\n",
    "#     include_all_users: bool = True\n",
    "# ) -> 'pyspark.sql.DataFrame':\n",
    "#     \"\"\"\n",
    "#     Fetches and constructs a Spark DataFrame containing observability data for all messages in a Genie space.\n",
    "\n",
    "#     Args:\n",
    "#         space_id (str): The Genie space ID.\n",
    "#         databricks_token (str): Databricks personal access token for authentication.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         include_all_users (bool, optional): Whether to include all users' conversations. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         pyspark.sql.DataFrame: DataFrame with observability records for the specified Genie space.\n",
    "#     \"\"\"\n",
    "#     host_url = host_url.rstrip('/')\n",
    "#     headers = {'Authorization': f'Bearer {databricks_token}', 'Content-Type': 'application/json'}\n",
    "    \n",
    "#     # Step 1: Get space name\n",
    "#     space_url = f\"{host_url}/api/2.0/genie/spaces/{space_id}\"\n",
    "#     space_resp = requests.get(space_url, headers=headers)\n",
    "#     space_resp.raise_for_status()\n",
    "#     space_name = space_resp.json().get('title', f\"Space_{space_id}\")\n",
    "    \n",
    "#     # Step 2: Get conversations\n",
    "#     conversations = _get_all_conversations(space_id, host_url, headers, include_all_users)\n",
    "    \n",
    "#     # Step 3: Extract and Flatten\n",
    "#     records = []\n",
    "#     for conv in conversations:\n",
    "#         messages = _get_all_conversation_messages(space_id, conv['conversation_id'], host_url, headers)\n",
    "#         for msg in messages:\n",
    "#             record = _extract_message_data(msg, space_id, space_name, host_url, headers)\n",
    "#             records.append(record)\n",
    "    \n",
    "#     # Step 4: Create Spark DataFrame\n",
    "#     spark = SparkSession.builder.getOrCreate()\n",
    "#     schema = _get_schema()\n",
    "    \n",
    "#     if not records:\n",
    "#         return spark.createDataFrame([], schema)\n",
    "        \n",
    "#     df = spark.createDataFrame(records, schema=schema)\n",
    "    \n",
    "#     # Convert timestamps to Datetime\n",
    "#     df = df.withColumn(\"created_datetime\", from_unixtime(col(\"created_timestamp\") / 1000).cast(TimestampType())) \\\n",
    "#            .withColumn(\"last_updated_datetime\", from_unixtime(col(\"last_updated_timestamp\") / 1000).cast(TimestampType())) \\\n",
    "#            .orderBy(\"created_timestamp\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def _get_all_conversations(space_id: str, host_url: str, headers: Dict[str, str], \n",
    "#                            include_all: bool) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Retrieves all conversations for a given Genie space, handling pagination.\n",
    "\n",
    "#     Args:\n",
    "#         space_id (str): Genie space ID.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         headers (Dict[str, str]): HTTP headers for authentication.\n",
    "#         include_all (bool): Whether to include all users' conversations.\n",
    "\n",
    "#     Returns:\n",
    "#         List[Dict[str, Any]]: List of conversation metadata dictionaries.\n",
    "#     \"\"\"\n",
    "#     all_conversations = []\n",
    "#     page_token = None\n",
    "    \n",
    "#     while True:\n",
    "#         url = f\"{host_url}/api/2.0/genie/spaces/{space_id}/conversations\"\n",
    "#         params = {'page_size': 100}\n",
    "        \n",
    "#         if page_token:\n",
    "#             params['page_token'] = page_token\n",
    "#         if include_all:\n",
    "#             params['include_all'] = 'true'\n",
    "        \n",
    "#         response = requests.get(url, headers=headers, params=params)\n",
    "#         response.raise_for_status()\n",
    "#         result = response.json()\n",
    "        \n",
    "#         conversations = result.get('conversations', [])\n",
    "#         all_conversations.extend(conversations)\n",
    "        \n",
    "#         page_token = result.get('next_page_token')\n",
    "#         if not page_token:\n",
    "#             break\n",
    "    \n",
    "#     return all_conversations\n",
    "\n",
    "\n",
    "# def _get_all_conversation_messages(space_id: str, conversation_id: str, \n",
    "#                                    host_url: str, headers: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Retrieves all messages from a specific conversation, handling pagination.\n",
    "\n",
    "#     Args:\n",
    "#         space_id (str): Genie space ID.\n",
    "#         conversation_id (str): Conversation ID.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         headers (Dict[str, str]): HTTP headers for authentication.\n",
    "\n",
    "#     Returns:\n",
    "#         List[Dict[str, Any]]: List of message dictionaries.\n",
    "#     \"\"\"\n",
    "#     all_messages = []\n",
    "#     page_token = None\n",
    "    \n",
    "#     while True:\n",
    "#         url = f\"{host_url}/api/2.0/genie/spaces/{space_id}/conversations/{conversation_id}/messages\"\n",
    "#         params = {'page_size': 100}\n",
    "        \n",
    "#         if page_token:\n",
    "#             params['page_token'] = page_token\n",
    "        \n",
    "#         response = requests.get(url, headers=headers, params=params)\n",
    "#         response.raise_for_status()\n",
    "#         result = response.json()\n",
    "        \n",
    "#         messages = result.get('messages', [])\n",
    "#         all_messages.extend(messages)\n",
    "        \n",
    "#         page_token = result.get('next_page_token')\n",
    "#         if not page_token:\n",
    "#             break\n",
    "    \n",
    "#     return all_messages\n",
    "    \n",
    "# def _extract_message_data(message: Dict[str, Any], space_id: str, space_name: str, host_url: str, headers: dict) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Extracts and flattens relevant fields from a Genie message object for observability.\n",
    "\n",
    "#     Args:\n",
    "#         message (Dict[str, Any]): Message dictionary from Genie API.\n",
    "#         space_id (str): Genie space ID.\n",
    "#         space_name (str): Genie space name.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         headers (dict): HTTP headers for authentication.\n",
    "\n",
    "#     Returns:\n",
    "#         Dict[str, Any]: Flattened record with observability fields.\n",
    "#     \"\"\"\n",
    "#     # Resolve User Email\n",
    "#     user_id = str(message.get('user_id')) if message.get('user_id') else None\n",
    "#     user_email = _resolve_email(user_id, host_url, headers)\n",
    "\n",
    "#     record = {\n",
    "#         'space_id': space_id,\n",
    "#         'space_name': space_name,\n",
    "#         'message_id': message.get('message_id'),\n",
    "#         'conversation_id': message.get('conversation_id'),\n",
    "#         'user_id': user_id,\n",
    "#         'user_email': user_email,\n",
    "#         'status': message.get('status'),\n",
    "#         'created_timestamp': message.get('created_timestamp'),\n",
    "#         'last_updated_timestamp': message.get('last_updated_timestamp'),\n",
    "#         'user_question': message.get('content'),\n",
    "#     }\n",
    "    \n",
    "#     ai_responses, sql_queries, statement_ids, suggested_qs = [], [], [], []\n",
    "    \n",
    "#     for att in message.get('attachments', []):\n",
    "#         # Text Attachment\n",
    "#         if att.get('text'):\n",
    "#             ai_responses.append(att['text'].get('content', ''))\n",
    "        \n",
    "#         # Query Attachment (Corrected sibling path)\n",
    "#         query_obj = att.get('query')\n",
    "#         if query_obj:\n",
    "#             sql_queries.append(query_obj.get('query', ''))\n",
    "#             s_id = query_obj.get('statement_id')\n",
    "#             if s_id: statement_ids.append(str(s_id))\n",
    "            \n",
    "#         # Suggested Questions\n",
    "#         if att.get('suggested_questions'):\n",
    "#             suggested_qs.extend(att['suggested_questions'].get('questions', []))\n",
    "\n",
    "#     # Helper to return None instead of empty string\n",
    "#     def join_clean(lst, sep=' | '): return sep.join(filter(None, lst)) if lst else None\n",
    "\n",
    "#     record.update({\n",
    "#         'ai_response': join_clean(ai_responses),\n",
    "#         'sql_query': join_clean(sql_queries),\n",
    "#         'statement_id': join_clean(statement_ids),\n",
    "#         'suggested_questions': join_clean(suggested_qs, sep=', '),\n",
    "#         'num_attachments': len(message.get('attachments', []))\n",
    "#     })\n",
    "    \n",
    "#     # Feedback - Where the \"Review Comments\" live\n",
    "#     feedback = message.get('feedback', {})\n",
    "#     record['feedback_rating'] = feedback.get('rating', 'NONE')\n",
    "        \n",
    "#     # Errors\n",
    "#     error = message.get('error', {})\n",
    "#     record['error_type'] = error.get('type')\n",
    "#     record['error_message'] = error.get('error')\n",
    "    \n",
    "#     return record\n",
    "\n",
    "# def _resolve_email(user_id: str, host_url: str, headers: dict) -> str:\n",
    "#     \"\"\"\n",
    "#     Resolves a Databricks user ID to an email address using the SCIM API, with caching.\n",
    "\n",
    "#     Args:\n",
    "#         user_id (str): Databricks user ID.\n",
    "#         host_url (str): Databricks workspace host URL.\n",
    "#         headers (dict): HTTP headers for authentication.\n",
    "\n",
    "#     Returns:\n",
    "#         str: User email if found, or a fallback string.\n",
    "#     \"\"\"\n",
    "#     if not user_id or user_id in [\"None\", \"0\"]: return None\n",
    "#     if user_id in USER_CACHE: return USER_CACHE[user_id]\n",
    "    \n",
    "#     try:\n",
    "#         url = f\"{host_url}/api/2.0/preview/scim/v2/Users/{user_id}\"\n",
    "#         resp = requests.get(url, headers=headers)\n",
    "#         if resp.status_code == 200:\n",
    "#             email = resp.json().get('userName')\n",
    "#             USER_CACHE[user_id] = email\n",
    "#             return email\n",
    "#     except: pass\n",
    "#     return f\"ID_{user_id}\"\n",
    "\n",
    "# def _get_schema() -> StructType:\n",
    "#     \"\"\"\n",
    "#     Returns the schema for the Genie observability Spark DataFrame.\n",
    "\n",
    "#     Returns:\n",
    "#         StructType: Spark schema for observability records.\n",
    "#     \"\"\"\n",
    "#     return StructType([\n",
    "#         StructField(\"space_id\", StringType(), True),\n",
    "#         StructField(\"space_name\", StringType(), True),\n",
    "#         StructField(\"message_id\", StringType(), True),\n",
    "#         StructField(\"conversation_id\", StringType(), True),\n",
    "#         StructField(\"user_id\", StringType(), True),\n",
    "#         StructField(\"user_email\", StringType(), True),\n",
    "#         StructField(\"status\", StringType(), True),\n",
    "#         StructField(\"created_timestamp\", LongType(), True),\n",
    "#         StructField(\"last_updated_timestamp\", LongType(), True),\n",
    "#         StructField(\"user_question\", StringType(), True),\n",
    "#         StructField(\"ai_response\", StringType(), True),\n",
    "#         StructField(\"sql_query\", StringType(), True),\n",
    "#         StructField(\"statement_id\", StringType(), True),\n",
    "#         StructField(\"suggested_questions\", StringType(), True),\n",
    "#         StructField(\"num_attachments\", IntegerType(), True),\n",
    "#         StructField(\"feedback_rating\", StringType(), True),\n",
    "#         StructField(\"error_type\", StringType(), True),\n",
    "#         StructField(\"error_message\", StringType(), True),\n",
    "#     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88cadb84-31a5-4b36-8b1f-0ad5a2406f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Set your Genie space ID\n",
    "# space_id = \"XXX\"  # Replace with your space ID\n",
    "\n",
    "# # Use WorkspaceClient to get host and token\n",
    "# from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# # Initialize the Databricks workspace client\n",
    "# w = WorkspaceClient()\n",
    "\n",
    "# # Retrieve the workspace host URL\n",
    "# host = w.config.host\n",
    "\n",
    "# # Retrieve the API token from the workspace client config\n",
    "# token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# # Fetch Genie observability data for the specified space\n",
    "# df = get_genie_observability_table(space_id, token, host)\n",
    "\n",
    "# # Display the resulting DataFrame\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5697fe63-c82e-442f-a9b7-715e4f1fc2d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from databricks.sdk import WorkspaceClient\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "\n",
    "# # Initialize the Databricks workspace client\n",
    "# w = WorkspaceClient()\n",
    "\n",
    "# # Get authentication parameters\n",
    "# token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "# host = w.config.host\n",
    "\n",
    "# # Fetch all Genie spaces\n",
    "# print(\"ðŸ” Fetching all Genie spaces...\")\n",
    "# spaces = []\n",
    "# page_token = None\n",
    "\n",
    "# while True:\n",
    "#     response = w.genie.list_spaces(page_token=page_token)\n",
    "#     for s in response.spaces:\n",
    "#         spaces.append({\n",
    "#             \"space_id\": getattr(s, \"space_id\", None),\n",
    "#             \"name\": getattr(s, \"title\", None),\n",
    "#             \"description\": getattr(s, \"description\", None),\n",
    "#             \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "#         })\n",
    "#     if not response.next_page_token or response.next_page_token == \"\":\n",
    "#         break\n",
    "#     page_token = response.next_page_token\n",
    "\n",
    "# print(f\"âœ“ Found {len(spaces)} Genie spaces\")\n",
    "\n",
    "# # Limit to 10 spaces\n",
    "# MAX_SPACES = 10\n",
    "# if len(spaces) > MAX_SPACES:\n",
    "#     print(f\"âš  Limiting processing to first {MAX_SPACES} spaces\\n\")\n",
    "#     spaces = spaces[:MAX_SPACES]\n",
    "# else:\n",
    "#     print()\n",
    "\n",
    "# # Collect observability data from all spaces\n",
    "# all_dfs = []\n",
    "\n",
    "# for i, space in enumerate(spaces, 1):\n",
    "#     space_id = space['space_id']\n",
    "#     space_name = space['name']\n",
    "    \n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"[{i}/{len(spaces)}] Processing Space: {space_name}\")\n",
    "#     print(f\"Space ID: {space_id}\")\n",
    "#     print(f\"{'='*80}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Get observability data for this space\n",
    "#         df_space = get_genie_observability_table(space_id, token, host)\n",
    "        \n",
    "#         if df_space.count() > 0:\n",
    "#             all_dfs.append(df_space)\n",
    "#             print(f\"\\nâœ“ Successfully extracted {df_space.count()} messages from {space_name}\")\n",
    "#         else:\n",
    "#             print(f\"\\nâš  No messages found in {space_name}\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nâŒ Error processing space {space_name}: {str(e)}\")\n",
    "#         continue\n",
    "\n",
    "# # Combine all DataFrames\n",
    "# if all_dfs:\n",
    "#     print(f\"\\n\\n{'='*80}\")\n",
    "#     print(\"ðŸ“Š COMBINING ALL RESULTS\")\n",
    "#     print(f\"{'='*80}\")\n",
    "    \n",
    "#     # Union all DataFrames\n",
    "#     df = all_dfs[0]\n",
    "#     for df_next in all_dfs[1:]:\n",
    "#         df = df.union(df_next)\n",
    "    \n",
    "#     total_messages = df.count()\n",
    "#     total_spaces = df.select(\"space_id\").distinct().count()\n",
    "    \n",
    "#     print(f\"\\nâœ… SUCCESS!\")\n",
    "#     print(f\"   Total Spaces Processed: {total_spaces}\")\n",
    "#     print(f\"   Total Messages Extracted: {total_messages}\")\n",
    "#     print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "#     display(df)\n",
    "# else:\n",
    "#     print(\"\\nâš  No data found across any Genie spaces\")\n",
    "#     df = None"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "04_genie_observability_deep_dive",
   "widgets": {
    "catalog_name": {
     "currentValue": "kg_demo_catalog",
     "nuid": "4e44dffa-bf52-4452-9c0e-5cee0134286e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "users",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "end_date": {
     "currentValue": "",
     "nuid": "291255da-24ff-4bae-9b9d-bc7602111a73",
     "typedWidgetInfo": {
      "autoCreated": true,
      "defaultValue": "",
      "label": null,
      "name": "end_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "end_date",
      "options": {
       "widgetType": "text",
       "autoCreated": true,
       "validationRegex": null
      }
     }
    },
    "genie_space_id": {
     "currentValue": "",
     "nuid": "a26b788e-e130-45a9-9fda-8b3f4111900a",
     "typedWidgetInfo": {
      "autoCreated": true,
      "defaultValue": "",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "widgetType": "text",
       "autoCreated": true,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "dqx_schema",
     "nuid": "80c4b03f-ad5e-4661-a5cb-c56cf9bbd3c0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "start_date": {
     "currentValue": "",
     "nuid": "6eaf4217-89c5-475f-8783-b4e0aee51b38",
     "typedWidgetInfo": {
      "autoCreated": true,
      "defaultValue": "",
      "label": null,
      "name": "start_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "start_date",
      "options": {
       "widgetType": "text",
       "autoCreated": true,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
