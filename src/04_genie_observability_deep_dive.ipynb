{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aee1651c-4ec6-4cc7-b6e6-df71e494ddef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Genie Space Observability E2E Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a0f9eb-9bdc-4655-8a2d-77fc32419849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Introduction\n",
    "This notebook covers how to leverage Genie REST APIs and Databricks System Tables to create an e2e Genie Observability workflow, complete with a Genie Observability Dashboard and a Meta-Genie space to ask questions about your Genie spaces!\n",
    "\n",
    "Running this notebook will create **Two Delta Tables**; _genie_observability_master_table_ and _genie_cost_analysis_master_table_.\n",
    "\n",
    "- **genie_observability_master_table:** This table consists of ALL the messages sen to your genie spaces along with Genie natural langauge response and the SQL code generated as part of it. There are additional columns that provide the conversation_id, user_feedback,user_email, the statement execution ID linked to the SQL code and much more.\n",
    "- **genie_cost_analysis_master_table:** This table consist of ALL information required to attribute costs to a genie space through Genie space ID. Additional columns include statement_id, user_email, start_time, cost per statement ID and much more. \n",
    "\n",
    "**Notes:**\n",
    "- The entire solution is vibe coded (with human in the loop!), so please verify code before running in prod.\n",
    "- The cost per query attribution logic is borrowed from DBSQL SME blog and dashboard. Link here: https://github.com/CodyAustinDavis/dbsql_sme/tree/main/Observability%20Dashboards%20and%20DBA%20Resources/Observability%20Lakeview%20Dashboard%20Templates/DBSQL%20Warehouse%20Advisor%20With%20Data%20Model \n",
    "\n",
    "**Usage Guidance:**\n",
    "- You can use this notebook as part of the wider Lakehouse Adoption Dashboard and pipeline that can be deployed as part of Databricks Asset Bundles. Equally, you can download this notebook and Dashboard JSON and incorporate it into your own workflow.\n",
    "- We recommend running this notebook in a test environment first and tailor the code as per your preferance.\n",
    "\n",
    "**Pre-requisites:**\n",
    "- You should be a Genie Space Author of atleast one Genie space (CAN MANAGE PERMISSION)\n",
    "- You should have enough permissions to create tables in a schema within a catalog of your choice\n",
    "- SELECT permission on system catalog and usage and billing schema\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4cd870-8b3a-474d-9225-2774b9725c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upgrade to the latest version of databricks_sdk package\n",
    "%pip install databricks_sdk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff69540-d03b-4f4e-9bb8-aab8014e1e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart Python to ensure all libraries are reloaded\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261aa6d3-a8ae-4eb3-8162-19618b71abe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a text widget for catalog_name with default value 'users'\n",
    "dbutils.widgets.text(\"catalog_name\", \"users\")\n",
    "# Create a text widget for schema_name with no default value\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "\n",
    "# Retrieve the value of catalog_name from the widget\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "# Retrieve the value of schema_name from the widget\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "# Ensure both catalog_name and schema_name are provided\n",
    "assert catalog_name and schema_name, \"catalog_name and schema_name must be provided\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb27c2d-e967-40f3-a3d4-d988889958ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get current user information using the workspace client\n",
    "current_user = w.current_user.me()\n",
    "user_name = current_user.user_name\n",
    "\n",
    "# Print the current user's username\n",
    "print(f\"Current user: {user_name}\")\n",
    "\n",
    "# Print the Databricks workspace host URL\n",
    "print(f\"Workspace: {w.config.host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ec0d27-ff1f-4464-a648-7ba92d1a60f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below we list all Genie spaces in the current workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0f699f-c346-4d5b-9754-193c3d26b41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store Genie space metadata\n",
    "spaces = []\n",
    "page_token = None\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Databricks Workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Paginate through all Genie spaces using the SDK\n",
    "while True:\n",
    "    response = w.genie.list_spaces(page_token=page_token)\n",
    "    for s in response.spaces:\n",
    "        # Append relevant space details to the list\n",
    "        spaces.append({\n",
    "            \"space_id\": getattr(s, \"space_id\", None),\n",
    "            \"name\": getattr(s, \"title\", None),\n",
    "            \"description\": getattr(s, \"description\", None),\n",
    "            \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "        })\n",
    "    # Break if there are no more pages\n",
    "    if not response.next_page_token or response.next_page_token == \"\":\n",
    "        break\n",
    "    page_token = response.next_page_token\n",
    "\n",
    "# Convert the list of spaces to a Pandas DataFrame\n",
    "genie_spaces_pdf = pd.DataFrame(spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d99a3ab-9b40-4a6e-b229-8711026c1845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame containing all Genie spaces metadata\n",
    "display(genie_spaces_pdf)\n",
    "print(\"---------------\")\n",
    "print(f\"Total Genie spaces: {len(genie_spaces_pdf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b087ff-9909-4518-9a4d-99bf099ef509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code cell below is where we define the single most important function that allows us to generate the genie_observability_master_table table. You can edit this function as per your requirements or even reverse engineer this to generate newer insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4062784e-580e-47b2-a255-480ad142ba8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# Cache for User Email lookups to prevent redundant API calls\n",
    "USER_CACHE = {}\n",
    "\n",
    "def get_genie_observability_table(\n",
    "    space_id: str, \n",
    "    databricks_token: str, \n",
    "    host_url: str,\n",
    "    include_all_users: bool = True\n",
    ") -> 'pyspark.sql.DataFrame':\n",
    "    \"\"\"\n",
    "    Fetches and constructs a Spark DataFrame containing observability data for all messages in a Genie space.\n",
    "\n",
    "    Args:\n",
    "        space_id (str): The Genie space ID.\n",
    "        databricks_token (str): Databricks personal access token for authentication.\n",
    "        host_url (str): Databricks workspace host URL.\n",
    "        include_all_users (bool, optional): Whether to include all users' conversations. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame with observability records for the specified Genie space.\n",
    "    \"\"\"\n",
    "    host_url = host_url.rstrip('/')\n",
    "    headers = {'Authorization': f'Bearer {databricks_token}', 'Content-Type': 'application/json'}\n",
    "    \n",
    "    # Step 1: Get space name\n",
    "    space_url = f\"{host_url}/api/2.0/genie/spaces/{space_id}\"\n",
    "    space_resp = requests.get(space_url, headers=headers)\n",
    "    space_resp.raise_for_status()\n",
    "    space_name = space_resp.json().get('title', f\"Space_{space_id}\")\n",
    "    \n",
    "    # Step 2: Get conversations\n",
    "    conversations = _get_all_conversations(space_id, host_url, headers, include_all_users)\n",
    "    \n",
    "    # Step 3: Extract and Flatten\n",
    "    records = []\n",
    "    for conv in conversations:\n",
    "        messages = _get_all_conversation_messages(space_id, conv['conversation_id'], host_url, headers)\n",
    "        for msg in messages:\n",
    "            record = _extract_message_data(msg, space_id, space_name, host_url, headers)\n",
    "            records.append(record)\n",
    "    \n",
    "    # Step 4: Create Spark DataFrame\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    schema = _get_schema()\n",
    "    \n",
    "    if not records:\n",
    "        return spark.createDataFrame([], schema)\n",
    "        \n",
    "    df = spark.createDataFrame(records, schema=schema)\n",
    "    \n",
    "    # Convert timestamps to Datetime\n",
    "    df = df.withColumn(\"created_datetime\", from_unixtime(col(\"created_timestamp\") / 1000).cast(TimestampType())) \\\n",
    "           .withColumn(\"last_updated_datetime\", from_unixtime(col(\"last_updated_timestamp\") / 1000).cast(TimestampType())) \\\n",
    "           .orderBy(\"created_timestamp\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def _get_all_conversations(space_id: str, host_url: str, headers: Dict[str, str], \n",
    "                           include_all: bool) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieves all conversations for a given Genie space, handling pagination.\n",
    "\n",
    "    Args:\n",
    "        space_id (str): Genie space ID.\n",
    "        host_url (str): Databricks workspace host URL.\n",
    "        headers (Dict[str, str]): HTTP headers for authentication.\n",
    "        include_all (bool): Whether to include all users' conversations.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of conversation metadata dictionaries.\n",
    "    \"\"\"\n",
    "    all_conversations = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{host_url}/api/2.0/genie/spaces/{space_id}/conversations\"\n",
    "        params = {'page_size': 100}\n",
    "        \n",
    "        if page_token:\n",
    "            params['page_token'] = page_token\n",
    "        if include_all:\n",
    "            params['include_all'] = 'true'\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        conversations = result.get('conversations', [])\n",
    "        all_conversations.extend(conversations)\n",
    "        \n",
    "        page_token = result.get('next_page_token')\n",
    "        if not page_token:\n",
    "            break\n",
    "    \n",
    "    return all_conversations\n",
    "\n",
    "\n",
    "def _get_all_conversation_messages(space_id: str, conversation_id: str, \n",
    "                                   host_url: str, headers: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieves all messages from a specific conversation, handling pagination.\n",
    "\n",
    "    Args:\n",
    "        space_id (str): Genie space ID.\n",
    "        conversation_id (str): Conversation ID.\n",
    "        host_url (str): Databricks workspace host URL.\n",
    "        headers (Dict[str, str]): HTTP headers for authentication.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of message dictionaries.\n",
    "    \"\"\"\n",
    "    all_messages = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{host_url}/api/2.0/genie/spaces/{space_id}/conversations/{conversation_id}/messages\"\n",
    "        params = {'page_size': 100}\n",
    "        \n",
    "        if page_token:\n",
    "            params['page_token'] = page_token\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        messages = result.get('messages', [])\n",
    "        all_messages.extend(messages)\n",
    "        \n",
    "        page_token = result.get('next_page_token')\n",
    "        if not page_token:\n",
    "            break\n",
    "    \n",
    "    return all_messages\n",
    "    \n",
    "def _extract_message_data(message: Dict[str, Any], space_id: str, space_name: str, host_url: str, headers: dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts and flattens relevant fields from a Genie message object for observability.\n",
    "\n",
    "    Args:\n",
    "        message (Dict[str, Any]): Message dictionary from Genie API.\n",
    "        space_id (str): Genie space ID.\n",
    "        space_name (str): Genie space name.\n",
    "        host_url (str): Databricks workspace host URL.\n",
    "        headers (dict): HTTP headers for authentication.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Flattened record with observability fields.\n",
    "    \"\"\"\n",
    "    # Resolve User Email\n",
    "    user_id = str(message.get('user_id')) if message.get('user_id') else None\n",
    "    user_email = _resolve_email(user_id, host_url, headers)\n",
    "\n",
    "    record = {\n",
    "        'space_id': space_id,\n",
    "        'space_name': space_name,\n",
    "        'message_id': message.get('message_id'),\n",
    "        'conversation_id': message.get('conversation_id'),\n",
    "        'user_id': user_id,\n",
    "        'user_email': user_email,\n",
    "        'status': message.get('status'),\n",
    "        'created_timestamp': message.get('created_timestamp'),\n",
    "        'last_updated_timestamp': message.get('last_updated_timestamp'),\n",
    "        'user_question': message.get('content'),\n",
    "    }\n",
    "    \n",
    "    ai_responses, sql_queries, statement_ids, suggested_qs = [], [], [], []\n",
    "    \n",
    "    for att in message.get('attachments', []):\n",
    "        # Text Attachment\n",
    "        if att.get('text'):\n",
    "            ai_responses.append(att['text'].get('content', ''))\n",
    "        \n",
    "        # Query Attachment (Corrected sibling path)\n",
    "        query_obj = att.get('query')\n",
    "        if query_obj:\n",
    "            sql_queries.append(query_obj.get('query', ''))\n",
    "            s_id = query_obj.get('statement_id')\n",
    "            if s_id: statement_ids.append(str(s_id))\n",
    "            \n",
    "        # Suggested Questions\n",
    "        if att.get('suggested_questions'):\n",
    "            suggested_qs.extend(att['suggested_questions'].get('questions', []))\n",
    "\n",
    "    # Helper to return None instead of empty string\n",
    "    def join_clean(lst, sep=' | '): return sep.join(filter(None, lst)) if lst else None\n",
    "\n",
    "    record.update({\n",
    "        'ai_response': join_clean(ai_responses),\n",
    "        'sql_query': join_clean(sql_queries),\n",
    "        'statement_id': join_clean(statement_ids),\n",
    "        'suggested_questions': join_clean(suggested_qs, sep=', '),\n",
    "        'num_attachments': len(message.get('attachments', []))\n",
    "    })\n",
    "    \n",
    "    # Feedback - Where the \"Review Comments\" live\n",
    "    feedback = message.get('feedback', {})\n",
    "    record['feedback_rating'] = feedback.get('rating', 'NONE')\n",
    "        \n",
    "    # Errors\n",
    "    error = message.get('error', {})\n",
    "    record['error_type'] = error.get('type')\n",
    "    record['error_message'] = error.get('error')\n",
    "    \n",
    "    return record\n",
    "\n",
    "def _resolve_email(user_id: str, host_url: str, headers: dict) -> str:\n",
    "    \"\"\"\n",
    "    Resolves a Databricks user ID to an email address using the SCIM API, with caching.\n",
    "\n",
    "    Args:\n",
    "        user_id (str): Databricks user ID.\n",
    "        host_url (str): Databricks workspace host URL.\n",
    "        headers (dict): HTTP headers for authentication.\n",
    "\n",
    "    Returns:\n",
    "        str: User email if found, or a fallback string.\n",
    "    \"\"\"\n",
    "    if not user_id or user_id in [\"None\", \"0\"]: return None\n",
    "    if user_id in USER_CACHE: return USER_CACHE[user_id]\n",
    "    \n",
    "    try:\n",
    "        url = f\"{host_url}/api/2.0/preview/scim/v2/Users/{user_id}\"\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        if resp.status_code == 200:\n",
    "            email = resp.json().get('userName')\n",
    "            USER_CACHE[user_id] = email\n",
    "            return email\n",
    "    except: pass\n",
    "    return f\"ID_{user_id}\"\n",
    "\n",
    "def _get_schema() -> StructType:\n",
    "    \"\"\"\n",
    "    Returns the schema for the Genie observability Spark DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        StructType: Spark schema for observability records.\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"space_id\", StringType(), True),\n",
    "        StructField(\"space_name\", StringType(), True),\n",
    "        StructField(\"message_id\", StringType(), True),\n",
    "        StructField(\"conversation_id\", StringType(), True),\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"user_email\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"created_timestamp\", LongType(), True),\n",
    "        StructField(\"last_updated_timestamp\", LongType(), True),\n",
    "        StructField(\"user_question\", StringType(), True),\n",
    "        StructField(\"ai_response\", StringType(), True),\n",
    "        StructField(\"sql_query\", StringType(), True),\n",
    "        StructField(\"statement_id\", StringType(), True),\n",
    "        StructField(\"suggested_questions\", StringType(), True),\n",
    "        StructField(\"num_attachments\", IntegerType(), True),\n",
    "        StructField(\"feedback_rating\", StringType(), True),\n",
    "        StructField(\"error_type\", StringType(), True),\n",
    "        StructField(\"error_message\", StringType(), True),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7169a669-09fd-4808-9a11-b7c51d306b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can test the function by running the function on a Genie space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71ecb12-93fd-42f5-a446-487f12f0a822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set your Genie space ID\n",
    "space_id = \"XXX\"  # Replace with your space ID\n",
    "\n",
    "# Use WorkspaceClient to get host and token\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Retrieve the workspace host URL\n",
    "host = w.config.host\n",
    "\n",
    "# Retrieve the API token from the workspace client config\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Fetch Genie observability data for the specified space\n",
    "df = get_genie_observability_table(space_id, token, host)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5092fae2-0439-4e4a-9e4c-7b18b4c1dabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below is a simple scripts that runs this function on ALL your genie spaces (currently limited to 10). \n",
    "\n",
    "Note: You may not have permssion to view messages of certain Genie spaces as you may not have access to the underlying tables. In this case you will see a lot of NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96ad200-f04b-420e-a77e-c749c900cfdf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769914978635}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get authentication parameters\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "host = w.config.host\n",
    "\n",
    "# Fetch all Genie spaces\n",
    "print(\"ğŸ” Fetching all Genie spaces...\")\n",
    "spaces = []\n",
    "page_token = None\n",
    "\n",
    "while True:\n",
    "    response = w.genie.list_spaces(page_token=page_token)\n",
    "    for s in response.spaces:\n",
    "        spaces.append({\n",
    "            \"space_id\": getattr(s, \"space_id\", None),\n",
    "            \"name\": getattr(s, \"title\", None),\n",
    "            \"description\": getattr(s, \"description\", None),\n",
    "            \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "        })\n",
    "    if not response.next_page_token or response.next_page_token == \"\":\n",
    "        break\n",
    "    page_token = response.next_page_token\n",
    "\n",
    "print(f\"âœ“ Found {len(spaces)} Genie spaces\")\n",
    "\n",
    "# Limit to 10 spaces\n",
    "MAX_SPACES = 10\n",
    "if len(spaces) > MAX_SPACES:\n",
    "    print(f\"âš  Limiting processing to first {MAX_SPACES} spaces\\n\")\n",
    "    spaces = spaces[:MAX_SPACES]\n",
    "else:\n",
    "    print()\n",
    "\n",
    "# Collect observability data from all spaces\n",
    "all_dfs = []\n",
    "\n",
    "for i, space in enumerate(spaces, 1):\n",
    "    space_id = space['space_id']\n",
    "    space_name = space['name']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{i}/{len(spaces)}] Processing Space: {space_name}\")\n",
    "    print(f\"Space ID: {space_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Get observability data for this space\n",
    "        df_space = get_genie_observability_table(space_id, token, host)\n",
    "        \n",
    "        if df_space.count() > 0:\n",
    "            all_dfs.append(df_space)\n",
    "            print(f\"\\nâœ“ Successfully extracted {df_space.count()} messages from {space_name}\")\n",
    "        else:\n",
    "            print(f\"\\nâš  No messages found in {space_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error processing space {space_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_dfs:\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"ğŸ“Š COMBINING ALL RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Union all DataFrames\n",
    "    df = all_dfs[0]\n",
    "    for df_next in all_dfs[1:]:\n",
    "        df = df.union(df_next)\n",
    "    \n",
    "    total_messages = df.count()\n",
    "    total_spaces = df.select(\"space_id\").distinct().count()\n",
    "    \n",
    "    print(f\"\\nâœ… SUCCESS!\")\n",
    "    print(f\"   Total Spaces Processed: {total_spaces}\")\n",
    "    print(f\"   Total Messages Extracted: {total_messages}\")\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    display(df)\n",
    "else:\n",
    "    print(\"\\nâš  No data found across any Genie spaces\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0aa111e-7973-42cd-ad46-5c3b80131761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Save the table as a delta table and add a nice detailed table description!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2b6e6e-a659-4c8a-82bc-378301c18104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the observability data to a Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.genie_observability_master_table\")\n",
    "\n",
    "# Add a detailed table description\n",
    "spark.sql(f\"\"\"\n",
    "  COMMENT ON TABLE {catalog_name}.{schema_name}.genie_observability_master_table IS \n",
    "  'Comprehensive observability table for Genie AI/BI spaces. Contains detailed message-level data including user questions, AI responses, generated SQL queries, execution metadata, user feedback ratings, and error information. Used for monitoring Genie usage, analyzing query patterns, and tracking user engagement across all accessible Genie spaces.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3cdb112-7eb8-4f1d-9938-26a223da8e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We successfully created the **genie_observability_master_table** table! Now lets proceed to create the **genie_cost_analysis_master_table** table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b591ee-8c1c-4110-a722-a9ae595e5af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**An Aside: How we approached attributing costs from a Genie Space.**\n",
    "\n",
    "The Strategy: We used a \"Time-Weighted\" approach. It treats the total cost of a warehouse as a \"pie\" and slices it up based on how long each query took to run relative to other queries.\n",
    "\n",
    "Step 1: Calculate the Total Bill (Per Warehouse) First, we look at the system billing logs (system.billing.usage) to calculate the total price tag (in DBUs and Dollars) for every SQL warehouse that was active during our specified dates.\n",
    "\n",
    "Step 2: Measure \"Work\" (Per Query) we look at the query history (system.query.history) and defines \"work\" as Duration (specifically, Compilation Time + Execution Time).\n",
    "\n",
    "We calculate how long a specific query took.\n",
    "\n",
    "We then calculate the Total Duration of all queries that ran on that same warehouse during that time window (using a window function).\n",
    "\n",
    "Step 3: Calculate the Ratio (The Slice) It compares the two numbers from Step 2 to create a percentage:\n",
    "\n",
    "\"This query took 1 minute. The warehouse ran queries for a total of 100 minutes. Therefore, this query is responsible for 1% of the bill.\"\n",
    "\n",
    "Step 4: Assign the Cost It multiplies that percentage (1%) by the total warehouse bill found in Step 1 to assign a specific dollar cost to the query.\n",
    "\n",
    "Step 5: Filter for Genie Finally, it filters the results to show only the queries that originated from your specific Genie Space (genie_space_id), letting you see exactly how much that specific AI agent cost you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd41dd60-43b2-4911-9bc1-1bb7fd57e67f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Use this query to attribute compute costs to a single Genie space. Copy and run in the SQL editor to test for your space. We will not be running this as part of this notebook\n",
    "# WITH warehouse_usage AS (\n",
    "#   -- Capture SQL compute costs for each warehouse within the specified date range.\n",
    "#   SELECT\n",
    "#     u.usage_metadata.warehouse_id,\n",
    "#     SUM(u.usage_quantity) AS total_billed_dbus, -- Total DBUs billed for the warehouse\n",
    "#     SUM(u.usage_quantity * p.pricing.default) AS total_billed_dollars -- Total cost in dollars for the warehouse\n",
    "#   FROM system.billing.usage u\n",
    "#   JOIN system.billing.list_prices p \n",
    "#     ON u.sku_name = p.sku_name\n",
    "#     AND u.usage_start_time >= p.price_start_time\n",
    "#     AND (u.usage_start_time < p.price_end_time OR p.price_end_time IS NULL)\n",
    "#   WHERE u.usage_start_time >= :start_date::timestamp -- Filter by start date\n",
    "#     AND u.usage_end_time <= :end_date::timestamp     -- Filter by end date\n",
    "#     AND u.usage_unit = 'DBU'                         -- Only DBU usage\n",
    "#     AND u.sku_name ILIKE '%SQL%'                     -- Only SQL compute SKUs\n",
    "#   GROUP BY 1\n",
    "# ),\n",
    "# query_base AS (\n",
    "#   -- Gather query execution details and compute total execution time per query.\n",
    "#   SELECT\n",
    "#     statement_id, -- Unique query statement ID\n",
    "#     executed_by,  -- User who executed the query\n",
    "#     statement_text, -- The SQL code executed\n",
    "#     compute.warehouse_id, -- Warehouse used for the query\n",
    "#     query_source.genie_space_id, -- Genie space ID associated with the query\n",
    "#     start_time, -- Query start time\n",
    "#     -- Total time spent compiling and executing the query\n",
    "#     (\n",
    "#       COALESCE(compilation_duration_ms, 0) + \n",
    "#       COALESCE(execution_duration_ms, 0)\n",
    "#     ) AS total_accurate_duration_ms,\n",
    "#     -- Total warehouse activity time for proportional allocation\n",
    "#     SUM(\n",
    "#       COALESCE(compilation_duration_ms, 0) + \n",
    "#       COALESCE(execution_duration_ms, 0)\n",
    "#     ) OVER (PARTITION BY compute.warehouse_id) AS total_warehouse_activity_ms\n",
    "#   FROM system.query.history\n",
    "#   WHERE start_time >= :start_date::timestamp -- Filter by start date\n",
    "#     AND start_time <= :end_date::timestamp   -- Filter by end date\n",
    "#     AND compute.warehouse_id IS NOT NULL     -- Only queries with a warehouse\n",
    "# ),\n",
    "# allocated_metrics AS (\n",
    "#   -- Allocate DBU and dollar costs to each query based on its share of warehouse activity.\n",
    "#   SELECT\n",
    "#     q.statement_id,\n",
    "#     q.executed_by,\n",
    "#     q.statement_text,\n",
    "#     q.genie_space_id,\n",
    "#     q.start_time,\n",
    "#     q.warehouse_id,\n",
    "#     q.total_accurate_duration_ms,\n",
    "#     -- Proportion of warehouse activity attributed to this query\n",
    "#     (q.total_accurate_duration_ms / NULLIF(q.total_warehouse_activity_ms, 0)) AS work_proportion,\n",
    "#     u.total_billed_dbus,    -- Total DBUs billed for the warehouse\n",
    "#     u.total_billed_dollars  -- Total dollars billed for the warehouse\n",
    "#   FROM query_base q\n",
    "#   -- Use LEFT JOIN to ensure queries are included even if billing data is missing\n",
    "#   LEFT JOIN warehouse_usage u ON q.warehouse_id = u.warehouse_id\n",
    "# )\n",
    "# SELECT \n",
    "#   statement_id, -- Query statement ID\n",
    "#   executed_by AS user_email, -- User email who ran the query\n",
    "#   start_time, -- Query start time\n",
    "#   genie_space_id, -- Genie space ID\n",
    "#   warehouse_id, -- Warehouse ID\n",
    "#   ROUND(total_accurate_duration_ms / 1000, 2) AS accurate_duration_seconds, -- Query duration in seconds\n",
    "#   ROUND(work_proportion * total_billed_dbus, 4) AS dbus_consumed,           -- DBUs consumed by the query\n",
    "#   ROUND(work_proportion * total_billed_dollars, 4) AS cost_usd,             -- Cost in USD for the query\n",
    "#   statement_text AS sql_code -- The SQL code executed\n",
    "# FROM allocated_metrics\n",
    "# WHERE genie_space_id = :genie_space_id -- Filter for the specified Genie space\n",
    "# ORDER BY start_time DESC; -- Show most recent queries first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "319b2f34-a68f-4358-8877-9a5cc0454a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the code below we use system tables and the ability to link query execution source to Genie Space ID to calculate the costs that arise from a genie space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2460bd11-768d-4ff7-b8b2-4a3b7b0842b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog_name}.{schema_name}.genie_cost_analysis_master_table AS\n",
    "WITH date_range AS (\n",
    "  -- Determine the analysis window: from the earliest Genie message to today.\n",
    "  SELECT \n",
    "    MIN(created_datetime) AS start_date,\n",
    "    CURRENT_DATE() AS end_date\n",
    "  FROM {catalog_name}.{schema_name}.genie_observability_master_table\n",
    "),\n",
    "warehouse_usage AS (\n",
    "  -- Aggregate total DBU and dollar costs for each SQL warehouse in the date range.\n",
    "  SELECT\n",
    "    u.usage_metadata.warehouse_id,\n",
    "    SUM(u.usage_quantity) AS total_billed_dbus, -- Total DBUs billed for the warehouse\n",
    "    SUM(u.usage_quantity * p.pricing.default) AS total_billed_dollars -- Total cost in dollars\n",
    "  FROM system.billing.usage u\n",
    "  JOIN system.billing.list_prices p \n",
    "    ON u.sku_name = p.sku_name\n",
    "    AND u.usage_start_time >= p.price_start_time\n",
    "    AND (u.usage_start_time < p.price_end_time OR p.price_end_time IS NULL)\n",
    "  CROSS JOIN date_range dr\n",
    "  WHERE u.usage_start_time >= dr.start_date -- Only usage within the Genie message window\n",
    "    AND u.usage_end_time <= dr.end_date\n",
    "    AND u.usage_unit = 'DBU'                -- Only DBU usage (compute)\n",
    "    AND u.sku_name ILIKE '%SQL%'            -- Only SQL compute SKUs\n",
    "  GROUP BY 1\n",
    "),\n",
    "query_base AS (\n",
    "  -- Gather all Genie-related queries and compute their execution durations.\n",
    "  SELECT\n",
    "    statement_id, -- Unique query statement ID\n",
    "    executed_by,\n",
    "    statement_text,\n",
    "    compute.warehouse_id,\n",
    "    query_source.genie_space_id,\n",
    "    start_time,\n",
    "    (COALESCE(compilation_duration_ms, 0) + COALESCE(execution_duration_ms, 0)) AS total_accurate_duration_ms,\n",
    "    SUM(COALESCE(compilation_duration_ms, 0) + COALESCE(execution_duration_ms, 0)) OVER (PARTITION BY compute.warehouse_id) AS total_warehouse_activity_ms\n",
    "  FROM system.query.history\n",
    "  CROSS JOIN date_range dr\n",
    "  WHERE start_time >= dr.start_date -- Only queries within the Genie message window\n",
    "    AND start_time <= dr.end_date\n",
    "    AND compute.warehouse_id IS NOT NULL\n",
    "),\n",
    "allocated_metrics AS (\n",
    "  -- Allocate warehouse costs to each query based on its share of total warehouse activity.\n",
    "  SELECT\n",
    "    q.statement_id,\n",
    "    q.executed_by,\n",
    "    q.statement_text,\n",
    "    q.genie_space_id,\n",
    "    q.start_time,\n",
    "    q.warehouse_id,\n",
    "    q.total_accurate_duration_ms,\n",
    "    (q.total_accurate_duration_ms / NULLIF(q.total_warehouse_activity_ms, 0)) AS work_proportion,\n",
    "    u.total_billed_dbus,\n",
    "    u.total_billed_dollars\n",
    "  FROM query_base q\n",
    "  LEFT JOIN warehouse_usage u ON q.warehouse_id = u.warehouse_id -- Include queries even if billing data is missing\n",
    ")\n",
    "SELECT \n",
    "  statement_id,\n",
    "  executed_by AS user_email,\n",
    "  start_time,\n",
    "  genie_space_id,\n",
    "  warehouse_id,\n",
    "  ROUND(total_accurate_duration_ms / 1000, 2) AS accurate_duration_seconds,\n",
    "  ROUND(work_proportion * total_billed_dbus, 4) AS dbus_consumed,\n",
    "  ROUND(work_proportion * total_billed_dollars, 4) AS cost_usd,\n",
    "  statement_text AS sql_code\n",
    "FROM allocated_metrics\n",
    "WHERE genie_space_id is NOT NULL\n",
    "ORDER BY start_time DESC\n",
    "\"\"\")\n",
    "print(\"Table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec6ffa4-9c2b-4325-b9dd-a0fb8ff5fbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a detailed table description\n",
    "spark.sql(f\"\"\"\n",
    "  COMMENT ON TABLE {catalog_name}.{schema_name}.genie_cost_analysis_master_table IS \n",
    "  'This table provides a granular cost attribution analysis for Databricks Genie spaces. It links individual SQL queries executed within Genie to the underlying SQL Warehouse compute costs. By calculating the \"work proportion\" of every query relative to the warehouse''s total activity, it estimates the specific DBU consumption and USD cost for each query statement.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bea3229f-7bbe-4806-9073-49bca9c600b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This concludes this notebook. We now have created two critical tables that will help us monitor and analyse our genie spaces across multiple dimensions:\n",
    "- Cost attribution and chargeback\n",
    "- Usability and accuracy\n",
    "- Insights to further improve the space by adding more tables, adding SQL examples, trusted assets etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c128ab42-23d0-455e-99fc-a3027bee4156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Take this to the next level\n",
    "You can enhance this notebook by incorporating more advanced techniques such as:\n",
    "- Run topic modelling on user user questions to identify common patterns that can help you improve youe Genie space\n",
    "- Incorporate more advanced cost attribution strategies for charge backs. See [LINK](https://github.com/databrickslabs/sandbox/tree/main/dbsql/cost_per_query/PrPr) and [LINK](https://www.databricks.com/resources/demos/tutorials/governance/system-tables?itm_data=demo_center) for inspiration.\n",
    "- Incorporate this notebook as part of your custome Databricks Jobs process"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "04_genie_observability_deep_dive",
   "widgets": {
    "catalog_name": {
     "currentValue": "kg_demo_catalog",
     "nuid": "4e44dffa-bf52-4452-9c0e-5cee0134286e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "users",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "default",
     "nuid": "80c4b03f-ad5e-4661-a5cb-c56cf9bbd3c0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
