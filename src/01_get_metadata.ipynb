{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c17e7780-b4dc-493f-8bbc-7394be6fd0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks_sdk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b3e27e-8c93-439b-abf4-04bfd02b9694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d3617a-92c0-4274-b19f-580847665655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"users\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "assert catalog_name and schema_name, \"catalog_name and schema_name must be provided\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4182e860-0fbe-4199-81b0-fff19b38aa64",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "genie"
    }
   },
   "outputs": [],
   "source": [
    "spaces = []\n",
    "page_token = None\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "while True:\n",
    "    response = w.genie.list_spaces(page_token=page_token)\n",
    "    for s in response.spaces:\n",
    "        spaces.append({\n",
    "            \"space_id\": getattr(s, \"space_id\", None),\n",
    "            \"name\": getattr(s, \"title\", None),\n",
    "            \"description\": getattr(s, \"description\", None),\n",
    "            \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "        })\n",
    "    if not response.next_page_token or response.next_page_token == \"\":\n",
    "        break\n",
    "    page_token = response.next_page_token\n",
    "\n",
    "pdf = pd.DataFrame(spaces)\n",
    "if not pdf.empty:\n",
    "    spark_df = spark.createDataFrame(pdf)\n",
    "    spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\n",
    "        f\"{catalog_name}.{schema_name}.adb_genie_spaces\"\n",
    "    )\n",
    "    print(f\"Loaded {spark_df.count()} Genie spaces into table adb_genie_spaces\")\n",
    "else:\n",
    "    print(\"No Genie spaces found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47818c67-4987-426f-af81-b4bb9860d098",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dashboards"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "\n",
    "#w = WorkspaceClient()\n",
    "\n",
    "rows = []\n",
    "for d in w.lakeview.list(page_size=100):\n",
    "    d_dict = d.as_dict()\n",
    "    # Convert timestamps to Python datetime if they are not already\n",
    "    create_time = d_dict.get(\"create_time\")\n",
    "    update_time = d_dict.get(\"update_time\")\n",
    "    if isinstance(create_time, str):\n",
    "        try:\n",
    "            create_time = datetime.fromisoformat(create_time)\n",
    "        except Exception:\n",
    "            create_time = None\n",
    "    if isinstance(update_time, str):\n",
    "        try:\n",
    "            update_time = datetime.fromisoformat(update_time)\n",
    "        except Exception:\n",
    "            update_time = None\n",
    "    rows.append((\n",
    "        d_dict.get(\"dashboard_id\"),\n",
    "        d_dict.get(\"display_name\"),\n",
    "        create_time,\n",
    "        d_dict.get(\"lifecycle_state\"),\n",
    "        update_time,\n",
    "        d_dict.get(\"warehouse_id\")\n",
    "    ))\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "schema = StructType([\n",
    "    StructField('dashboard_id', StringType(), True),\n",
    "    StructField('display_name', StringType(), True),\n",
    "    StructField('create_time', TimestampType(), True),\n",
    "    StructField('lifecycle_state', StringType(), True),\n",
    "    StructField('update_time', TimestampType(), True),\n",
    "    StructField('warehouse_id', StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    rows,\n",
    "    schema\n",
    ")\n",
    "df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.adb_dashboards\")\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d10f43-1970-466e-959d-56218da097a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schedules"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "\n",
    "schedules = []\n",
    "\n",
    "dashboard_ids = [\n",
    "    row.dashboard_id\n",
    "    for row in spark.table(f\"{catalog_name}.{schema_name}.adb_dashboards\").select(\"dashboard_id\").collect()\n",
    "]\n",
    "\n",
    "for dashboard_id in dashboard_ids:\n",
    "    try:\n",
    "        for sched in w.lakeview.list_schedules(dashboard_id=dashboard_id):\n",
    "            sched_dict = sched.as_dict()\n",
    "            subscriber = sched_dict.get(\"subscriber\", {})\n",
    "            destination_subscriber = subscriber.get(\"destination_subscriber\")\n",
    "            user_subscriber = subscriber.get(\"user_subscriber\")\n",
    "            schedules.append({\n",
    "                \"dashboard_id\": dashboard_id,\n",
    "                \"schedule_id\": sched_dict.get(\"schedule_id\"),\n",
    "                \"create_time\": sched_dict.get(\"create_time\"),\n",
    "                \"display_name\": sched_dict.get(\"display_name\"),\n",
    "                \"pause_status\": sched_dict.get(\"pause_status\")\n",
    "            })\n",
    "    except Exception as e:\n",
    "        # Optionally log or print the dashboard_id that failed\n",
    "        continue\n",
    "\n",
    "if schedules:\n",
    "    import pandas as pd\n",
    "    pdf_sched = pd.DataFrame(schedules)\n",
    "    spark_df_sched = spark.createDataFrame(pdf_sched)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.adb_dashboard_schedules\")\n",
    "    spark_df_sched.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.adb_dashboard_schedules\")\n",
    "    #display(spark_df_sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2bd360b-f600-4381-a924-b0e2f4d86b35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schedule_Subscriptions"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook Python\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import pandas as pd\n",
    "\n",
    "source_table = f\"{catalog_name}.{schema_name}.adb_dashboards\"\n",
    "target_table = f\"{catalog_name}.{schema_name}.abd_dashboard_subscriptions\"  # as requested\n",
    "\n",
    "#w = WorkspaceClient()\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def get_field(obj_or_dict, *names):\n",
    "    \"\"\"Return the first non-None among possible field names (works for dicts or objects).\"\"\"\n",
    "    for n in names:\n",
    "        if isinstance(obj_or_dict, dict):\n",
    "            if n in obj_or_dict and obj_or_dict[n] is not None:\n",
    "                return obj_or_dict[n]\n",
    "        else:\n",
    "            v = getattr(obj_or_dict, n, None)\n",
    "            if v is not None:\n",
    "                return v\n",
    "    return None\n",
    "\n",
    "def to_dict_safe(x):\n",
    "    return (getattr(x, \"as_dict\", lambda: {})() or {}) if x is not None else {}\n",
    "\n",
    "# ---------- get dashboard ids ----------\n",
    "#dash_df = spark.table(source_table).select(\"dashboard_id\").distinct().collect()\n",
    "sched_df = spark.table(f\"{catalog_name}.{schema_name}.adb_dashboard_schedules\").collect()\n",
    "rows = []\n",
    "count = 0\n",
    "\n",
    "for r in sched_df:  # avoids collecting everything to driver at once\n",
    "    dashboard_id = r[\"dashboard_id\"]\n",
    "    schedule_id = r[\"schedule_id\"]\n",
    "    try:\n",
    "                    # enumerate subscriptions for this schedule\n",
    "        for sub in w.lakeview.list_subscriptions(dashboard_id=dashboard_id, schedule_id=schedule_id):\n",
    "                subdict = to_dict_safe(sub)\n",
    "                subscriber  = get_field(subdict, \"subscriber\") or get_field(sub, \"subscriber\") or {}\n",
    "\n",
    "                user_subscriber = get_field(subscriber, \"user_subscriber\")\n",
    "                destination_subscriber = get_field(subscriber, \"destination_subscriber\")\n",
    "\n",
    "                user_id = (\n",
    "                    get_field(user_subscriber or {}, \"user_id\", \"id\")\n",
    "                    if user_subscriber is not None else None\n",
    "                )\n",
    "\n",
    "                subscription_id = get_field(subdict, \"subscription_id\") or get_field(sub, \"subscription_id\")\n",
    "\n",
    "                # capture destination info if present (email/slack/webhook/etc.)\n",
    "                destination_type = get_field(destination_subscriber or {}, \"destination_type\", \"type\")\n",
    "                destination_id   = get_field(destination_subscriber or {}, \"destination_id\", \"destination\", \"id\")\n",
    "\n",
    "                rows.append({\n",
    "                    \"dashboard_id\": dashboard_id,\n",
    "                    \"schedule_id\": schedule_id,\n",
    "                    \"subscription_id\": subscription_id,\n",
    "                    \"create_time\": create_time,\n",
    "                    \"user_id\": user_id,\n",
    "                    \"destination_id\": destination_id,\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        # If youâ€™d prefer to see failures in the table, uncomment the append below\n",
    "        # rows.append({\"dashboard_id\": dashboard_id, \"error\": str(e)})\n",
    "        continue\n",
    "\n",
    "print(f\"Collected {count} subscription rows across dashboards.\")\n",
    "\n",
    "# ---------- write to Delta table ----------\n",
    "pdf = pd.DataFrame(rows)\n",
    "\n",
    "# If there are no rows, create an empty DF with the expected schema so the table still exists\n",
    "if pdf.empty:\n",
    "    pdf = pd.DataFrame(columns=[\n",
    "        \"dashboard_id\",\"schedule_id\",\"subscription_id\",\"create_time\",\n",
    "        \"user_id\",\"destination_id\"\n",
    "    ])\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "spark_df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Optional: try to cast timestamps/ids nicely (depends on your upstream data)\n",
    "from pyspark.sql import functions as F\n",
    "spark_df = (\n",
    "    spark_df\n",
    "    .withColumn(\"create_time\", F.to_timestamp(\"create_time\"))\n",
    "    .select(\"dashboard_id\", \"schedule_id\", \"subscription_id\", \"create_time\", \"user_id\", \"destination_id\")\n",
    ")\n",
    "\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "\n",
    "#display(spark.table(target_table).limit(50))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7040052326826818,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_get_metadata",
   "widgets": {
    "catalog_name": {
     "currentValue": "mido_edw_dev",
     "nuid": "442fccc1-a0de-49e0-8672-9002e015ad39",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mido_edw_dev",
      "label": null,
      "name": "catalog_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "mido_edw_dev",
      "label": null,
      "name": "catalog_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema_name": {
     "currentValue": "adb_usage",
     "nuid": "46db1e8c-ec31-4ee4-a472-75aad508cb52",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "adb_usage",
      "label": null,
      "name": "schema_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "adb_usage",
      "label": null,
      "name": "schema_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
