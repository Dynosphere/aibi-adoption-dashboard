{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f422268d-cbfa-48ac-bc8e-ca7634353e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AI/BI Adoption Dashboard - Metadata Collection\n",
    "\n",
    "This notebook collects metadata from various Databricks AI/BI services using the Databricks SDK.\n",
    "\n",
    "## Tables Created\n",
    "\n",
    "This notebook creates the following Delta tables:\n",
    "\n",
    "### Genie\n",
    "* `adb_genie_spaces` - All Genie spaces\n",
    "* `adb_genie_conversations` - All conversations within Genie spaces\n",
    "* `adb_genie_messages` - All messages with query results and errors\n",
    "\n",
    "### Dashboards\n",
    "* `adb_dashboards` - All Lakeview dashboards\n",
    "* `adb_dashboard_schedules` - Dashboard schedules\n",
    "* `adb_dashboard_subscriptions` - Dashboard subscriptions\n",
    "\n",
    "### Models & Serving\n",
    "* `adb_models` - Unity Catalog registered models\n",
    "* `adb_serving_endpoints` - Model serving endpoints\n",
    "\n",
    "### Apps\n",
    "* `adb_apps` - Databricks Apps\n",
    "\n",
    "## Parameters\n",
    "* `catalog_name` - Target catalog for tables\n",
    "* `schema_name` - Target schema for tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c17e7780-b4dc-493f-8bbc-7394be6fd0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks_sdk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b3e27e-8c93-439b-abf4-04bfd02b9694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d3617a-92c0-4274-b19f-580847665655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"users\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"skip_get_conversations\", \"true\")\n",
    "\n",
    "skip_get_conversations = dbutils.widgets.get(\"skip_get_conversations\").lower() == 'true'\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "assert catalog_name and schema_name, \"catalog_name and schema_name must be provided\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68092ce0-a2ce-4873-9155-d0253916bfc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a51ee72c-e963-4ce2-83e9-1c5def44c5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Genie Spaces, Conversations, and Messages\n",
    "\n",
    "This section collects metadata about Genie spaces and their usage:\n",
    "* **adb_genie_spaces**: All Genie spaces in the workspace\n",
    "* **adb_genie_conversations**: All conversations within each Genie space\n",
    "* **adb_genie_messages**: All messages within each conversation, including query results and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4182e860-0fbe-4199-81b0-fff19b38aa64",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "genie spaces"
    }
   },
   "outputs": [],
   "source": [
    "spaces = []\n",
    "page_token = None\n",
    "\n",
    "# w = WorkspaceClient()\n",
    "\n",
    "while True:\n",
    "    response = w.genie.list_spaces(page_token=page_token)\n",
    "    for s in response.spaces:\n",
    "        spaces.append({\n",
    "            \"space_id\": getattr(s, \"space_id\", None),\n",
    "            \"name\": getattr(s, \"title\", None),\n",
    "            \"description\": getattr(s, \"description\", None),\n",
    "            \"warehouse_id\": getattr(s, \"warehouse_id\", None)\n",
    "        })\n",
    "    if not response.next_page_token or response.next_page_token == \"\":\n",
    "        break\n",
    "    page_token = response.next_page_token\n",
    "\n",
    "pdf = pd.DataFrame(spaces)\n",
    "if not pdf.empty:\n",
    "    genie_spaces_df = spark.createDataFrame(pdf)\n",
    "    genie_spaces_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\n",
    "        f\"{catalog_name}.{schema_name}.adb_genie_spaces\"\n",
    "    )\n",
    "    print(f\"Loaded {genie_spaces_df.count()} Genie spaces into table adb_genie_spaces\")\n",
    "else:\n",
    "    print(\"No Genie spaces found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c6222e1-f120-4507-b426-dde63d116b4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Genie Conversations"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#w = WorkspaceClient()\n",
    "\n",
    "conversations = []\n",
    "has_manage_on_all_spaces = True\n",
    "include_all_conversations = has_manage_on_all_spaces\n",
    "# Get all space IDs from the previously created table\n",
    "space_ids = [\n",
    "    row.space_id\n",
    "    for row in spark.table(f\"{catalog_name}.{schema_name}.adb_genie_spaces\").select('space_id').collect()\n",
    "]\n",
    "\n",
    "print(f\"Fetching conversations for {len(space_ids)} Genie spaces...\")\n",
    "\n",
    "for space_id in space_ids:\n",
    "    try:\n",
    "        page_token = None\n",
    "        while True:\n",
    "            response = w.genie.list_conversations(space_id=space_id, include_all=include_all_conversations, page_token=page_token)\n",
    "            for conv in response.conversations:\n",
    "                conv_dict = conv.as_dict()\n",
    "                conv_dict['space_id'] = space_id\n",
    "                conversations.append(conv_dict)\n",
    "\n",
    "            if not response.next_page_token or response.next_page_token == \"\":\n",
    "                break\n",
    "            page_token = response.next_page_token\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching conversations for space {space_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "if conversations:\n",
    "    genie_conversations_df = spark.createDataFrame(conversations).withColumn('created_timestamp', F.from_unixtime(F.col('created_timestamp')/1000).cast('timestamp'))\n",
    "    genie_conversations_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "        f\"{catalog_name}.{schema_name}.adb_genie_conversations\"\n",
    "    )\n",
    "    print(f\"Loaded {genie_conversations_df.count()} Genie conversations into table adb_genie_conversations\")\n",
    "else:\n",
    "    print(\"No Genie conversations found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0d07be-3539-4647-a930-7728f3b0dd19",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Genie Messages"
    }
   },
   "outputs": [],
   "source": [
    "#w = WorkspaceClient()\n",
    "\n",
    "messages = []\n",
    "\n",
    "# Get all space_id and conversation_id pairs from the previously created table\n",
    "conversation_pairs = [\n",
    "    (row.space_id, row.conversation_id)\n",
    "    for row in spark.table(f\"{catalog_name}.{schema_name}.adb_genie_conversations\").select(\"space_id\", \"conversation_id\").collect()\n",
    "]\n",
    "\n",
    "print(f\"Fetching messages for {len(conversation_pairs)} conversations...\")\n",
    "\n",
    "for space_id, conversation_id in conversation_pairs:\n",
    "    if skip_get_conversations:\n",
    "        print('Skipping conversations pull as skip_get_conversations is set to true')\n",
    "        break\n",
    "    try:\n",
    "        page_token = None\n",
    "        while True:\n",
    "            response = w.genie.list_conversation_messages(\n",
    "                space_id=space_id,\n",
    "                conversation_id=conversation_id,\n",
    "                page_token=page_token\n",
    "            )\n",
    "            if not response.messages:\n",
    "                break\n",
    "            \n",
    "            for msg in response.messages:\n",
    "                # Extract query info from attachments\n",
    "                statement_id = None\n",
    "                query_text = None\n",
    "                row_count = None\n",
    "                \n",
    "                if msg.attachments:\n",
    "                    for attachment in msg.attachments:\n",
    "                        if attachment.query:\n",
    "                            statement_id = attachment.query.statement_id\n",
    "                            query_text = attachment.query.query\n",
    "                            if attachment.query.query_result_metadata:\n",
    "                                row_count = attachment.query.query_result_metadata.row_count\n",
    "                            break  # Use first query attachment\n",
    "                \n",
    "                # Extract error info if available\n",
    "                error_message = None\n",
    "                if msg.error:\n",
    "                    error_message = msg.error.error\n",
    "                \n",
    "                messages.append({\n",
    "                    \"space_id\": space_id,\n",
    "                    \"conversation_id\": conversation_id,\n",
    "                    \"message_id\": msg.message_id,\n",
    "                    \"content\": msg.content,\n",
    "                    \"user_id\": msg.user_id,\n",
    "                    \"created_timestamp\": msg.created_timestamp,\n",
    "                    \"last_updated_timestamp\": msg.last_updated_timestamp,\n",
    "                    \"status\": msg.status.value if msg.status else None,\n",
    "                    \"num_attachments\": len(msg.attachments) if msg.attachments else 0,\n",
    "                    \"statement_id\": statement_id,\n",
    "                    \"query_text\": query_text,\n",
    "                    \"row_count\": row_count,\n",
    "                    \"error_message\": error_message\n",
    "                })\n",
    "            \n",
    "            if not response.next_page_token or response.next_page_token == \"\":\n",
    "                break\n",
    "            page_token = response.next_page_token\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching messages for conversation {conversation_id} in space {space_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "if messages:\n",
    "    genie_messages_df = spark.createDataFrame(messages, 'space_id string, conversation_id string, message_id string, content string, user_id string, created_timestamp bigint, last_updated_timestamp bigint, status string, num_attachments int, statement_id string, query_text string, row_count string, error_message string') \\\n",
    "        .withColumn('created_timestamp', F.from_unixtime(F.col('created_timestamp')/1000).cast('timestamp')) \\\n",
    "        .withColumn('last_updated_timestamp', F.from_unixtime(F.col('last_updated_timestamp')/1000).cast('timestamp'))\n",
    "    genie_messages_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "        f\"{catalog_name}.{schema_name}.adb_genie_messages\"\n",
    "    )\n",
    "    print(f\"Loaded {genie_messages_df.count()} Genie messages into table adb_genie_messages\")\n",
    "else:\n",
    "    print(\"No Genie messages found or process skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e895791e-6f87-4035-b075-75d4c28af9ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lakeview Dashboards, Schedules, and Subscriptions\n",
    "\n",
    "This section collects metadata about Lakeview dashboards and their distribution:\n",
    "* **adb_dashboards**: All Lakeview dashboards in the workspace\n",
    "* **adb_dashboard_schedules**: Scheduled refresh/distribution for dashboards\n",
    "* **abd_dashboard_subscriptions**: User and destination subscriptions for dashboard schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47818c67-4987-426f-af81-b4bb9860d098",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dashboards"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "\n",
    "#w = WorkspaceClient()\n",
    "\n",
    "rows = []\n",
    "for d in w.lakeview.list(page_size=100):\n",
    "    d_dict = d.as_dict()\n",
    "    # Convert timestamps to Python datetime if they are not already\n",
    "    create_time = d_dict.get(\"create_time\")\n",
    "    update_time = d_dict.get(\"update_time\")\n",
    "    if isinstance(create_time, str):\n",
    "        try:\n",
    "            create_time = datetime.fromisoformat(create_time)\n",
    "        except Exception:\n",
    "            create_time = None\n",
    "    if isinstance(update_time, str):\n",
    "        try:\n",
    "            update_time = datetime.fromisoformat(update_time)\n",
    "        except Exception:\n",
    "            update_time = None\n",
    "    rows.append((\n",
    "        d_dict.get(\"dashboard_id\"),\n",
    "        d_dict.get(\"display_name\"),\n",
    "        create_time,\n",
    "        d_dict.get(\"lifecycle_state\"),\n",
    "        update_time,\n",
    "        d_dict.get(\"warehouse_id\")\n",
    "    ))\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "schema = StructType([\n",
    "    StructField('dashboard_id', StringType(), True),\n",
    "    StructField('display_name', StringType(), True),\n",
    "    StructField('create_time', TimestampType(), True),\n",
    "    StructField('lifecycle_state', StringType(), True),\n",
    "    StructField('update_time', TimestampType(), True),\n",
    "    StructField('warehouse_id', StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    rows,\n",
    "    schema\n",
    ")\n",
    "df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.adb_dashboards\")\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d10f43-1970-466e-959d-56218da097a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schedules"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "\n",
    "schedules = []\n",
    "\n",
    "dashboard_ids = [\n",
    "    row.dashboard_id\n",
    "    for row in spark.table(f\"{catalog_name}.{schema_name}.adb_dashboards\").select(\"dashboard_id\").collect()\n",
    "]\n",
    "\n",
    "for dashboard_id in dashboard_ids:\n",
    "    try:\n",
    "        for sched in w.lakeview.list_schedules(dashboard_id=dashboard_id):\n",
    "            sched_dict = sched.as_dict()\n",
    "            subscriber = sched_dict.get(\"subscriber\", {})\n",
    "            destination_subscriber = subscriber.get(\"destination_subscriber\")\n",
    "            user_subscriber = subscriber.get(\"user_subscriber\")\n",
    "            schedules.append({\n",
    "                \"dashboard_id\": dashboard_id,\n",
    "                \"schedule_id\": sched_dict.get(\"schedule_id\"),\n",
    "                \"create_time\": sched_dict.get(\"create_time\"),\n",
    "                \"display_name\": sched_dict.get(\"display_name\"),\n",
    "                \"pause_status\": sched_dict.get(\"pause_status\")\n",
    "            })\n",
    "    except Exception as e:\n",
    "        # Optionally log or print the dashboard_id that failed\n",
    "        continue\n",
    "\n",
    "if schedules:\n",
    "    import pandas as pd\n",
    "    pdf_sched = pd.DataFrame(schedules)\n",
    "    spark_df_sched = spark.createDataFrame(pdf_sched)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.adb_dashboard_schedules\")\n",
    "    spark_df_sched.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.adb_dashboard_schedules\")\n",
    "    #display(spark_df_sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2bd360b-f600-4381-a924-b0e2f4d86b35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schedule_Subscriptions"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook Python\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import pandas as pd\n",
    "\n",
    "source_table = f\"{catalog_name}.{schema_name}.adb_dashboards\"\n",
    "target_table = f\"{catalog_name}.{schema_name}.adb_dashboard_subscriptions\"  # as requested\n",
    "\n",
    "#w = WorkspaceClient()\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def get_field(obj_or_dict, *names):\n",
    "    \"\"\"Return the first non-None among possible field names (works for dicts or objects).\"\"\"\n",
    "    for n in names:\n",
    "        if isinstance(obj_or_dict, dict):\n",
    "            if n in obj_or_dict and obj_or_dict[n] is not None:\n",
    "                return obj_or_dict[n]\n",
    "        else:\n",
    "            v = getattr(obj_or_dict, n, None)\n",
    "            if v is not None:\n",
    "                return v\n",
    "    return None\n",
    "\n",
    "def to_dict_safe(x):\n",
    "    return (getattr(x, \"as_dict\", lambda: {})() or {}) if x is not None else {}\n",
    "\n",
    "# ---------- get dashboard ids ----------\n",
    "#dash_df = spark.table(source_table).select(\"dashboard_id\").distinct().collect()\n",
    "sched_df = spark.table(f\"{catalog_name}.{schema_name}.adb_dashboard_schedules\").collect()\n",
    "rows = []\n",
    "count = 0\n",
    "\n",
    "for r in sched_df:  # avoids collecting everything to driver at once\n",
    "    dashboard_id = r[\"dashboard_id\"]\n",
    "    schedule_id = r[\"schedule_id\"]\n",
    "    try:\n",
    "                    # enumerate subscriptions for this schedule\n",
    "        for sub in w.lakeview.list_subscriptions(dashboard_id=dashboard_id, schedule_id=schedule_id):\n",
    "                subdict = to_dict_safe(sub)\n",
    "                subscriber  = get_field(subdict, \"subscriber\") or get_field(sub, \"subscriber\") or {}\n",
    "\n",
    "                user_subscriber = get_field(subscriber, \"user_subscriber\")\n",
    "                destination_subscriber = get_field(subscriber, \"destination_subscriber\")\n",
    "\n",
    "                user_id = (\n",
    "                    get_field(user_subscriber or {}, \"user_id\", \"id\")\n",
    "                    if user_subscriber is not None else None\n",
    "                )\n",
    "\n",
    "                subscription_id = get_field(subdict, \"subscription_id\") or get_field(sub, \"subscription_id\")\n",
    "\n",
    "                # capture destination info if present (email/slack/webhook/etc.)\n",
    "                destination_type = get_field(destination_subscriber or {}, \"destination_type\", \"type\")\n",
    "                destination_id   = get_field(destination_subscriber or {}, \"destination_id\", \"destination\", \"id\")\n",
    "\n",
    "                rows.append({\n",
    "                    \"dashboard_id\": dashboard_id,\n",
    "                    \"schedule_id\": schedule_id,\n",
    "                    \"subscription_id\": subscription_id,\n",
    "                    \"create_time\": create_time,\n",
    "                    \"user_id\": user_id,\n",
    "                    \"destination_id\": destination_id,\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        # If youâ€™d prefer to see failures in the table, uncomment the append below\n",
    "        # rows.append({\"dashboard_id\": dashboard_id, \"error\": str(e)})\n",
    "        continue\n",
    "\n",
    "print(f\"Collected {count} subscription rows across dashboards.\")\n",
    "\n",
    "# ---------- write to Delta table ----------\n",
    "pdf = pd.DataFrame(rows)\n",
    "\n",
    "# If there are no rows, create an empty DF with the expected schema so the table still exists\n",
    "if pdf.empty:\n",
    "    pdf = pd.DataFrame(columns=[\n",
    "        \"dashboard_id\",\"schedule_id\",\"subscription_id\",\"create_time\",\n",
    "        \"user_id\",\"destination_id\"\n",
    "    ])\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
    "spark_df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Optional: try to cast timestamps/ids nicely (depends on your upstream data)\n",
    "from pyspark.sql import functions as F\n",
    "spark_df = (\n",
    "    spark_df\n",
    "    .withColumn(\"create_time\", F.to_timestamp(\"create_time\"))\n",
    "    .select(\"dashboard_id\", \"schedule_id\", \"subscription_id\", \"create_time\", \"user_id\", \"destination_id\")\n",
    ")\n",
    "\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "\n",
    "#display(spark.table(target_table).limit(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0815ece0-2241-4704-ba22-69a5af898120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unity Catalog Models\n",
    "\n",
    "This section collects metadata about registered models in Unity Catalog:\n",
    "* **adb_models**: All registered models with permissions and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa7c740-5a5e-4354-b57a-dd75fb4ea9a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#w = WorkspaceClient()\n",
    "\n",
    "models = []\n",
    "\n",
    "try:\n",
    "    for model in w.registered_models.list():\n",
    "        model_dict = model.as_dict() if hasattr(model, 'as_dict') else {}\n",
    "        \n",
    "        # Get permissions summary\n",
    "        num_users_with_access = 0\n",
    "        num_groups_with_access = 0\n",
    "        try:\n",
    "            full_name = model_dict.get('full_name') or f\"{model_dict.get('catalog_name', '')}.{model_dict.get('schema_name', '')}.{model_dict.get('name', '')}\"\n",
    "            if full_name and full_name != '..':\n",
    "                perms = w.registered_models.get_permissions(full_name)\n",
    "                if perms and hasattr(perms, 'access_control_list'):\n",
    "                    acl = perms.access_control_list or []\n",
    "                    for entry in acl:\n",
    "                        if hasattr(entry, 'user_name') and entry.user_name:\n",
    "                            num_users_with_access += 1\n",
    "                        if hasattr(entry, 'group_name') and entry.group_name:\n",
    "                            num_groups_with_access += 1\n",
    "        except Exception as e:\n",
    "            # Permissions may not be accessible, continue without them\n",
    "            pass\n",
    "        \n",
    "        # Convert timestamps\n",
    "        created_at = model_dict.get('created_at')\n",
    "        updated_at = model_dict.get('updated_at')\n",
    "        if isinstance(created_at, (int, float)) and created_at > 0:\n",
    "            try:\n",
    "                created_at = datetime.fromtimestamp(created_at / 1000)\n",
    "            except Exception:\n",
    "                created_at = None\n",
    "        if isinstance(updated_at, (int, float)) and updated_at > 0:\n",
    "            try:\n",
    "                updated_at = datetime.fromtimestamp(updated_at / 1000)\n",
    "            except Exception:\n",
    "                updated_at = None\n",
    "        \n",
    "        models.append({\n",
    "            \"full_name\": model_dict.get('full_name'),\n",
    "            \"name\": model_dict.get('name'),\n",
    "            \"catalog_name\": model_dict.get('catalog_name'),\n",
    "            \"schema_name\": model_dict.get('schema_name'),\n",
    "            \"created_at\": created_at,\n",
    "            \"created_by\": model_dict.get('created_by'),\n",
    "            \"updated_at\": updated_at,\n",
    "            \"updated_by\": model_dict.get('updated_by'),\n",
    "            \"owner\": model_dict.get('owner'),\n",
    "            \"comment\": model_dict.get('comment'),\n",
    "            \"num_users_with_access\": num_users_with_access,\n",
    "            \"num_groups_with_access\": num_groups_with_access\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"Error listing models: {e}\")\n",
    "\n",
    "pdf = pd.DataFrame(models)\n",
    "if not pdf.empty:\n",
    "    spark_df = spark.createDataFrame(pdf)\n",
    "    spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\n",
    "        f\"{catalog_name}.{schema_name}.adb_models\"\n",
    "    )\n",
    "    print(f\"Loaded {spark_df.count()} models into table adb_models\")\n",
    "else:\n",
    "    print(\"No models found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "708a3791-e4b4-4deb-a655-93b176535226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Serving Endpoints\n",
    "\n",
    "This section collects metadata about model serving endpoints:\n",
    "* **adb_serving_endpoints**: All serving endpoints with their models and permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131f046d-6a3b-4f82-9c85-3b5655d59731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#w = WorkspaceClient()\n",
    "\n",
    "serving_endpoints = []\n",
    "\n",
    "try:\n",
    "    for endpoint in w.serving_endpoints.list():\n",
    "        endpoint_dict = endpoint.as_dict() if hasattr(endpoint, 'as_dict') else {}\n",
    "        \n",
    "        # Get permissions summary\n",
    "        num_users_with_access = 0\n",
    "        num_groups_with_access = 0\n",
    "        try:\n",
    "            endpoint_name = endpoint_dict.get('name')\n",
    "            if endpoint_name:\n",
    "                perms = w.serving_endpoints.get_permissions(endpoint_name)\n",
    "                if perms and hasattr(perms, 'access_control_list'):\n",
    "                    acl = perms.access_control_list or []\n",
    "                    for entry in acl:\n",
    "                        if hasattr(entry, 'user_name') and entry.user_name:\n",
    "                            num_users_with_access += 1\n",
    "                        if hasattr(entry, 'group_name') and entry.group_name:\n",
    "                            num_groups_with_access += 1\n",
    "        except Exception as e:\n",
    "            # Permissions may not be accessible, continue without them\n",
    "            pass\n",
    "        \n",
    "        # Extract model info from config\n",
    "        config = endpoint_dict.get('config', {})\n",
    "        models_info = []\n",
    "        if isinstance(config, dict):\n",
    "            served_models = config.get('served_models', [])\n",
    "            if isinstance(served_models, list):\n",
    "                for model in served_models:\n",
    "                    if isinstance(model, dict):\n",
    "                        model_name = model.get('model_name') or model.get('name')\n",
    "                        if model_name:\n",
    "                            models_info.append(model_name)\n",
    "        \n",
    "        # Convert timestamps\n",
    "        creation_timestamp = endpoint_dict.get('creation_timestamp')\n",
    "        last_updated_timestamp = endpoint_dict.get('last_updated_timestamp')\n",
    "        if isinstance(creation_timestamp, (int, float)) and creation_timestamp > 0:\n",
    "            try:\n",
    "                creation_timestamp = datetime.fromtimestamp(creation_timestamp / 1000)\n",
    "            except Exception:\n",
    "                creation_timestamp = None\n",
    "        if isinstance(last_updated_timestamp, (int, float)) and last_updated_timestamp > 0:\n",
    "            try:\n",
    "                last_updated_timestamp = datetime.fromtimestamp(last_updated_timestamp / 1000)\n",
    "            except Exception:\n",
    "                last_updated_timestamp = None\n",
    "        \n",
    "        serving_endpoints.append({\n",
    "            \"name\": endpoint_dict.get('name'),\n",
    "            \"id\": endpoint_dict.get('id'),\n",
    "            \"creation_timestamp\": creation_timestamp,\n",
    "            \"creator\": endpoint_dict.get('creator'),\n",
    "            \"last_updated_timestamp\": last_updated_timestamp,\n",
    "            \"state\": endpoint_dict.get('state'),\n",
    "            \"models\": ','.join(models_info) if models_info else None,\n",
    "            \"num_users_with_access\": num_users_with_access,\n",
    "            \"num_groups_with_access\": num_groups_with_access\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"Error listing serving endpoints: {e}\")\n",
    "\n",
    "pdf = pd.DataFrame(serving_endpoints)\n",
    "if not pdf.empty:\n",
    "    spark_df = spark.createDataFrame(pdf)\n",
    "    spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\n",
    "        f\"{catalog_name}.{schema_name}.adb_serving_endpoints\"\n",
    "    )\n",
    "    print(f\"Loaded {spark_df.count()} serving endpoints into table adb_serving_endpoints\")\n",
    "else:\n",
    "    print(\"No serving endpoints found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99f666a3-0901-4ef4-ba29-88f19cefbcae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks Apps\n",
    "\n",
    "This section collects metadata about Databricks Apps:\n",
    "* **adb_apps**: All apps with their status and permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69274ce-6c31-4620-b27b-5b7ee28d6ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#w = WorkspaceClient()\n",
    "\n",
    "apps = []\n",
    "\n",
    "try:\n",
    "    for app in w.apps.list():\n",
    "        app_dict = app.as_dict() if hasattr(app, 'as_dict') else {}\n",
    "        \n",
    "        # Get permissions summary\n",
    "        num_users_with_access = 0\n",
    "        num_groups_with_access = 0\n",
    "        try:\n",
    "            app_name = app_dict.get('name')\n",
    "            if app_name:\n",
    "                perms = w.apps.get_permissions(app_name)\n",
    "                if perms and hasattr(perms, 'access_control_list'):\n",
    "                    acl = perms.access_control_list or []\n",
    "                    for entry in acl:\n",
    "                        if hasattr(entry, 'user_name') and entry.user_name:\n",
    "                            num_users_with_access += 1\n",
    "                        if hasattr(entry, 'group_name') and entry.group_name:\n",
    "                            num_groups_with_access += 1\n",
    "        except Exception as e:\n",
    "            # Permissions may not be accessible, continue without them\n",
    "            pass\n",
    "        \n",
    "        # Convert timestamps\n",
    "        create_time = app_dict.get('create_time')\n",
    "        update_time = app_dict.get('update_time')\n",
    "        if isinstance(create_time, str):\n",
    "            try:\n",
    "                create_time = datetime.fromisoformat(create_time.replace('Z', '+00:00'))\n",
    "            except Exception:\n",
    "                create_time = None\n",
    "        if isinstance(update_time, str):\n",
    "            try:\n",
    "                update_time = datetime.fromisoformat(update_time.replace('Z', '+00:00'))\n",
    "            except Exception:\n",
    "                update_time = None\n",
    "        \n",
    "        apps.append({\n",
    "            \"name\": app_dict.get('name'),\n",
    "            \"id\": app_dict.get('id'),\n",
    "            \"create_time\": create_time,\n",
    "            \"creator\": app_dict.get('creator'),\n",
    "            \"update_time\": update_time,\n",
    "            \"updater\": app_dict.get('updater'),\n",
    "            \"url\": app_dict.get('url'),\n",
    "            \"app_status\": app_dict.get('app_status'),\n",
    "            \"compute_status\": app_dict.get('compute_status'),\n",
    "            \"num_users_with_access\": num_users_with_access,\n",
    "            \"num_groups_with_access\": num_groups_with_access\n",
    "        })\n",
    "except Exception as e:\n",
    "    print(f\"Error listing apps: {e}\")\n",
    "\n",
    "pdf = pd.DataFrame(apps)\n",
    "if not pdf.empty:\n",
    "    spark_df = spark.createDataFrame(pdf)\n",
    "    spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\n",
    "        f\"{catalog_name}.{schema_name}.adb_apps\"\n",
    "    )\n",
    "    print(f\"Loaded {spark_df.count()} apps into table adb_apps\")\n",
    "else:\n",
    "    print(\"No apps found.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7868052644289571,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_get_metadata",
   "widgets": {
    "catalog_name": {
     "currentValue": "field_eng_slc",
     "nuid": "442fccc1-a0de-49e0-8672-9002e015ad39",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "users",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "adoption",
     "nuid": "46db1e8c-ec31-4ee4-a472-75aad508cb52",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "skip_get_conversations": {
     "currentValue": "false",
     "nuid": "3b81d1ee-af43-455e-a9be-5ee9604077cb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "true",
      "label": null,
      "name": "skip_get_conversations",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "true",
      "label": null,
      "name": "skip_get_conversations",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
